[
{"authors": ["Simon Gottschalk", "Elena Demidova"], "title": ["EventKG: A Multilingual Event-Centric Temporal Knowledge Graph"], "date": ["2018-04-12T14:12:48Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1804.04526v1"], "summary": ["  One of the key requirements to facilitate semantic analytics of information\nregarding contemporary and historical events on the Web, in the news and in\nsocial media is the availability of reference knowledge repositories containing\ncomprehensive representations of events and temporal relations. Existing\nknowledge graphs, with popular examples including DBpedia, YAGO and Wikidata,\nfocus mostly on entity-centric information and are insufficient in terms of\ntheir coverage and completeness with respect to events and temporal relations.\nEventKG presented in this paper is a multilingual event-centric temporal\nknowledge graph that addresses this gap. EventKG incorporates over 690 thousand\ncontemporary and historical events and over 2.3 million temporal relations\nextracted from several large-scale knowledge graphs and semi-structured sources\nand makes them available through a canonical representation.\n"]},
{"authors": ["Xiangnan Ren", "Olivier Cur\u00e9", "Hubert Naacke", "Guohui Xiao"], "title": ["BigSR: an empirical study of real-time expressive RDF stream reasoning\n  on modern Big Data platforms"], "date": ["2018-04-12T08:15:17Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1804.04367v1"], "summary": ["  The trade-off between language expressiveness and system scalability (E&S) is\na well-known problem in RDF stream reasoning. Higher expressiveness supports\nmore complex reasoning logic, however, it may also hinder system scalability.\nCurrent research mainly focuses on logical frameworks suitable for stream\nreasoning as well as the implementation and the evaluation of prototype\nsystems. These systems are normally developed in a centralized setting which\nsuffer from inherent limited scalability, while an in-depth study of applying\ndistributed solutions to cover E&S is still missing. In this paper, we aim to\nexplore the feasibility of applying modern distributed computing frameworks to\nmeet E&S all together. To do so, we first propose BigSR, a technical\ndemonstrator that supports a positive fragment of the LARS framework. For the\nsake of generality and to cover a wide variety of use cases, BigSR relies on\nthe two main execution models adopted by major distributed execution\nframeworks: Bulk Synchronous Processing (BSP) and Record-at-A-Time (RAT).\nAccordingly, we implement BigSR on top of Apache Spark Streaming (BSP model)\nand Apache Flink (RAT model). In order to conclude on the impacts of BSP and\nRAT on E&S, we analyze the ability of the two models to support distributed\nstream reasoning and identify several types of use cases characterized by their\nlevels of support. This classification allows for quantifying the E&S trade-off\nby assessing the scalability of each type of use case \\wrt its level of\nexpressiveness. Then, we conduct a series of experiments with 15 queries from 4\ndifferent datasets. Our experiments show that BigSR over both BSP and RAT\ngenerally scales up to high throughput beyond million-triples per second (with\nor without recursion), and RAT attains sub-millisecond delay for stateless\nquery operators.\n"]},
{"authors": ["Ramdoot Pydipaty", "Amit Saha"], "title": ["On Using Non-Volatile Memory in Apache Lucene"], "date": ["2018-04-12T06:39:28Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1804.04343v1"], "summary": ["  Apache Lucene is a widely popular information retrieval library used to\nprovide search functionality in an extremely wide variety of applications.\nNaturally, it has to efficiently index and search large number of documents.\nWith non-volatile memory in DIMM form factor (NVDIMM), software now has access\nto durable, byte-addressable memory with write latency within an order of\nmagnitude of DRAM write latency.\n  In this preliminary article, we present the first reported work on the impact\nof using NVDIMM on the performance of committing, searching, and near-real time\nsearching in Apache Lucene. We show modest improvements by using NVM but, our\nempirical study suggests that bigger impact requires redesigning Lucene to\naccess NVM as byte-addressable memory using loads and stores, instead of\naccessing NVM via the file system.\n"]},
{"authors": ["Houari Mahfoud"], "title": ["Graph Pattern Matching Preserving Label-Repetition Constraints"], "date": ["2018-04-12T00:04:05Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1804.04260v1"], "summary": ["  Graph pattern matching is a routine process for a wide variety of\napplications such as social network analysis. It is typically defined in terms\nof subgraph isomorphism which is NP-Complete. To lower its complexity, many\nextensions of graph simulation have been proposed which focus on some\ntopological constraints of pattern graphs that can be preserved in\npolynomial-time over data graphs. We discuss in this paper the satisfaction of\na new topological constraint, called Label-Repetition constraint. To the best\nof our knowledge, existing polynomial approaches fail to preserve this\nconstraint, and moreover, one can adopt only subgraph isomorphism for this end\nwhich is cost-prohibitive. We present first a necessary and sufficient\ncondition that a data subgraph must satisfy to preserve the Label-Repetition\nconstraints of the pattern graph. Furthermore, we define matching based on a\nnotion of triple simulation, an extension of graph simulation by considering\nthe new topological constraint. We show that with this extension, graph pattern\nmatching can be performed in polynomial-time, by providing such an algorithm.\nOur algorithm is sub-quadratic in the size of data graphs only, and quartic in\ngeneral. We show that our results can be combined with orthogonal approaches\nfor more expressive graph pattern matching.\n"]},
{"authors": ["Xiaofeng Yang", "Deepak Ajwani", "Wolfgang Gatterbauer", "Patrick K. Nicholson", "Mirek Riedewald", "Alessandra Sala"], "title": ["Any-k: Anytime Top-k Tree Pattern Retrieval in Labeled Graphs"], "date": ["2018-02-16T18:21:54Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.06060v3"], "summary": ["  Many problems in areas as diverse as recommendation systems, social network\nanalysis, semantic search, and distributed root cause analysis can be modeled\nas pattern search on labeled graphs (also called \"heterogeneous information\nnetworks\" or HINs). Given a large graph and a query pattern with node and edge\nlabel constraints, a fundamental challenge is to nd the top-k matches ac-\ncording to a ranking function over edge and node weights. For users, it is di\ncult to select value k . We therefore propose the novel notion of an any-k\nranking algorithm: for a given time budget, re- turn as many of the top-ranked\nresults as possible. Then, given additional time, produce the next lower-ranked\nresults quickly as well. It can be stopped anytime, but may have to continues\nuntil all results are returned. This paper focuses on acyclic patterns over\narbitrary labeled graphs. We are interested in practical algorithms that\neffectively exploit (1) properties of heterogeneous networks, in particular\nselective constraints on labels, and (2) that the users often explore only a\nfraction of the top-ranked results. Our solution, KARPET, carefully integrates\naggressive pruning that leverages the acyclic nature of the query, and\nincremental guided search. It enables us to prove strong non-trivial time and\nspace guarantees, which is generally considered very hard for this type of\ngraph search problem. Through experimental studies we show that KARPET achieves\nrunning times in the order of milliseconds for tree patterns on large networks\nwith millions of nodes and edges.\n"]},
{"authors": ["Yongjoo Park", "Barzan Mozafari", "Joseph Sorenson", "Junhao Wang"], "title": ["VerdictDB: Universalizing Approximate Query Processing"], "date": ["2018-04-03T00:33:34Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1804.00770v3"], "summary": ["  Despite 25 years of research in academia, approximate query processing (AQP)\nhas had little industrial adoption. One of the major causes of this slow\nadoption is the reluctance of traditional vendors to make radical changes to\ntheir legacy codebases, and the preoccupation of newer vendors (e.g.,\nSQL-on-Hadoop products) with implementing standard features. Additionally, the\nfew AQP engines that are available are each tied to a specific platform and\nrequire users to completely abandon their existing databases---an unrealistic\nexpectation given the infancy of the AQP technology. Therefore, we argue that a\nuniversal solution is needed: a database-agnostic approximation engine that\nwill widen the reach of this emerging technology across various platforms.\n  Our proposal, called VerdictDB, uses a middleware architecture that requires\nno changes to the backend database, and thus, can work with all off-the-shelf\nengines. Operating at the driver-level, VerdictDB intercepts analytical queries\nissued to the database and rewrites them into another query that, if executed\nby any standard relational engine, will yield sufficient information for\ncomputing an approximate answer. VerdictDB uses the returned result set to\ncompute an approximate answer and error estimates, which are then passed on to\nthe user or application. However, lack of access to the query execution layer\nintroduces significant challenges in terms of generality, correctness, and\nefficiency. This paper shows how VerdictDB overcomes these challenges and\ndelivers up to 171$\\times$ speedup (18.45$\\times$ on average) for a variety of\nexisting engines, such as Impala, Spark SQL, and Amazon Redshift, while\nincurring less than 2.6% relative error. VerdictDB is open-sourced under Apache\nLicense.\n"]},
{"authors": ["Marco Cavallo", "\u00c7a\u011fatay Demiralp"], "title": ["Clustrophile 2: Guided Visual Clustering Analysis"], "date": ["2018-04-09T15:05:56Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1804.03048v1"], "summary": ["  Data clustering is a common unsupervised learning method frequently used in\nexploratory data analysis. However, identifying relevant structures in\nunlabeled, high-dimensional data is nontrivial, requiring iterative\nexperimentation with clustering parameters as well as data features and\ninstances. The space of possible clusterings for a typical dataset is vast, and\nnavigating in this vast space is also challenging. The absence of ground-truth\nlabels makes it impossible to define an optimal solution, thus requiring user\njudgment to establish what can be considered a satisfiable clustering result.\nData scientists need adequate interactive tools to effectively explore and\nnavigate the large space of clusterings so as to improve the effectiveness of\nexploratory clustering analysis. We introduce \\textit{Clustrophile 2}, a new\ninteractive tool for guided clustering analysis. \\textit{Clustrophile 2} guides\nusers in clustering-based exploratory analysis, adapts user feedback to improve\nuser guidance, facilitates the interpretation of clusters, and helps quickly\nreason about differences between clusterings. To this end, \\textit{Clustrophile\n2} contributes a novel feature, the clustering tour, to help users choose\nclustering parameters and assess the quality of different clustering results in\nrelation to current analysis goals and user expectations. We evaluate\n\\textit{Clustrophile 2} through a user study with 12 data scientists, who used\nour tool to explore and interpret sub-cohorts in a dataset of Parkinson's\ndisease patients. Results suggest that \\textit{Clustrophile 2} improves the\nspeed and effectiveness of exploratory clustering analysis for both experts and\nnon-experts.\n"]},
{"authors": ["Christopher De Sa", "Ihab F. Ilyas", "Benny Kimelfeld", "Christopher Re", "Theodoros Rekatsinas"], "title": ["A Formal Framework For Probabilistic Unclean Databases"], "date": ["2018-01-21T01:56:07Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.06750v2"], "summary": ["  Most theoretical frameworks that focus on data errors and inconsistencies\nfollow logic-based reasoning. Yet, practical data cleaning tools need to\nincorporate statistical reasoning to be effective in real-world data cleaning\ntasks. Motivated by these empirical successes, we propose a formal framework\nfor unclean databases, where two types of statistical knowledge are\nincorporated: The first represents a belief of how intended (clean) data is\ngenerated, and the second represents a belief of how noise is introduced in the\nactual observed database instance. To capture this noisy channel model, we\nintroduce the concept of a Probabilistic Unclean Database (PUD), a triple that\nconsists of a probabilistic database that we call the intention, a\nprobabilistic data transformator that we call the realization and captures how\nnoise is introduced, and a dirty observed database instance that we call the\nobservation. We define three computational problems in the PUD framework:\ncleaning (infer the most probable clean instance given a PUD), probabilistic\nquery answering (compute the probability of an answer tuple over the unclean\nobserved instance), and learning (estimate the most likely intention and\nrealization models of a PUD given a collection of training data). We illustrate\nthe PUD framework on concrete representations of the intention and realization,\nshow that they generalize traditional concepts of repairs such as cardinality\nand value repairs, draw connection to consistent query answering, and prove\ntractability results. We further show that parameters can be learned in\npractical instantiations, and in fact, prove that under certain conditions we\ncan learn a PUD directly from a single dirty database instance without any need\nfor clean examples.\n"]},
{"authors": ["Ahmet Kara", "Hung Q. Ngo", "Milos Nikolic", "Dan Olteanu", "Haozhe Zhang"], "title": ["Counting Triangles under Updates in Worst-Case Optimal Time"], "date": ["2018-04-09T00:51:11Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1804.02780v1"], "summary": ["  We consider the problem of incrementally maintaining the triangle count query\nunder single- tuple updates to the input relations. We introduce an approach\nthat exhibits a space-time tradeoff such that the space-time product is\nquadratic in the size of the input database and the update time can be as low\nas the square root of this size. This lowest update time is worst-case optimal\nconditioned on the Online Matrix-Vector Multiplication conjecture.\n  The classical and factorized incremental view maintenance approaches are\nrecovered as special cases of our approach within the space-time tradeoff. In\nparticular, they require linear- time update maintenance, which is suboptimal.\nOur approach also recovers the worst-case optimal time complexity for computing\nthe triangle count in the non-incremental setting.\n"]},
{"authors": ["Philipp Eichmann", "Carsten Binnig", "Tim Kraska", "Emanuel Zgraggen"], "title": ["IDEBench: A Benchmark for Interactive Data Exploration"], "date": ["2018-04-07T21:23:16Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1804.02593v1"], "summary": ["  Existing benchmarks for analytical database systems such as TPC-DS and TPC-H\nare designed for static reporting scenarios. The main metric of these\nbenchmarks is the performance of running individual SQL queries over a\nsynthetic database. In this paper, we argue that such benchmarks are not\nsuitable for evaluating database workloads originating from interactive data\nexploration (IDE) systems where most queries are ad-hoc, not based on\npredefined reports, and built incrementally. As a main contribution, we present\na novel benchmark called IDEBench that can be used to evaluate the performance\nof database systems for IDE workloads. As opposed to traditional benchmarks for\nanalytical database systems, our goal is to provide more meaningful workloads\nand datasets that can be used to benchmark IDE query engines, with a particular\nfocus on metrics that capture the trade-off between query performance and\nquality of the result. As a second contribution, this paper evaluates and\ndiscusses the performance results of selected IDE query engines using our\nbenchmark. The study includes two commercial systems, as well as two research\nprototypes (IDEA, approXimateDB/XDB), and one traditional analytical database\nsystem (MonetDB).\n"]},
{"authors": ["Muhammad Ebraheem", "Saravanan Thirumuruganathan", "Shafiq Joty", "Mourad Ouzzani", "Nan Tang"], "title": ["DeepER -- Deep Entity Resolution"], "date": ["2017-10-02T12:02:58Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.00597v4"], "summary": ["  Entity resolution (ER) is a key data integration problem. Despite the efforts\nin 70+ years in all aspects of ER, there is still a high demand for\ndemocratizing ER - humans are heavily involved in labeling data, performing\nfeature engineering, tuning parameters, and defining blocking functions. With\nthe recent advances in deep learning, in particular distributed representation\nof words (a.k.a. word embeddings), we present a novel ER system, called DeepER,\nthat achieves good accuracy, high efficiency, as well as ease-of-use (i.e.,\nmuch less human efforts). For accuracy, we use sophisticated composition\nmethods, namely uni- and bi-directional recurrent neural networks (RNNs) with\nlong short term memory (LSTM) hidden units, to convert each tuple to a\ndistributed representation (i.e., a vector), which can in turn be used to\neffectively capture similarities between tuples. We consider both the case\nwhere pre-trained word embeddings are available as well the case where they are\nnot; we present ways to learn and tune the distributed representations. For\nefficiency, we propose a locality sensitive hashing (LSH) based blocking\napproach that uses distributed representations of tuples; it takes all\nattributes of a tuple into consideration and produces much smaller blocks,\ncompared with traditional methods that consider only a few attributes. For\nease-of-use, DeepER requires much less human labeled data and does not need\nfeature engineering, compared with traditional machine learning based\napproaches which require handcrafted features, and similarity functions along\nwith their associated thresholds. We evaluate our algorithms on multiple\ndatasets (including benchmarks, biomedical data, as well as multi-lingual data)\nand the extensive experimental results show that DeepER outperforms existing\nsolutions.\n"]},
{"authors": ["Habib Saissi", "Marco Serafini", "Neeraj Suri"], "title": ["Scaling Out Acid Applications with Operation Partitioning"], "date": ["2018-04-05T16:31:58Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1804.01942v1"], "summary": ["  OLTP applications with high workloads that cannot be served by a single\nserver need to scale out to multiple servers. Typically, scaling out entails\nassigning a different partition of the application state to each server. But\ndata partitioning is at odds with preserving the strong consistency guarantees\nof ACID transactions, a fundamental building block of many OLTP applications.\nThe more we scale out and spread data across multiple servers, the more\nfrequent distributed transactions accessing data at different servers will be.\nWith a large number of servers, the high cost of distributed transactions makes\nscaling out ineffective or even detrimental.\n  In this paper we propose Operation Partitioning, a novel paradigm to scale\nout OLTP applications that require ACID guarantees. Operation Partitioning\nindirectly partitions data across servers by partitioning the application's\noperations through static analysis. This partitioning of operations yields to a\nlock-free Conveyor Belt protocol for distributed coordination, which can scale\nout unmodified applications running on top of unmodified database management\nsystems. We implement the protocol in a system called Elia and use it to scale\nout two applications, TPC-W and RUBiS. Our experiments show that Elia can\nincrease maximum throughput by up to 4.2x and reduce latency by up to 58.6x\ncompared to MySQL Cluster while at the same time providing a stronger isolation\nguarantee (serializability instead of read committed).\n"]},
{"authors": ["Aarthy Shivram Arun", "Sai Vikneshwar Mani Jayaraman", "Christopher R\u00e9", "Atri Rudra"], "title": ["Hypertree Decompositions Revisited for PGMs"], "date": ["2018-04-05T01:05:34Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1804.01640v1"], "summary": ["  We revisit the classical problem of exact inference on probabilistic\ngraphical models (PGMs). Our algorithm is based on recent worst-case optimal\ndatabase join algorithms, which can be asymptotically faster than traditional\ndata processing methods. We present the first empirical evaluation of these new\nalgorithms via JoinInfer, a new exact inference engine. We empirically explore\nthe properties of the data for which our engine can be expected to outperform\ntraditional inference engines refining current theoretical notions. Further,\nJoinInfer outperforms existing state-of-the-art inference engines (ACE, IJGP\nand libDAI) on some standard benchmark datasets by up to a factor of 630x.\nFinally, we propose a promising data-driven heuristic that extends JoinInfer to\nautomatically tailor its parameters and/or switch to the traditional inference\nalgorithms.\n"]},
{"authors": ["Jianbin Qin", "Chuan Xiao"], "title": ["Pigeonring: A Principle for Faster Thresholded Similarity Search"], "date": ["2018-04-04T22:01:43Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1804.01614v1"], "summary": ["  The pigeonhole principle states that if n items are contained in m boxes,\nthen at least one box has no fewer than n/m items. It is utilized to solve many\ndata management problems, especially for thresholded similarity searches.\nDespite many pigeonhole principle-based solutions proposed in the last few\ndecades, the condition stated by the principle is weak. It only constrains the\nnumber of items in a single box. By organizing the boxes in a ring, we observe\nthat the number of items in multiple boxes are also constrained. We propose a\nnew principle called the pigeonring principle which formally captures such\nconstraints and yields stronger conditions. To utilize the pigeonring\nprinciple, we focus on problems defined in the form of identifying data objects\nwhose similarities or distances to the query is constrained by a threshold.\nMany solutions to these problems utilize the pigeonhole principle to find\ncandidates that satisfy a filtering condition. By the pigeonring principle,\nstronger filtering conditions can be established. We show that the pigeonhole\nprinciple is a special case of the pigeonring principle. This suggests that all\nthe solutions based on the pigeonhole principle are possible to be accelerated\nby the pigeonring principle. A universal filtering framework is introduced to\nencompass the solutions to these problems based on the pigeonring principle.\nBesides, we discuss how to quickly find candidates specified by the pigeonring\nprinciple with minor modifications on top of existing algorithms. Experimental\nresults on real datasets demonstrate the applicability of the pigeonring\nprinciple as well as the superior performance of the algorithms based on the\nprinciple.\n"]},
{"authors": ["Leopoldo Bertossi"], "title": ["Characterizing and Computing Causes for Query Answers in Databases from\n  Database Repairs and Repair Programs"], "date": ["2017-12-04T11:00:38Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1712.01001v3"], "summary": ["  A correspondence between database tuples as causes for query answers in\ndatabases and tuple-based repairs of inconsistent databases with respect to\ndenial constraints has already been established. In this work, answer-set\nprograms that specify repairs of databases are used as a basis for solving\ncomputational and reasoning problems about causes. Here, causes are also\nintroduced at the attribute level by appealing to a both null-based and\nattribute-based repair semantics. The corresponding repair programs are\npresented, and they are used as a basis for computation and reasoning about\nattribute-level causes.\n"]},
{"authors": ["Yang Cao", "Masatoshi Yoshikawa", "Yonghui Xiao", "Li Xiong"], "title": ["Quantifying Differential Privacy in Continuous Data Release under\n  Temporal Correlations"], "date": ["2017-11-29T01:25:05Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1711.11436v2"], "summary": ["  Differential Privacy (DP) has received increasing attention as a rigorous\nprivacy framework. Many existing studies employ traditional DP mechanisms\n(e.g., the Laplace mechanism) as primitives to continuously release private\ndata for protecting privacy at each time point (i.e., event-level privacy),\nwhich assume that the data at different time points are independent, or that\nadversaries do not have knowledge of correlation between data. However,\ncontinuously generated data tend to be temporally correlated, and such\ncorrelations can be acquired by adversaries. In this paper, we investigate the\npotential privacy loss of a traditional DP mechanism under temporal\ncorrelations. First, we analyze the privacy leakage of a DP mechanism under\ntemporal correlation that can be modeled using Markov Chain. Our analysis\nreveals that, the event-level privacy loss of a DP mechanism may\n\\textit{increase over time}. We call the unexpected privacy loss\n\\textit{temporal privacy leakage} (TPL). Although TPL may increase over time,\nwe find that its supremum may exist in some cases. Second, we design efficient\nalgorithms for calculating TPL. Third, we propose data releasing mechanisms\nthat convert any existing DP mechanism into one against TPL. Experiments\nconfirm that our approach is efficient and effective.\n"]},
{"authors": ["Manuel Namici"], "title": ["R2RML Mappings in OBDA Systems: Enabling Comparison among OBDA Tools"], "date": ["2018-04-04T13:43:21Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1804.01405v1"], "summary": ["  In today's large enterprises there is a significant increasing trend in the\namount of data that has to be stored and processed. To complicate this scenario\nthe complexity of organizing and managing a large collection of data,\nstructured according to a single, unified schema, makes so that there is almost\nnever a single place where to look to satisfy an information need.\n  The Ontology-Based Data Access (OBDA) paradigm aims at mitigating this\nphenomenon by providing to the users of the system a unified and shared\nconceptual view of the domain of interest (ontology), while still enabling the\ndata to be stored in different data sources, which are managed by a relational\ndatabase. In an OBDA system the link between the data stored at the sources and\nthe ontology is provided through a declarative specification given in terms of\na set of mappings.\n  In this work we focus on comparing two of the available systems for OBDA,\nnamely, Mastro and Ontop, by adopting OBDA specifications based on W3C\nrecommendations. We first show how support for R2RML mappings has been\nintegrated in Mastro, which was the last feature missing in order to enable the\nsystem to use specifications based solely on W3C recommendations relevant to\nOBDA. We then proceed in performing a comparison between these systems over two\nOBDA specifications, the NPD Benchmark and the ACI specification.\n"]},
{"authors": ["Thomas Guyet", "Ren\u00e9 Quiniou"], "title": ["NegPSpan: efficient extraction of negative sequential patterns with\n  embedding constraints"], "date": ["2018-04-04T06:47:32Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1804.01256v1"], "summary": ["  Mining frequent sequential patterns consists in extracting recurrent\nbehaviors, modeled as patterns, in a big sequence dataset. Such patterns inform\nabout which events are frequently observed in sequences, i.e. what does really\nhappen. Sometimes, knowing that some specific event does not happen is more\ninformative than extracting a lot of observed events. Negative sequential\npatterns (NSP) formulate recurrent behaviors by patterns containing both\nobserved events and absent events. Few approaches have been proposed to mine\nsuch NSPs. In addition, the syntax and semantics of NSPs differ in the\ndifferent methods which makes it difficult to compare them. This article\nprovides a unified framework for the formulation of the syntax and the\nsemantics of NSPs. Then, we introduce a new algorithm, NegPSpan, that extracts\nNSPs using a PrefixSpan depth-first scheme and enabling maxgap constraints that\nother approaches do not take into account. The formal framework allows for\nhighlighting the differences between the proposed approach wrt to the methods\nfrom the literature, especially wrt the state of the art approach eNSP.\nIntensive experiments on synthetic and real datasets show that NegPSpan can\nextract meaningful NSPs and that it can process bigger datasets than eNSP\nthanks to significantly lower memory requirements and better computation times.\n"]},
{"authors": ["Marc Shapiro", "Pierre Sutra"], "title": ["Database Consistency Models"], "date": ["2018-04-03T11:33:18Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1804.00914v1"], "summary": ["  A data store allows application processes to put and get data from a shared\nmemory. In general, a data store cannot be modelled as a strictly sequential\nprocess. Applications observe non-sequential behaviours, called anomalies. The\nset of pos- sible behaviours, and conversely of possible anomalies, constitutes\nthe consistency model of the data store.\n"]},
{"authors": ["Hung Dang", "Anh Dinh", "Ee-Chien Chang", "Beng Chin Ooi"], "title": ["Chain of Trust: Can Trusted Hardware Help Scaling Blockchains?"], "date": ["2018-04-02T05:33:18Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1804.00399v2"], "summary": ["  As blockchain systems proliferate, there remains an unresolved scalability\nproblem of their underlying distributed consensus protocols. Byzantine Fault\nTolerance (BFT) consensus protocols offer high transaction throughput, but can\nonly support small networks. Proof-of-Work (PoW) consensus protocol, on the\nother hand, can support thousands of nodes, but at the expense of poor\ntransaction throughput. Two potential approaches to address these scalability\nbarriers are by relaxing the threat model, or employing sharding technique to\ndeal with large networks. Nonetheless, their effectiveness against\ndata-intensive blockchain workloads remains to be seen.\n  In this work, we study the use and effectiveness of trusted hardware on\nscaling distributed consensus protocols, and by their extension, blockchain\nsystems. We first analyze existing approaches that harness trusted hardware to\nenhance scalability, and identify their limitations. Drawing insights from\nthese results, we propose two design principles, namely scale up by scaling\ndown and prioritize consensus messages, that enable the consensus protocols to\nscale. We illustrate the two principles by presenting optimizations that\nimprove upon state-of-the-art solutions, and demonstrate via our extensive\nevaluations that they indeed offer better scalability. In particular,\nintegrating our optimizations into Hyperledger Fabric achieves up to 7x higher\nthroughput, while enabling the system to remain operational as the network size\nincreases. Another optimization that we introduce to Hyperledger Sawtooth\nallows the system to sustain high throughput as the network grows. Finally, our\nnew design for sharding protocols reduces the cost of shard creation phase by\nup to 35x.\n"]},
{"authors": ["Jing Zhong", "Roy D. Yates", "Emina Soljanin"], "title": ["Minimizing Content Staleness in Dynamo-Style Replicated Storage Systems"], "date": ["2018-04-02T21:45:43Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1804.00742v1"], "summary": ["  Consistency in data storage systems requires any read operation to return the\nmost recent written version of the content. In replicated storage systems,\nconsistency comes at the price of delay due to large-scale write and read\noperations. Many applications with low latency requirements tolerate data\nstaleness in order to provide high availability and low operation latency.\nUsing age of information as the staleness metric, we examine a data updating\nsystem in which real-time content updates are replicated and stored in a\nDynamo-style quorum- based distributed system. A source sends updates to all\nthe nodes in the system and waits for acknowledgements from the earliest subset\nof nodes, known as a write quorum. An interested client fetches the update from\nanother set of nodes, defined as a read quorum. We analyze the staleness-delay\ntradeoff in replicated storage by varying the write quorum size. With a larger\nwrite quorum, an instantaneous read is more likely to get the latest update\nwritten by the source. However, the age of the content written to the system is\nmore likely to become stale as the write quorum size increases. For shifted\nexponential distributed write delay, we derive the age optimized write quorum\nsize that balances the likelihood of reading the latest update and the\nfreshness of the latest update written by the source.\n"]},
{"authors": ["Mohammed Eunus Ali", "Kaysar Abdullah", "Shadman Saqib Eusuf", "Farhana M. Choudhury", "J. Shane Culpepper", "Timos Sellis"], "title": ["The Maximum Trajectory Coverage Query in Spatial Databases"], "date": ["2018-04-02T15:51:22Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1804.00599v1"], "summary": ["  With the widespread use of GPS-enabled mobile devices, an unprecedented\namount of trajectory data is becoming available from various sources such as\nBikely, GPS-wayPoints, and Uber. The rise of innovative transportation services\nand recent break-throughs in autonomous vehicles will lead to the continued\ngrowth of trajectory data and related applications. Supporting these services\nin emerging platforms will require more efficient query processing in\ntrajectory databases. In this paper, we propose two new coverage queries for\ntrajectory databases: (i) k Maximizing Reverse Range Search on Trajectories\n(kMaxRRST); and (ii) a Maximum k Coverage Range Search on Trajectories\n(MaxkCovRST). We propose a novel index structure, the Trajectory Quadtree\n(TQ-tree) that utilizes a quadtree to hierarchically organize trajectories into\ndifferent quadtree nodes, and then applies a z-ordering to further organize the\ntrajectories by spatial locality inside each node. This structure is highly\neffective in pruning the trajectory search space, which is of independent\ninterest. By exploiting the TQ-tree data structure, we develop a\ndivide-and-conquer approach to compute the trajectory \"service value\", and a\nbest-first strategy to explore the trajectories using the appropriate upper\nbound on the service value to efficiently process a kMaxRRST query. Moreover,\nto solve the MaxkCovRST, which is a non-submodular NP-hard problem, we propose\na greedy approximation which also exploits the TQ-tree. We evaluate our\nalgorithms through an extensive experimental study on several real datasets,\nand demonstrate that our TQ-tree based algorithms outperform common baselines\nby two to three orders of magnitude.\n"]},
{"authors": ["Xi Zheng"], "title": ["Database as a Service - Current Issues and Its Future"], "date": ["2018-04-02T12:08:01Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1804.00465v1"], "summary": ["  With the prevalence of applications in cloud, Database as a Service (DBaaS)\nbecomes a promising method to provide cloud applications with reliable and\nflexible data storage services. It provides a number of interesting features to\ncloud developers, however, it suffers a few drawbacks: long learning curve and\ndevelopment cycle, lacking of in-depth support for NoSQL, lacking of flexible\nconfiguration for security and privacy, and high cost models. In this paper, we\ninvestigate these issues among current DBaaS providers and propose a novel\nTrinity Model that can significantly reduce the learning curves, improve the\nsecurity and privacy, and accelerate database design and development. We\nfurther elaborate our ongoing and future work on developing large real-world\nSaaS projects using this new DBaaS model.\n"]},
{"authors": ["Egor V. Kostylev", "Dan Suciu"], "title": ["A Note on the Hardness of the Critical Tuple Problem"], "date": ["2018-04-02T10:22:51Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1804.00443v1"], "summary": ["  The notion of critical tuple was introduced by Miklau and Suciu (Gerome\nMiklau and Dan Suciu. A formal analysis of information disclosure in data\nexchange. J. Comput. Syst. Sci., 73(3):507-534, 2007), who also claimed that\nthe problem of checking whether a tuple is non-critical is complete for the\nsecond level of the polynomial hierarchy. Kostylev identified an error in the\n12-page-long hardness proof. It turns out that the issue is rather fundamental:\nthe proof can be adapted to show hardness of a relative variant of\ntuple-non-criticality, but we have neither been able to prove the original\nclaim nor found an algorithm for it of lower complexity. In this note we state\nformally the relative variant and present an alternative, simplified proof of\nits hardness; we also give an NP-hardness proof for the original problem, the\nbest lower bound we have been able to show. Hence, the precise complexity of\nthe original critical tuple problem remains open.\n"]},
{"authors": ["Zhaoqiang Chen", "Qun Chen", "Fengfeng Fan", "Yanyan Wang", "Zhuo Wang", "Youcef Nafa", "Zhanhuai Li", "Hailong Liu", "Wei Pan"], "title": ["Enabling Quality Control for Entity Resolution: A Human and Machine\n  Cooperation Framework"], "date": ["2017-09-30T14:18:24Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.00204v2"], "summary": ["  Even though many machine algorithms have been proposed for entity resolution,\nit remains very challenging to find a solution with quality guarantees. In this\npaper, we propose a novel HUman and Machine cOoperation (HUMO) framework for\nentity resolution (ER), which divides an ER workload between the machine and\nthe human. HUMO enables a mechanism for quality control that can flexibly\nenforce both precision and recall levels. We introduce the optimization problem\nof HUMO, minimizing human cost given a quality requirement, and then present\nthree optimization approaches: a conservative baseline one purely based on the\nmonotonicity assumption of precision, a more aggressive one based on sampling\nand a hybrid one that can take advantage of the strengths of both previous\napproaches. Finally, we demonstrate by extensive experiments on real and\nsynthetic datasets that HUMO can achieve high-quality results with reasonable\nreturn on investment (ROI) in terms of human cost, and it performs considerably\nbetter than the state-of-the-art alternatives in quality control.\n"]},
{"authors": ["Prasetya Utama", "Nathaniel Weir", "Fuat Basik", "Carsten Binnig", "Ugur Cetintemel", "Benjamin H\u00e4ttasch", "Amir Ilkhechi", "Shekar Ramaswamy", "Arif Usta"], "title": ["An End-to-end Neural Natural Language Interface for Databases"], "date": ["2018-04-02T05:36:38Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1804.00401v1"], "summary": ["  The ability to extract insights from new data sets is critical for decision\nmaking. Visual interactive tools play an important role in data exploration\nsince they provide non-technical users with an effective way to visually\ncompose queries and comprehend the results. Natural language has recently\ngained traction as an alternative query interface to databases with the\npotential to enable non-expert users to formulate complex questions and\ninformation needs efficiently and effectively. However, understanding natural\nlanguage questions and translating them accurately to SQL is a challenging\ntask, and thus Natural Language Interfaces for Databases (NLIDBs) have not yet\nmade their way into practical tools and commercial products.\n  In this paper, we present DBPal, a novel data exploration tool with a natural\nlanguage interface. DBPal leverages recent advances in deep models to make\nquery understanding more robust in the following ways: First, DBPal uses a deep\nmodel to translate natural language statements to SQL, making the translation\nprocess more robust to paraphrasing and other linguistic variations. Second, to\nsupport the users in phrasing questions without knowing the database schema and\nthe query features, DBPal provides a learned auto-completion model that\nsuggests partial query extensions to users during query formulation and thus\nhelps to write complex queries.\n"]},
{"authors": ["Yu-Hsuan Kuo", "Cho-Chun Chiu", "Daniel Kifer", "Michael Hay", "Ashwin Machanavajjhala"], "title": ["Differentially Private Hierarchical Group Size Estimation"], "date": ["2018-04-02T01:51:10Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1804.00370v1"], "summary": ["  Consider the problem of estimating, for every integer j, the number of\nhouseholds with j people in them, while protecting the privacy of individuals.\nAdd in a geographical component, so that the household size distribution can be\ncompared at the national, state, and county levels. This is an instance of the\nprivate hierarchical group size estimation problem, in which each group is\nassociated with a size and a hierarchical attribute. In this paper, we\nintroduce this problem, along with appropriate error metrics and propose a\ndifferentially private solution that generates group size estimates that are\nconsistent across all levels of the hierarchy.\n"]},
{"authors": ["Ashish Tapdiya", "Daniel Fabbri"], "title": ["A comparative analysis of state-of-the-art SQL-on-Hadoop systems for\n  interactive analytics"], "date": ["2018-03-31T23:16:01Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1804.00224v1"], "summary": ["  Hadoop is emerging as the primary data hub in enterprises, and SQL represents\nthe de facto language for data analysis. This combination has led to the\ndevelopment of a variety of SQL-on-Hadoop systems in use today. While the\nvarious SQL-on-Hadoop systems target the same class of analytical workloads,\ntheir different architectures, design decisions and implementations impact\nquery performance. In this work, we perform a comparative analysis of four\nstate-of-the-art SQL-on-Hadoop systems (Impala, Drill, Spark SQL and Phoenix)\nusing the Web Data Analytics micro benchmark and the TPC-H benchmark on the\nAmazon EC2 cloud platform. The TPC-H experiment results show that, although\nImpala outperforms other systems (4.41x - 6.65x) in the text format, trade-offs\nexists in the parquet format, with each system performing best on subsets of\nqueries. A comprehensive analysis of execution profiles expands upon the\nperformance results to provide insights into performance variations,\nperformance bottlenecks and query execution characteristics.\n"]},
{"authors": ["Guna Prasaad", "G. Ramalingam", "Kaushik Rajan"], "title": ["Scaling Ordered Stream Processing on Shared-Memory Multicores"], "date": ["2018-03-30T03:50:08Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1803.11328v1"], "summary": ["  Many modern applications require real-time processing of large volumes of\nhigh-speed data. Such data processing needs can be modeled as a streaming\ncomputation. A streaming computation is specified as a dataflow graph that\nexposes multiple opportunities for parallelizing its execution, in the form of\ndata, pipeline and task parallelism. On the other hand, many important\napplications require that processing of the stream be ordered, where inputs are\nprocessed in the same order as they arrive. There is a fundamental conflict\nbetween ordered processing and parallelizing the streaming computation. This\npaper focuses on the problem of effectively parallelizing ordered streaming\ncomputations on a shared-memory multicore machine.\n  We first address the key challenges in exploiting data parallelism in the\nordered setting. We present a low-latency, non-blocking concurrent data\nstructure to order outputs produced by concurrent workers on an operator. We\nalso propose a new approach to parallelizing partitioned stateful operators\nthat can handle load imbalance across partitions effectively and mostly avoid\ndelays due to ordering. We illustrate the trade-offs and effectiveness of our\nconcurrent data-structures on micro-benchmarks and streaming queries from the\nTPCx-BB benchmark. We then present an adaptive runtime that dynamically maps\nthe exposed parallelism in the computation to that of the machine. We propose\nseveral intuitive scheduling heuristics and compare them empirically on the\nTPCx-BB queries. We find that for streaming computations, heuristics that\nexploit as much pipeline parallelism as possible perform better than those that\nseek to exploit data parallelism.\n"]},
{"authors": ["Ran M. Bittmann", "Philippe Nemery", "Xingtian Shi", "Michael Kemelmakher", "Mengjiao Wang"], "title": ["Frequent Item-set Mining without Ubiquitous Items"], "date": ["2018-03-29T14:52:33Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1803.11105v1"], "summary": ["  Frequent Item-set Mining (FIM), sometimes called Market Basket Analysis (MBA)\nor Association Rule Learning (ARL), are Machine Learning (ML) methods for\ncreating rules from datasets of transactions of items. Most methods identify\nitems likely to appear together in a transaction based on the support (i.e. a\nminimum number of relative co-occurrence of the items) for that hypothesis.\nAlthough this is a good indicator to measure the relevance of the assumption\nthat these items are likely to appear together, the phenomenon of very frequent\nitems, referred to as ubiquitous items, is not addressed in most algorithms.\nUbiquitous items have the same entropy as infrequent items, and not\ncontributing significantly to the knowledge. On the other hand, they have\nstrong effect on the performance of the algorithms and sometimes preventing the\nconvergence of the FIM algorithms and thus the provision of meaningful results.\nThis paper discusses the phenomenon of ubiquitous items and demonstrates how\nignoring these has a dramatic effect on the computation performances but with a\nlow and controlled effect on the significance of the results.\n"]},
{"authors": ["Bikram Karmakar", "Indranil Mukhopadhyay"], "title": ["Statistical Validity and Consistency of Big Data Analytics: A General\n  Framework"], "date": ["2018-03-29T02:15:03Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1803.10901v1"], "summary": ["  Informatics and technological advancements have triggered generation of huge\nvolume of data with varied complexity in its management and analysis. Big Data\nanalytics is the practice of revealing hidden aspects of such data and making\ninferences from it. Although storage, retrieval and management of Big Data seem\npossible through efficient algorithm and system development, concern about\nstatistical consistency remains to be addressed in view of its specific\ncharacteristics. Since Big Data does not conform to standard analytics, we need\nproper modification of the existing statistical theory and tools. Here we\npropose, with illustrations, a general statistical framework and an algorithmic\nprinciple for Big Data analytics that ensure statistical accuracy of the\nconclusions. The proposed framework has the potential to push forward\nadvancement of Big Data analytics in the right direction. The\npartition-repetition approach proposed here is broad enough to encompass all\npractical data analytic problems.\n"]},
{"authors": ["Yun William Yu", "Griffin M. Weber"], "title": ["HyperMinHash: MinHash in LogLog space"], "date": ["2017-10-23T18:02:16Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.08436v3"], "summary": ["  In this extended abstract, we describe and analyse a streaming probabilistic\nsketch, HYPERMINHASH, to estimate the Jaccard index (or Jaccard similarity\ncoefficient) over two sets $A$ and $B$. HyperMinHash can be thought of as a\ncompression of standard $\\log n$-space MinHash by building off of a HyperLogLog\ncount-distinct sketch. For a multiplicative approximation error $1+ \\epsilon$\non a Jaccard index $ t $, given a random oracle, HyperMinHash needs\n$O\\left(\\epsilon^{-2} \\left( \\log\\log n + \\log \\frac{1}{ t \\epsilon}\n\\right)\\right)$ space. Unlike comparable Jaccard index fingerprinting\nalgorithms (such as b-bit MinHash, which uses less space), HyperMinHash retains\nMinHash's features of streaming updates, unions, and cardinality estimation.\nOur new algorithm allows estimating Jaccard indices of 0.01 for set\ncardinalities on the order of $10^{19}$ with relative error of around 10\\%\nusing 64KiB of memory; MinHash can only estimate Jaccard indices for\ncardinalities of $10^{10}$ with the same memory consumption. Note that we will\noperate in the unbounded data stream model and assume both a random oracle and\nshared randomness.\n"]},
{"authors": ["Doris Xin", "Litian Ma", "Shuchen Song", "Aditya Parameswaran"], "title": ["How Developers Iterate on Machine Learning Workflows -- A Survey of the\n  Applied Machine Learning Literature"], "date": ["2018-03-27T20:38:05Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1803.10311v1"], "summary": ["  Machine learning workflow development is anecdotally regarded to be an\niterative process of trial-and-error with humans-in-the-loop. However, we are\nnot aware of quantitative evidence corroborating this popular belief. A\nquantitative characterization of iteration can serve as a benchmark for machine\nlearning workflow development in practice, and can aid the development of\nhuman-in-the-loop machine learning systems. To this end, we conduct a\nsmall-scale survey of the applied machine learning literature from five\ndistinct application domains. We collect and distill statistics on the role of\niteration within machine learning workflow development, and report preliminary\ntrends and insights from our investigation, as a starting point towards this\nbenchmark. Based on our findings, we finally describe desiderata for effective\nand versatile human-in-the-loop machine learning systems that can cater to\nusers in diverse domains.\n"]},
{"authors": ["Miguel Romero"], "title": ["The tractability frontier of well-designed SPARQL queries"], "date": ["2017-12-23T17:23:08Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1712.08809v3"], "summary": ["  We study the complexity of query evaluation of SPARQL queries. We focus on\nthe fundamental fragment of well-designed SPARQL restricted to the AND,\nOPTIONAL and UNION operators. Our main result is a structural characterisation\nof the classes of well-designed queries that can be evaluated in polynomial\ntime. In particular, we introduce a new notion of width called domination\nwidth, which relies on the well-known notion of treewidth. We show that, under\nsome complexity theoretic assumptions, the classes of well-designed queries\nthat can be evaluated in polynomial time are precisely those of bounded\ndomination width.\n"]},
{"authors": ["Hung Q. Ngo"], "title": ["Worst-Case Optimal Join Algorithms: Techniques, Results, and Open\n  Problems"], "date": ["2018-03-27T07:13:49Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1803.09930v1"], "summary": ["  Worst-case optimal join algorithms are the class of join algorithms whose\nruntime match the worst-case output size of a given join query. While the first\nprovably worse-case optimal join algorithm was discovered relatively recently,\nthe techniques and results surrounding these algorithms grow out of decades of\nresearch from a wide range of areas, intimately connecting graph theory,\nalgorithms, information theory, constraint satisfaction, database theory, and\ngeometric inequalities. These ideas are not just paperware: in addition to\nacademic project implementations, two variations of such algorithms are the\nwork-horse join algorithms of commercial database and data analytics engines.\n  This paper aims to be a brief introduction to the design and analysis of\nworst-case optimal join algorithms. We discuss the key techniques for proving\nruntime and output size bounds. We particularly focus on the fascinating\nconnection between join algorithms and information theoretic inequalities, and\nthe idea of how one can turn a proof into an algorithm. Finally, we conclude\nwith a representative list of fundamental open problems in this area.\n"]},
{"authors": ["Shaleen Deep", "Paraschos Koutris"], "title": ["Compressed Representations of Conjunctive Query Results"], "date": ["2017-09-18T22:09:24Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.06186v3"], "summary": ["  Relational queries, and in particular join queries, often generate large\noutput results when executed over a huge dataset. In such cases, it is often\ninfeasible to store the whole materialized output if we plan to reuse it\nfurther down a data processing pipeline. Motivated by this problem, we study\nthe construction of space-efficient compressed representations of the output of\nconjunctive queries, with the goal of supporting the efficient access of the\nintermediate compressed result for a given access pattern. In particular, we\ninitiate the study of an important tradeoff: minimizing the space necessary to\nstore the compressed result, versus minimizing the answer time and delay for an\naccess request over the result. Our main contribution is a novel parameterized\ndata structure, which can be tuned to trade off space for answer time. The\ntradeoff allows us to control the space requirement of the data structure\nprecisely, and depends both on the structure of the query and the access\npattern. We show how we can use the data structure in conjunction with query\ndecomposition techniques, in order to efficiently represent the outputs for\nseveral classes of conjunctive queries.\n"]},
{"authors": ["Kexin Rong", "Clara E. Yoon", "Karianne J. Bergen", "Hashem Elezabi", "Peter Bailis", "Philip Levis", "Gregory C. Beroza"], "title": ["Locality-Sensitive Hashing for Earthquake Detection: A Case Study\n  Scaling Data-Driven Science"], "date": ["2018-03-26T20:43:25Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1803.09835v1"], "summary": ["  In this work, we report on a novel application of Locality Sensitive Hashing\n(LSH) to seismic data at scale. Based on the high waveform similarity between\nreoccurring earthquakes, our application identifies potential earthquakes by\nsearching for similar time series segments via LSH. However, a straightforward\nimplementation of this LSH-enabled application has difficulty scaling beyond 3\nmonths of continuous time series data measured at a single seismic station. As\na case study of a data-driven science workflow, we illustrate how domain\nknowledge can be incorporated into the workload to improve both the efficiency\nand result quality. We describe several end-to-end optimizations of the\nanalysis pipeline from pre-processing to post-processing, which allow the\napplication to scale to time series data measured at multiple seismic stations.\nOur optimizations enable an over 100x speed up in the end-to-end analysis\npipeline. This improved scalability enabled seismologists to perform seismic\nanalysis on more than ten years of continuous time series data from over ten\nseismic stations, and has directly enabled the discovery of 597 new earthquakes\nnear the Diablo Canyon nuclear power plant in California and 6123 new\nearthquakes in New Zealand.\n"]},
{"authors": ["Yihan Sun", "Daniel Ferizovic", "Guy E. Blelloch"], "title": ["PAM: Parallel Augmented Maps"], "date": ["2016-12-16T22:02:49Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1612.05665v3"], "summary": ["  Ordered (key-value) maps are an important and widely-used data type for\nlarge-scale data processing frameworks. Beyond simple search, insertion and\ndeletion, more advanced operations such as range extraction, filtering, and\nbulk updates form a critical part of these frameworks.\n  We describe an interface for ordered maps that is augmented to support fast\nrange queries and sums, and introduce a parallel and concurrent library called\nPAM (Parallel Augmented Maps) that implements the interface. The interface\nincludes a wide variety of functions on augmented maps ranging from basic\ninsertion and deletion to more interesting functions such as union,\nintersection, filtering, extracting ranges, splitting, and range-sums. We\ndescribe algorithms for these functions that are efficient both in theory and\npractice.\n  As examples of the use of the interface and the performance of PAM, we apply\nthe library to four applications: simple range sums, interval trees, 2D range\ntrees, and ranked word index searching. The interface greatly simplifies the\nimplementation of these data structures over direct implementations.\nSequentially the code achieves performance that matches or exceeds existing\nlibraries designed specially for a single application, and in parallel our\nimplementation gets speedups ranging from 40 to 90 on 72 cores with 2-way\nhyperthreading.\n"]},
{"authors": ["Alexander J. Titus", "Audrey Flower", "Patrick Hagerty", "Paul Gamble", "Charlie Lewis", "Todd Stavish", "Kevin P. OConnell", "Greg Shipley", "Stephanie M. Rogers"], "title": ["SIG-DB: leveraging homomorphic encryption to Securely Interrogate\n  privately held Genomic DataBases"], "date": ["2018-03-26T13:09:12Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1803.09565v1"], "summary": ["  Genomic data are becoming increasingly valuable as we develop methods to\nutilize the information at scale and gain a greater understanding of how\ngenetic information relates to biological function. Advances in synthetic\nbiology and the decreased cost of sequencing are increasing the amount of\nprivately held genomic data. As the quantity and value of private genomic data\ngrows, so does the incentive to acquire and protect such data, which creates a\nneed to store and process these data securely. We present an algorithm for the\nSecure Interrogation of Genomic DataBases (SIG-DB). The SIG-DB algorithm\nenables databases of genomic sequences to be searched with an encrypted query\nsequence without revealing the query sequence to the Database Owner or any of\nthe database sequences to the Querier. SIG-DB is the first application of its\nkind to take advantage of locality-sensitive hashing and homomorphic encryption\nto allow generalized sequence-to-sequence comparisons of genomic data.\n"]},
{"authors": ["Grigory Yaroslavtsev", "Adithya Vadapalli"], "title": ["Massively Parallel Algorithms and Hardness for Single-Linkage Clustering\n  Under $\\ell_p$-Distances"], "date": ["2017-10-04T00:48:54Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.01431v2"], "summary": ["  We present massively parallel (MPC) algorithms and hardness of approximation\nresults for computing Single-Linkage Clustering of $n$ input $d$-dimensional\nvectors under Hamming, $\\ell_1, \\ell_2$ and $\\ell_\\infty$ distances. All our\nalgorithms run in $O(\\log n)$ rounds of MPC for any fixed $d$ and achieve\n$(1+\\epsilon)$-approximation for all distances (except Hamming for which we\nshow an exact algorithm). We also show constant-factor inapproximability\nresults for $o(\\log n)$-round algorithms under standard MPC hardness\nassumptions (for sufficiently large dimension depending on the distance used).\nEfficiency of implementation of our algorithms in Apache Spark is demonstrated\nthrough experiments on a variety of datasets exhibiting speedups of several\norders of magnitude.\n"]},
{"authors": ["An Yan", "Nicholas Weber"], "title": ["Mining Open Government Data Used in Scientific Research"], "date": ["2018-02-08T23:18:44Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.03074v3"], "summary": ["  In the following paper, we describe results from mining citations, mentions,\nand links to open government data (OGD) in peer-reviewed literature. We\ninductively develop a method for categorizing how OGD are used by different\nresearch communities, and provide descriptive statistics about the publication\nyears, publication outlets, and OGD sources. Our results demonstrate that, 1.\nThe use of OGD in research is steadily increasing from 2009 to 2016; 2.\nResearchers use OGD from 96 different open government data portals, with\ndata.gov.uk and data.gov being the most frequent sources; and, 3.Contrary to\nprevious findings, we provide evidence suggesting that OGD from developing\nnations, notably India and Kenya, are being frequently used to fuel scientific\ndiscoveries. The findings of this paper contribute to ongoing research agendas\naimed at tracking the impact of open government data initiatives, and provides\nan initial description of how open government data are valuable to diverse\nscientific research communities.\n"]},
{"authors": ["Timnit Gebru", "Jamie Morgenstern", "Briana Vecchione", "Jennifer Wortman Vaughan", "Hanna Wallach", "Hal Daume\u00e9 III", "Kate Crawford"], "title": ["Datasheets for Datasets"], "date": ["2018-03-23T23:22:18Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1803.09010v1"], "summary": ["  Currently there is no standard way to identify how a dataset was created, and\nwhat characteristics, motivations, and potential skews it represents. To begin\nto address this issue, we propose the concept of a datasheet for datasets, a\nshort document to accompany public datasets, commercial APIs, and pretrained\nmodels. The goal of this proposal is to enable better communication between\ndataset creators and users, and help the AI community move toward greater\ntransparency and accountability. By analogy, in computer hardware, it has\nbecome industry standard to accompany everything from the simplest components\n(e.g., resistors), to the most complex microprocessor chips, with datasheets\ndetailing standard operating characteristics, test results, recommended usage,\nand other information. We outline some of the questions a datasheet for\ndatasets should answer. These questions focus on when, where, and how the\ntraining data was gathered, its recommended use cases, and, in the case of\nhuman-centric datasets, information regarding the subjects' demographics and\nconsent as applicable. We develop prototypes of datasheets for two well-known\ndatasets: Labeled Faces in The Wild~\\cite{lfw} and the Pang \\& Lee Polarity\nDataset~\\cite{polarity}.\n"]},
{"authors": ["Anna C. Gilbert", "Audra McMillan"], "title": ["Local Differential Privacy for Physical Sensor Data and Sparse Recovery"], "date": ["2017-05-31T03:15:22Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1706.05916v4"], "summary": ["  In this work we explore the utility of locally differentially private thermal\nsensor data. We design a locally differentially private recovery algorithm for\nthe 1-dimensional, discrete heat source location problem and analyse its\nperformance in terms of the Earth Mover Distance error. Our work indicates that\nit is possible to produce locally private sensor measurements that both keep\nthe exact locations of the heat sources private and permit recovery of the\n\"general geographic vicinity\" of the sources. We also discuss the relationship\nbetween the property of an inverse problem being ill-conditioned and the amount\nof noise needed to maintain privacy.\n"]},
{"authors": ["Thomas Hartmann", "Francois Fouquet", "Assaad Moawad", "Romain Rouvoy", "Yves Le Traon"], "title": ["GreyCat: Efficient What-If Analytics for Data in Motion at Scale"], "date": ["2018-03-23T07:48:15Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1803.09627v1"], "summary": ["  Over the last few years, data analytics shifted from a descriptive era,\nconfined to the explanation of past events, to the emergence of predictive\ntechniques. Nonetheless, existing predictive techniques still fail to\neffectively explore alternative futures, which continuously diverge from\ncurrent situations when exploring the effects of what-if decisions. Enabling\nprescriptive analytics therefore calls for the design of scalable systems that\ncan cope with the complexity and the diversity of underlying data models. In\nthis article, we address this challenge by combining graphs and time series\nwithin a scalable storage system that can organize a massive amount of\nunstructured and continuously changing data into multi-dimensional data models,\ncalled Many-Worlds Graphs. We demonstrate that our open source implementation,\nGreyCat, can efficiently fork and update thousands of parallel worlds composed\nof millions of timestamped nodes, such as what-if exploration.\n"]},
{"authors": ["Jennifer Ortiz", "Magdalena Balazinska", "Johannes Gehrke", "S. Sathiya Keerthi"], "title": ["Learning State Representations for Query Optimization with Deep\n  Reinforcement Learning"], "date": ["2018-03-22T22:39:32Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1803.08604v1"], "summary": ["  Deep reinforcement learning is quickly changing the field of artificial\nintelligence. These models are able to capture a high level understanding of\ntheir environment, enabling them to learn difficult dynamic tasks in a variety\nof domains. In the database field, query optimization remains a difficult\nproblem. Our goal in this work is to explore the capabilities of deep\nreinforcement learning in the context of query optimization. At each state, we\nbuild queries incrementally and encode properties of subqueries through a\nlearned representation. The challenge here lies in the formation of the state\ntransition function, which defines how the current subquery state combines with\nthe next query operation (action) to yield the next state. As a first step in\nthis direction, we focus the state representation problem and the formation of\nthe state transition function. We describe our approach and show preliminary\nresults. We further discuss how we can use the state representation to improve\nquery optimization using reinforcement learning.\n"]},
{"authors": ["Alexandre Bazin", "Jessie Carbonnel", "Marianne Huchard", "Giacomo Kahn"], "title": ["On-demand Relational Concept Analysis"], "date": ["2018-03-21T10:50:26Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1803.07847v1"], "summary": ["  Formal Concept Analysis and its associated conceptual structures have been\nused to support exploratory search through conceptual navigation. Relational\nConcept Analysis (RCA) is an extension of Formal Concept Analysis to process\nrelational datasets. RCA and its multiple interconnected structures represent\ngood candidates to support exploratory search in relational datasets, as they\nare enabling navigation within a structure as well as between the connected\nstructures. However, building the entire structures does not present an\nefficient solution to explore a small localised area of the dataset, for\ninstance to retrieve the closest alternatives to a given query. In these cases,\ngenerating only a concept and its neighbour concepts at each navigation step\nappears as a less costly alternative. In this paper, we propose an algorithm to\ncompute a concept and its neighbourhood in extended concept lattices. The\nconcepts are generated directly from the relational context family, and possess\nboth formal and relational attributes. The algorithm takes into account two RCA\nscaling operators. We illustrate it on an example.\n"]},
{"authors": ["Richard Nock", "Stephen Hardy", "Wilko Henecka", "Hamish Ivey-Law", "Giorgio Patrini", "Guillaume Smith", "Brian Thorne"], "title": ["Entity Resolution and Federated Learning get a Federated Resolution"], "date": ["2018-03-11T20:53:18Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1803.04035v2"], "summary": ["  Consider two data providers, each maintaining records of different feature\nsets about common entities. They aim to learn a linear model over the whole set\nof features. This problem of federated learning over vertically partitioned\ndata includes a crucial upstream issue: entity resolution, i.e. finding the\ncorrespondence between the rows of the datasets. It is well known that entity\nresolution, just like learning, is mistake-prone in the real world. Despite the\nimportance of the problem, there has been no formal assessment of how errors in\nentity resolution impact learning.\n  In this paper, we provide a thorough answer to this question, answering how\noptimal classifiers, empirical losses, margins and generalisation abilities are\naffected. While our answer spans a wide set of losses --- going beyond proper,\nconvex, or classification calibrated ---, it brings simple practical arguments\nto upgrade entity resolution as a preprocessing step to learning. One of these\nsuggests that entity resolution should be aimed at controlling or minimizing\nthe number of matching errors between examples of distinct classes. In our\nexperiments, we modify a simple token-based entity resolution algorithm so that\nit indeed aims at avoiding matching rows belonging to different classes, and\nperform experiments in the setting where entity resolution relies on noisy\ndata, which is very relevant to real world domains. Notably, our approach\ncovers the case where one peer \\textit{does not} have classes, or a noisy\nrecord of classes. Experiments display that using the class information during\nentity resolution can buy significant uplift for learning at little expense\nfrom the complexity standpoint.\n"]},
{"authors": ["Nadime Francis", "Alastair Green", "Paolo Guagliardo", "Leonid Libkin", "Tobias Lindaaker", "Victor Marsault", "Stefan Plantikow", "Mats Rydberg", "Martin Schuster", "Petra Selmer", "Andr\u00e9s Taylor"], "title": ["Formal Semantics of the Language Cypher"], "date": ["2018-02-27T16:01:36Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.09984v2"], "summary": ["  Cypher is a query language for property graphs. It was originally designed\nand implemented as part of the Neo4j graph database, and it is currently used\nin a growing number of commercial systems, industrial applications and research\nprojects. In this work, we provide denotational semantics of the core fragment\nof the read-only part of Cypher, which features in particular pattern matching,\nfiltering, and most relational operations on tables.\n"]},
{"authors": ["Mahmoud Abo Khamis", "Hung Q. Ngo", "XuanLong Nguyen", "Dan Olteanu", "Maximilian Schleich"], "title": ["AC/DC: In-Database Learning Thunderstruck"], "date": ["2018-03-20T15:17:14Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1803.07480v1"], "summary": ["  We report on the design and implementation of the AC/DC gradient descent\nsolver for a class of optimization problems over normalized databases. AC/DC\ndecomposes an optimization problem into a set of aggregates over the join of\nthe database relations. It then uses the answers to these aggregates to\niteratively improve the solution to the problem until it converges.\n  The challenges faced by AC/DC are the large database size, the mixture of\ncontinuous and categorical features, and the large number of aggregates to\ncompute. AC/DC addresses these challenges by employing a sparse data\nrepresentation, factorized computation, problem reparameterization under\nfunctional dependencies, and a data structure that supports shared computation\nof aggregates.\n  To train polynomial regression models and factorization machines of up to\n141K features over the join of a real-world dataset of up to 86M tuples, AC/DC\nneeds up to 30 minutes on one core of a commodity machine. This is up to three\norders of magnitude faster than its competitors R, MadLib, libFM, and\nTensorFlow whenever they finish and thus do not exceed memory limitation,\n24-hour timeout, or internal design limitations.\n"]},
{"authors": ["Yuliang Li", "Alin Deutsch", "Victor Vianu"], "title": ["SpinArt: A Spin-based Verifier for Artifact Systems"], "date": ["2017-05-26T04:13:17Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1705.09427v3"], "summary": ["  Data-driven workflows, of which IBM's Business Artifacts are a prime\nexponent, have been successfully deployed in practice, adopted in industrial\nstandards, and have spawned a rich body of research in academia, focused\nprimarily on static analysis. In previous work, we obtained theoretical results\non the verification of a rich model incorporating core elements of IBM's\nsuccessful Guard-Stage-Milestone (GSM) artifact model. The results showed\ndecidability of verification of temporal properties of a large class of GSM\nworkflows and established its complexity. Following up on these results, the\npresent paper reports on the implementation of SpinArt, a practical verifier\nbased on the classical model-checking tool Spin. The implementation includes\nnontrivial optimizations and achieves good performance on real-world business\nprocess examples. Our results shed light on the capabilities and limitations of\noff-the-shelf verifiers in the context of data-driven workflows.\n"]},
{"authors": ["Barkha Javed", "Zaheer Khan", "Richard McClatchey"], "title": ["An Adaptable System to Support Provenance Management for the Public\n  Policy-Making Process in Smart Cities"], "date": ["2018-03-19T09:52:53Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1803.06850v1"], "summary": ["  Government policies aim to address public issues and problems and therefore\nplay a pivotal role in peoples lives. The creation of public policies, however,\nis complex given the perspective of large and diverse stakeholders involvement,\nconsiderable human participation, lengthy processes, complex task specification\nand the non-deterministic nature of the process. The inherent complexities of\nthe policy process impart challenges for designing a computing system that\nassists in supporting and automating the business process pertaining to policy\nsetup, which also raises concerns for setting up a tracking service in the\npolicy-making environment. A tracking service informs how decisions have been\ntaken during policy creation and can provide useful and intrinsic information\nregarding the policy process. At present, there exists no computing system that\nassists in tracking the complete process that has been employed for policy\ncreation. To design such a system, it is important to consider the policy\nenvironment challenges; for this a novel network and goal based approach has\nbeen framed and is covered in detail in this paper. Furthermore, smart\ngovernance objectives that include stakeholders participation and citizens\ninvolvement have been considered. Thus, the proposed approach has been devised\nby considering smart governance principles and the knowledge environment of\npolicy making where tasks are largely dependent on policy makers decisions and\non individual policy objectives. Our approach reckons the human dimension for\ndeciding and defining autonomous process activities at run time. Furthermore,\nwith the network-based approach, so-called provenance data tracking is employed\nwhich enables the capture of policy process.\n"]},
{"authors": ["R McClatchey", "A Branson", "J Shamdasani", "P. Emin"], "title": ["Evolvable Systems for Big Data Management in Business"], "date": ["2018-03-19T09:01:38Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1803.07434v1"], "summary": ["  Big Data systems are increasingly having to be longer lasting,\nenterprise-wide and interoperable with other (legacy or new) systems.\nFurthermore many organizations operate in an external environment which\ndictates change at an unforeseeable rate and requires evolution in system\nrequirements. In these cases system development does not have a definitive end\npoint, rather it continues in a mutually constitutive cycle with the\norganization and its requirements. Also when the period of design is of such\nduration that the technology may well evolve or when the required technology is\nnot mature at the outset, then the design process becomes considerably more\ndifficult. Not only that but if the system must inter-operate with other\nsystems then the design process becomes considerably more difficult. Ideally in\nthese circumstances the design must also be able to evolve in order to react to\nchanging technologies and requirements and to ensure traceability between the\ndesign and the evolving system specification. In other words extended design\nperiods necessitate adaptable design specifications. For interoperability Big\nData systems need to be discoverable and to work with information about other\nsystems with which they need to cooperate over time. We have developed software\ncalled CRISTAL-ISE that enables dynamic system evolution and interoperability\nfor Big Data systems, it has been commercialised as the Agilium-NG BPM product\nand is described in this paper.\n"]},
{"authors": ["Iqbal H. Sarker", "Flora D. Salim"], "title": ["Mining User Behavioral Rules from Smartphone Data through Association\n  Analysis"], "date": ["2018-03-19T04:37:33Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1804.01379v1"], "summary": ["  The increasing popularity of smart mobile phones and their powerful sensing\ncapabilities have enabled the collection of rich contextual information and\nmobile phone usage records through the device logs. This paper formulates the\nproblem of mining behavioral association rules of individual mobile phone users\nutilizing their smartphone data. Association rule learning is the most popular\ntechnique to discover rules utilizing large datasets. However, it is well-known\nthat a large proportion of association rules generated are redundant. This\nredundant production makes not only the rule-set unnecessarily large but also\nmakes the decision making process more complex and ineffective. In this paper,\nwe propose an approach that effectively identifies the redundancy in\nassociations and extracts a concise set of behavioral association rules that\nare non-redundant. The effectiveness of the proposed approach is examined by\nconsidering the real mobile phone datasets of individual users.\n"]},
{"authors": ["Yuhang Zhang", "Kee Siong Ng", "Michael Walker", "Pauline Chou", "Tania Churchill", "Peter Christen"], "title": ["Scalable Entity Resolution Using Probabilistic Signatures on Parallel\n  Databases"], "date": ["2017-12-27T21:36:50Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1712.09691v3"], "summary": ["  Accurate and efficient entity resolution is an open challenge of particular\nrelevance to intelligence organisations that collect large datasets from\ndisparate sources with differing levels of quality and standard. Starting from\na first-principles formulation of entity resolution, this paper presents a\nnovel Entity Resolution algorithm that introduces a data-driven blocking and\nrecord-linkage technique based on the probabilistic identification of entity\nsignatures in data. The scalability and accuracy of the proposed algorithm are\nevaluated using benchmark datasets and shown to achieve state-of-the-art\nresults. The proposed algorithm can be implemented simply on modern parallel\ndatabases, which allows it to be deployed with relative ease in large\nindustrial applications.\n"]},
{"authors": ["Yasuhito Asano", "Soichiro Hidaka", "Zhenjiang Hu", "Yasunori Ishihara", "Hiroyuki Kato", "Hsiang-Shang Ko", "Keisuke Nakano", "Makoto Onizuka", "Yuya Sasaki", "Toshiyuki Shimizu", "Kanae Tsushima", "Masatoshi Yoshikawa"], "title": ["A View-based Programmable Architecture for Controlling and Integrating\n  Decentralized Data"], "date": ["2018-03-18T15:05:05Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1803.06674v1"], "summary": ["  The view and the view update are known mechanism for controlling access of\ndata and for integrating data of different schemas. Despite intensive and long\nresearch on them in both the database community and the programming language\ncommunity, we are facing difficulties to use them in practice. The main reason\nis that we are lacking of control over the view update strategy to deal with\ninherited ambiguity of view update for a given view.\n  This vision paper aims to provide a new language-based approach to\ncontrolling and integrating decentralized data based on the view, and establish\na software foundation for systematic construction of such data management\nsystems. Our key observation is that a view should be defined through a view\nupdate strategy rather than a query. In other words, the view definition should\nbe extracted from the view update strategy, which is in sharp contrast to the\ntraditional approaches where the view update strategy is derived from the view\ndefinition.\n  In this paper, we present the first programmable architecture with a\ndeclarative language for specifying update strategies over views, whose unique\nview definition can be automatically derived, and show how it can be\neffectively used to control data access, integrate data generally allowing\ncoexistence of GAV (global as view) and LAV (local as view), and perform both\nanalysis and updates on the integrated data. We demonstrate its usefulness\nthrough development of a privacy-preserving ride-sharing alliance system,\ndiscuss its application scope, and highlight future challenges.\n"]},
{"authors": ["Lior Shabtay", "Rami Yaari", "Itai Dattner"], "title": ["A Guided FP-growth algorithm for fast mining of frequent itemsets from\n  big data"], "date": ["2018-03-18T09:57:34Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1803.06632v1"], "summary": ["  In this paper we present the GFP-growth (Guided FP-growth) algorithm, a novel\nmethod for finding the count of a given list of itemsets in large data. Unlike\nFP-growth, our algorithm is designed to focus on the specific multiple itemsets\nof interest and hence its time and memory costs are better. We prove that the\nGFP-growth algorithm yields the exact frequency-counts for the required\nitemsets. We show that for a number of different problems, a solution can be\ndevised which takes advantage of the efficient implementation of multi-targeted\nmining for boosting the performance. In particular, we study in detail the\nproblem of generating the minority-class rules from imbalanced data, a scenario\nthat appears in many real-life domains such as medical applications, failure\nprediction, network and cyber security, and maintenance. We develop the\nMinority-Report Algorithm that uses the GFP-growth for boosting performance. We\nprove some theoretical properties of the Minority-Report Algorithm and\ndemonstrate its superior performance using simulations and real data.\n"]},
{"authors": ["Apurba Das", "Michael Svendsen", "Srikanta Tirthapura"], "title": ["Incremental Maintenance of Maximal Cliques in a Dynamic Graph"], "date": ["2016-01-23T21:15:26Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1601.06311v3"], "summary": ["  We consider the maintenance of the set of all maximal cliques in a dynamic\ngraph that is changing through the addition or deletion of edges. We present\nnearly tight bounds on the magnitude of change in the set of maximal cliques,\nas well as the first change-sensitive algorithms for clique maintenance, whose\nruntime is proportional to the magnitude of the change in the set of maximal\ncliques. We present experimental results showing these algorithms are efficient\nin practice and are faster than prior work by two to three orders of magnitude.\n"]},
{"authors": ["Leopoldo Bertossi", "Georg Gottlob", "Reinhard Pichler"], "title": ["Datalog: Bag Semantics via Set Semantics"], "date": ["2018-03-17T02:00:47Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1803.06445v1"], "summary": ["  Duplicates in data management are common and problematic. In this work, we\npresent a translation of Datalog under bag semantics into a well-behaved\nextension of Datalog (the so-called warded Datalog+-) under set semantics. From\na theoretical point of view, this allows us to reason on bag semantics by\nmaking use of the well-established theoretical foundations of set semantics.\nFrom a practical point of view, this allows us to handle the bag semantics of\nDatalog by powerful, existing query engines for the required extension of\nDatalog. Moreover, this translation has the potential for further extensions --\nabove all to capture the bag semantics of the semantic web query language\nSPARQL.\n"]},
{"authors": ["Rachel Cummings", "Sara Krehbiel", "Kevin A. Lai", "Uthaipon Tantipongpipat"], "title": ["Differential Privacy for Growing Databases"], "date": ["2018-03-16T22:05:44Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1803.06416v1"], "summary": ["  We study the design of differentially private algorithms for adaptive\nanalysis of dynamically growing databases, where a database accumulates new\ndata entries while the analysis is ongoing. We provide a collection of tools\nfor machine learning and other types of data analysis that guarantee\ndifferential privacy and accuracy as the underlying databases grow arbitrarily\nlarge. We give both a general technique and a specific algorithm for adaptive\nanalysis of dynamically growing databases. Our general technique is illustrated\nby two algorithms that schedule black box access to some algorithm that\noperates on a fixed database to generically transform private and accurate\nalgorithms for static databases into private and accurate algorithms for\ndynamically growing databases. These results show that almost any private and\naccurate algorithm can be rerun at appropriate points of data growth with\nminimal loss of accuracy, even when data growth is unbounded. Our specific\nalgorithm directly adapts the private multiplicative weights algorithm to the\ndynamic setting, maintaining the accuracy guarantee of the static setting\nthrough unbounded data growth. Along the way, we develop extensions of several\nother differentially private algorithms to the dynamic setting, which may be of\nindependent interest for future work on the design of differentially private\nalgorithms for growing databases.\n"]},
{"authors": ["Youngbin Kim", "Jimmy Lin"], "title": ["Serverless Data Analytics with Flint"], "date": ["2018-03-16T18:02:27Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1803.06354v1"], "summary": ["  Serverless architectures organized around loosely-coupled function\ninvocations represent an emerging design for many applications. Recent work\nmostly focuses on user-facing products and event-driven processing pipelines.\nIn this paper, we explore a completely different part of the application space\nand examine the feasibility of analytical processing on big data using a\nserverless architecture. We present Flint, a prototype Spark execution engine\nthat takes advantage of AWS Lambda to provide a pure pay-as-you-go cost model.\nWith Flint, a developer uses PySpark exactly as before, but without needing an\nactual Spark cluster. We describe the design, implementation, and performance\nof Flint, along with the challenges associated with serverless analytics.\n"]},
{"authors": ["Weijie Zhao", "Florin Rusu", "Bin Dong", "Kesheng Wu", "Anna Y. Q. Ho", "Peter Nugent"], "title": ["Distributed Caching for Complex Querying of Raw Arrays"], "date": ["2018-03-16T06:33:24Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1803.06089v1"], "summary": ["  As applications continue to generate multi-dimensional data at exponentially\nincreasing rates, fast analytics to extract meaningful results is becoming\nextremely important. The database community has developed array databases that\nalleviate this problem through a series of techniques. In-situ mechanisms\nprovide direct access to raw data in the original format---without loading and\npartitioning. Parallel processing scales to the largest datasets. In-memory\ncaching reduces latency when the same data are accessed across a workload of\nqueries. However, we are not aware of any work on distributed caching of\nmulti-dimensional raw arrays. In this paper, we introduce a distributed\nframework for cost-based caching of multi-dimensional arrays in native format.\nGiven a set of files that contain portions of an array and an online query\nworkload, the framework computes an effective caching plan in two stages.\nFirst, the plan identifies the cells to be cached locally from each of the\ninput files by continuously refining an evolving R-tree index. In the second\nstage, an optimal assignment of cells to nodes that collocates dependent cells\nin order to minimize the overall data transfer is determined. We design cache\neviction and placement heuristic algorithms that consider the historical query\nworkload. A thorough experimental evaluation over two real datasets in three\nfile formats confirms the superiority -- by as much as two orders of magnitude\n-- of the proposed framework over existing techniques in terms of cache\noverhead and workload execution time.\n"]},
{"authors": ["Zhixin Qi", "Hongzhi Wang", "Jianzhong Li", "Hong Gao"], "title": ["Impacts of Dirty Data: and Experimental Evaluation"], "date": ["2018-03-16T04:23:00Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1803.06071v1"], "summary": ["  Data quality issues have attracted widespread attention due to the negative\nimpacts of dirty data on data mining and machine learning results. The\nrelationship between data quality and the accuracy of results could be applied\non the selection of the appropriate algorithm with the consideration of data\nquality and the determination of the data share to clean. However, rare\nresearch has focused on exploring such relationship. Motivated by this, this\npaper conducts an experimental comparison for the effects of missing,\ninconsistent and conflicting data on classification, clustering, and regression\nalgorithms. Based on the experimental findings, we provide guidelines for\nalgorithm selection and data cleaning.\n"]},
{"authors": ["Sara Cohen", "Aviv Zohar"], "title": ["Database Perspectives on Blockchains"], "date": ["2018-03-15T21:58:39Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1803.06015v1"], "summary": ["  Modern blockchain systems are a fresh look at the paradigm of distributed\ncomputing, applied under assumptions of large-scale public networks. They can\nbe used to store and share information without a trusted central party. There\nhas been much effort to develop blockchain systems for a myriad of uses,\nranging from cryptocurrencies to identity control, supply chain management,\netc. None of this work has directly studied the fundamental database issues\nthat arise when using blockchains as the underlying infrastructure to manage\ndata.\n  The key difference between using blockchains to store data and centrally\ncontrolled databases is that transactions are accepted to a blockchain via a\nconsensus mechanism. Hence, once a user has issued a transaction, she cannot be\ncertain if it will be accepted. Moreover, a yet unaccepted transaction cannot\nbe retracted by the user, and may be appended to the blockchain in the future.\nThis causes difficulties as the user may wish to reissue a transaction, if it\nwas not accepted. Yet this data may then become appended twice to the\nblockchain.\n  In this paper we present a database perspective on blockchains by introducing\nformal foundations for blockchains as a storage layer that underlies a\ndatabase. The main issue that we tackle is uncertainty in transaction appending\nthat is a result of the consensus mechanism. We study two flavors of\ntransaction appending problems: (1) the complexity of determining whether it is\npossible for a denial constraint to be contradicted, given the state of the\nblockchain, pending transactions, and integrity constraints and (2) the\ncomplexity of generating transactions that are mutually (in)consistent with\ngiven subsets of pending transactions. Solving these problems is critical to\nensure that users can issue transactions consistent with their intentions.\nFinally, we chart important directions for future work.\n"]},
{"authors": ["Boyi Hou", "Qun Chen", "Zhaoqiang Chen", "Youcef Nafa", "Zhanhuai Li"], "title": ["i-HUMO: An Interactive Human and Machine Cooperation Framework for\n  Entity Resolution with Quality Guarantees"], "date": ["2018-03-15T12:45:46Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1803.05714v1"], "summary": ["  Even though many approaches have been proposed for entity resolution (ER), it\nremains very challenging to find one with quality guarantees. To this end, we\npropose an interactive HUman and Machine cOoperation framework for ER, denoted\nby i-HUMO. Similar to the existing HUMO framework, i-HUMO enforces both\nprecision and recall levels by dividing an ER workload between the human and\nthe machine. It essentially makes the machine label easy instances while\nassigning more challenging instances to the human. However, i-HUMO is a major\nimprovement over HUMO in that it is interactive: its process of human workload\nselection is optimized based on real-time risk analysis on human-labeled\nresults as well as pre-specified machine metrics. In this paper, we first\nintroduce the i-HUMO framework and then present the risk analysis technique to\nprioritize the instances for manual labeling. Finally, we empirically evaluate\ni-HUMO's performance on real data. Our extensive experiments show that i-HUMO\nis effective in enforcing quality guarantees, and compared with the\nstate-of-the-art alternatives, it can achieve better quality control with\nreduced human cost.\n"]},
{"authors": ["Fernando Florenzano", "Cristian Riveros", "Martin Ugarte", "Stijn Vansummeren", "Domagoj Vrgoc"], "title": ["Constant delay algorithms for regular document spanners"], "date": ["2018-03-14T13:44:53Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1803.05277v1"], "summary": ["  Regular expressions and automata models with capture variables are core tools\nin rule-based information extraction. These formalisms, also called regular\ndocument spanners, use regular languages in order to locate the data that a\nuser wants to extract from a text document, and then store this data into\nvariables. Since document spanners can easily generate large outputs, it is\nimportant to have good evaluation algorithms that can generate the extracted\ndata in a quick succession, and with relatively little precomputation time.\nTowards this goal, we present a practical evaluation algorithm that allows\nconstant delay enumeration of a spanner's output after a precomputation phase\nthat is linear in the document. While the algorithm assumes that the spanner is\nspecified in a syntactic variant of variable set automata, we also study how it\ncan be applied when the spanner is specified by general variable set automata,\nregex formulas, or spanner algebras. Finally, we study the related problem of\ncounting the number of outputs of a document spanner, providing a fine grained\nanalysis of the classes of document spanners that support efficient enumeration\nof their results.\n"]},
{"authors": ["Torsten Kilias", "Alexander L\u00f6ser", "Felix A. Gers", "Richard Koopmanschap", "Ying Zhang", "Martin Kersten"], "title": ["IDEL: In-Database Entity Linking with Neural Embeddings"], "date": ["2018-03-13T15:35:42Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1803.04884v1"], "summary": ["  We present a novel architecture, In-Database Entity Linking (IDEL), in which\nwe integrate the analytics-optimized RDBMS MonetDB with neural text mining\nabilities. Our system design abstracts core tasks of most neural entity linking\nsystems for MonetDB. To the best of our knowledge, this is the first defacto\nimplemented system integrating entity-linking in a database. We leverage the\nability of MonetDB to support in-database-analytics with user defined functions\n(UDFs) implemented in Python. These functions call machine learning libraries\nfor neural text mining, such as TensorFlow. The system achieves zero cost for\ndata shipping and transformation by utilizing MonetDB's ability to embed Python\nprocesses in the database kernel and exchange data in NumPy arrays. IDEL\nrepresents text and relational data in a joint vector space with neural\nembeddings and can compensate errors with ambiguous entity representations. For\ndetecting matching entities, we propose a novel similarity function based on\njoint neural embeddings which are learned via minimizing pairwise contrastive\nranking loss. This function utilizes a high dimensional index structures for\nfast retrieval of matching entities. Our first implementation and experiments\nusing the WebNLG corpus show the effectiveness and the potentials of IDEL.\n"]},
{"authors": ["Babak Salimi", "Johannes Gehrke", "Dan Suciu"], "title": ["HypDB: Detect, Explain And Resolve Bias in OLAP"], "date": ["2018-03-12T22:54:11Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1803.04562v1"], "summary": ["  On line analytical processing (OLAP) is an essential element of\ndecision-support systems. OLAP tools provide insights and understanding needed\nfor improved decision making. However, the answers to OLAP queries can be\nbiased and lead to perplexing and incorrect insights. In this paper, we propose\nHypDB, a system to detect, explain, and to resolve bias in decision-support\nqueries. We give a simple definition of a \\emph{biased query}, which performs a\nset of independence tests on the data to detect bias. We propose a novel\ntechnique that gives explanations for bias, thus assisting an analyst in\nunderstanding what goes on. Additionally, we develop an automated method for\nrewriting a biased query into an unbiased query, which shows what the analyst\nintended to examine. In a thorough evaluation on several real datasets we show\nboth the quality and the performance of our techniques, including the\ncompletely automatic discovery of the revolutionary insights from a famous 1973\ndiscrimination case.\n"]},
{"authors": ["Ryan Marcus", "Olga Papaemmanouil"], "title": ["Deep Reinforcement Learning for Join Order Enumeration"], "date": ["2018-02-28T20:00:33Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1803.00055v2"], "summary": ["  Join order selection plays a significant role in query performance. However,\nmodern query optimizers typically employ static join enumeration algorithms\nthat do not receive any feedback about the quality of the resulting plan.\nHence, optimizers often repeatedly choose the same bad plan, as they do not\nhave a mechanism for \"learning from their mistakes\". In this paper, we argue\nthat existing deep reinforcement learning techniques can be applied to address\nthis challenge. These techniques, powered by artificial neural networks, can\nautomatically improve decision making by incorporating feedback from their\nsuccesses and failures. Towards this goal, we present ReJOIN, a\nproof-of-concept join enumerator, and present preliminary results indicating\nthat ReJOIN can match or outperform the PostgreSQL optimizer in terms of plan\nquality and join enumeration efficiency.\n"]},
{"authors": ["Bertil Chapuis", "Benoit Garbinato"], "title": ["Geodabs: Trajectory Indexing Meets Fingerprinting at Scale"], "date": ["2018-03-12T14:48:06Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1803.04292v1"], "summary": ["  Finding trajectories and discovering motifs that are similar in large\ndatasets is a central problem for a wide range of applications. Solutions\naddressing this problem usually rely on spatial indexing and on the computation\nof a similarity measure in polynomial time. Although effective in the context\nof sparse trajectory datasets, this approach is too expensive in the context of\ndense datasets, where many trajectories potentially match with a given query.\nIn this paper, we apply fingerprinting, a copy-detection mechanism used in the\ncontext of textual data, to trajectories. To this end, we fingerprint\ntrajectories with geodabs, a construction based on geohash aimed at trajectory\nfingerprinting. We demonstrate that by relying on the properties of a space\nfilling curve geodabs can be used to build sharded inverted indexes. We show\nhow normalization affects precision and recall, two key measures in information\nretrieval. We then demonstrate that the probabilistic nature of fingerprinting\nhas a marginal effect on the quality of the results. Finally, we evaluate our\nmethod in terms of performances and show that, in contrast with existing\nmethods, it is not affected by the density of the trajectory dataset and that\nit can be efficiently distributed.\n"]},
{"authors": ["Diego Didona", "Rachid Guerraoui", "Jingjing Wang", "Willy Zwaenepoel"], "title": ["Causal Consistency and Latency Optimality: Friend or Foe?"], "date": ["2018-03-12T13:18:05Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1803.04237v1"], "summary": ["  Causal consistency is an attractive consistency model for replicated data\nstores. It is provably the strongest model that tolerates partitions, it avoids\nthe long latencies associated with strong consistency, and, especially when\nusing read-only transactions, it prevents many of the anomalies of weaker\nconsistency models. Recent work has shown that causal consistency allows\n\"latency-optimal\" read-only transactions, that are nonblocking, single-version\nand single-round in terms of communication. On the surface, this latency\noptimality is very appealing, as the vast majority of applications are assumed\nto have read-dominated workloads.\n  In this paper, we show that such \"latency-optimal\" read-only transactions\ninduce an extra overhead on writes, the extra overhead is so high that\nperformance is actually jeopardized, even in read-dominated workloads. We show\nthis result from a practical and a theoretical angle.\n  First, we present a protocol that implements \"almost laten- cy-optimal\" ROTs\nbut does not impose on the writes any of the overhead of latency-optimal\nprotocols. In this protocol, ROTs are nonblocking, one version and can be\nconfigured to use either two or one and a half rounds of client-server\ncommunication. We experimentally show that this protocol not only provides\nbetter throughput, as expected, but also surprisingly better latencies for all\nbut the lowest loads and most read-heavy workloads.\n  Then, we prove that the extra overhead imposed on writes by latency-optimal\nread-only transactions is inherent, i.e., it is not an artifact of the design\nwe consider, and cannot be avoided by any implementation of latency-optimal\nread-only transactions. We show in particular that this overhead grows linearly\nwith the number of clients.\n"]},
{"authors": ["Dimitrios Vasilas", "Marc Shapiro", "Bradley King"], "title": ["A Modular Design for Geo-Distributed Querying"], "date": ["2018-03-12T07:39:56Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1803.04141v1"], "summary": ["  Most distributed storage systems provide limited abilities for querying data\nby attributes other than their primary keys. Supporting efficient search on\nsecondary attributes is challenging as applications pose varying requirements\nto query processing systems, and no single system design can be suitable for\nall needs. In this paper, we show how to overcome these challenges in order to\nextend distributed data stores to support queries on secondary attributes. We\npropose a modular architecture that is flexible and allows query processing\nsystems to make trade-offs according to different use case requirements. We\ndescribe adap-tive mechanisms that make use of this flexibility to enable query\nprocessing systems to dynamically adjust to query and write operation\nworkloads.\n"]},
{"authors": ["Michael Gowanlock", "Ben Karsin"], "title": ["GPU Accelerated Self-join for the Distance Similarity Metric"], "date": ["2018-03-12T05:28:44Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1803.04120v1"], "summary": ["  The self-join finds all objects in a dataset within a threshold of each other\ndefined by a similarity metric. As such, the self-join is a building block for\nthe field of databases and data mining, and is employed in Big Data\napplications. In this paper, we advance a GPU-efficient algorithm for the\nsimilarity self-join that uses the Euclidean distance metric. The\nsearch-and-refine strategy is an efficient approach for low dimensionality\ndatasets, as index searches degrade with increasing dimension (i.e., the curse\nof dimensionality). Thus, we target the low dimensionality problem, and compare\nour GPU self-join to a search-and-refine implementation, and a state-of-the-art\nparallel algorithm. In low dimensionality, there are several unique challenges\nassociated with efficiently solving the self-join problem on the GPU. Low\ndimensional data often results in higher data densities, causing a significant\nnumber of distance calculations and a large result set. As dimensionality\nincreases, index searches become increasingly exhaustive, forming a performance\nbottleneck. We advance several techniques to overcome these challenges using\nthe GPU. The techniques we propose include a GPU-efficient index that employs a\nbounded search, a batching scheme to accommodate large result set sizes, and a\nreduction in distance calculations through duplicate search removal. Our GPU\nself-join outperforms both search-and-refine and state-of-the-art algorithms.\n"]},
{"authors": ["Pedram Gharani", "Kenrick Fernande", "Vineet Raghu"], "title": ["TRAJEDI: Trajectory Dissimilarity"], "date": ["2018-03-09T23:07:09Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1803.03716v1"], "summary": ["  The vast increase in our ability to obtain and store trajectory data\nnecessitates trajectory analytics techniques to extract useful information from\nthis data. Pair-wise distance functions are a foundation building block for\ncommon operations on trajectory datasets including constrained SELECT queries,\nk-nearest neighbors, and similarity and diversity algorithms. The accuracy and\nperformance of these operations depend heavily on the speed and accuracy of the\nunderlying trajectory distance function, which is in turn affected by\ntrajectory calibration. Current methods either require calibrated data, or\nperform calibration of the entire relevant dataset first, which is expensive\nand time consuming for large datasets. We present TRAJEDI, a calibrationaware\npair-wise distance calculation scheme that outperforms naive approaches while\npreserving accuracy. We also provide analyses of parameter tuning to trade-off\nbetween speed and accuracy. Our scheme is usable with any diversity, similarity\nor k-nearest neighbor algorithm.\n"]},
{"authors": ["Sahaana Suri", "Peter Bailis"], "title": ["DROP: Dimensionality Reduction Optimization for Time Series"], "date": ["2017-08-01T06:58:15Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1708.00183v2"], "summary": ["  Dimensionality reduction is a critical step in analytics pipelines for\nhigh-volume, high-dimensional time series. Principal Component Analysis (PCA)\nis frequently the method of choice for many applications, yet is often\nprohibitively expensive for large datasets. Many theoretical means of\naccelerating PCA via sampling have recently been proposed, but these techniques\ntypically treat PCA as a reusable statistical operator, independent of\ndownstream analytics workflows. We demonstrate how accounting for downstream\nanalytics operations during dimensionality reduction via PCA allows stochastic\nmethods to efficiently operate over very small (e.g., 1%) subsamples of input\ndata, thus reducing computational overhead and end-to-end runtime. This enables\nend-to-end optimization over both dimensionality reduction and analytics tasks.\nBy combining techniques spanning progressive sampling, approximate query\nprocessing, and cost-based optimization, our proposed dimensionality reduction\noptimizer enables end-to-end speedups of up to 13.9 times over classic\nSVD-based PCA techniques, and achieves parity with and can exceed conventional\napproaches like FFT and PAA by up to 3 times in end-to-end workflows.\n"]},
{"authors": ["Bart Scheers", "Steven Bloemen", "Hannes M\u00fchleisen", "Pim Schellart", "Arjen van Elteren", "Martin Kersten", "Paul J. Groot"], "title": ["Fast in-database cross-matching of high-cadence, high-density source\n  lists with an up-to-date sky model"], "date": ["2018-03-07T11:22:40Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1803.02601v1"], "summary": ["  Coming high-cadence wide-field optical telescopes will image hundreds of\nthousands of sources per minute. Besides inspecting the near real-time data\nstreams for transient and variability events, the accumulated data archive is a\nwealthy laboratory for making complementary scientific discoveries.\n  The goal of this work is to optimise column-oriented database techniques to\nenable the construction of a full-source and light-curve database for\nlarge-scale surveys, that is accessible by the astronomical community.\n  We adopted LOFAR's Transients Pipeline as the baseline and modified it to\nenable the processing of optical images that have much higher source densities.\nThe pipeline adds new source lists to the archive database, while\ncross-matching them with the known cataloged sources in order to build a full\nlight-curve archive. We investigated several techniques of indexing and\npartitioning the largest tables, allowing for faster positional source look-ups\nin the cross matching algorithms. We monitored all query run times in long-term\npipeline runs where we processed a subset of IPHAS data that have image source\ndensity peaks over $170,000$ per field of view ($500,000$ deg$^{-2}$).\n  Our analysis demonstrates that horizontal table partitions of declination\nwidths of one-degree control the query run times. Usage of an index strategy\nwhere the partitions are densily sorted according to source declination yields\nanother improvement. Most queries run in sublinear time and a few (<20%) run in\nlinear time, because of dependencies on input source-list and result-set size.\nWe observed that for this logical database partitioning schema the limiting\ncadence the pipeline achieved with processing IPHAS data is 25 seconds.\n"]},
{"authors": ["Nikos Bikakis", "Vana Kalogeraki", "Dimitrios Gunopulos"], "title": ["Social Event Scheduling"], "date": ["2018-01-30T13:16:51Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.09973v2"], "summary": ["  A major challenge for social event organizers (e.g., event planning and\nmarketing companies, venues) is attracting the maximum number of participants,\nsince it has great impact on the success of the event, and, consequently, the\nexpected gains (e.g., revenue, artist/brand publicity). In this paper, we\nintroduce the Social Event Scheduling (SES) problem, which schedules a set of\nsocial events considering user preferences and behavior, events' spatiotemporal\nconflicts, and competing vents, in order to maximize the overall number of\nattendees. We show that SES is strongly NP-hard, even in highly restricted\ninstances. To cope with the hardness of the SES problem we design a greedy\napproximation algorithm. Finally, we evaluate our method experimentally using a\ndataset from the Meetup event-based social network.\n"]},
{"authors": ["Edward Gan", "Jialin Ding", "Kai Sheng Tai", "Vatsal Sharan", "Peter Bailis"], "title": ["Moment-Based Quantile Sketches for Efficient High Cardinality\n  Aggregation Queries"], "date": ["2018-03-06T00:48:59Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1803.01969v1"], "summary": ["  Interactive analytics increasingly involves querying for quantiles over\nspecific sub-populations and time windows of high cardinality datasets. Data\nprocessing engines such as Druid and Spark use mergeable summaries to estimate\nquantiles on these large datasets, but summary merge times are a bottleneck\nduring high-cardinality aggregation. We show how a compact and efficiently\nmergeable quantile sketch can support aggregation workloads. This data\nstructure, which we refer to as the moments sketch, operates with a small\nmemory footprint (200 bytes) and computationally efficient (50ns) merges by\ntracking only a set of summary statistics, notably the sample moments. We\ndemonstrate how we can efficiently and practically estimate quantiles using the\nmethod of moments and the maximum entropy principle, and show how the use of a\ncascade further improves query time for threshold predicates. Empirical\nevaluation on real-world datasets shows that the moments sketch can achieve\nless than 1 percent error with 40 times less merge overhead than comparable\nsummaries, improving end query time in the MacroBase engine by up to 7 times\nand the Druid engine by up to 60 times.\n"]},
{"authors": ["G\u00f6sta Grahne", "Ali Moallemi"], "title": ["Universal (and Existential) Nulls"], "date": ["2018-03-05T00:48:08Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1803.01445v1"], "summary": ["  Incomplete Information research is quite mature when it comes to so called\n{\\em existential nulls}, where an existential null is a value stored in the\ndatabase, representing an unknown object. For some reason {\\em universal\nnulls}, that is, values representing {\\em all} possible objects, have received\nalmost no attention. We remedy the situation in this paper, by showing that a\nsuitable finite representation mechanism, called {\\em Star Cylinders}, handling\nuniversal nulls can be developed based on the {\\em Cylindric Set Algebra} of\nHenkin, Monk and Tarski. We provide a finitary version of the cylindric set\nalgebra, called {\\em Cylindric Star Algebra}, and show that our star-cylinders\nare closed under this algebra. Moreover, we show that any {\\em First Order\nRelational Calculus} query over databases containing universal nulls can be\ntranslated into an equivalent expression in our cylindric star-algebra, and\nvice versa, in time polynomial in the size of the database.\n  The representation mechanism is then extended to {\\em Naive Star Cylinders},\nwhich are star-cylinders allowing existential nulls in addition to universal\nnulls. For positive queries (with universal quantification), the well known\nnaive evaluation technique can still be applied on the existential nulls,\nthereby allowing polynomial time evaluation of certain answers on databases\ncontaining both universal and existential nulls. If precise answers are\nrequired, certain answer evaluation with universal and existential nulls\nremains in coNP. Note that the problem is coNP-hard, already for positive\nexistential queries and databases with only existential nulls. If inequalities\n$\\neg(x_i\\approx x_j)$ are allowed, reasoning over existential databases is\nknown to be $\\Pi^p_2$-complete, and it remains in $\\Pi^p_2$ when universal\nnulls and full first order queries are allowed.\n"]},
{"authors": ["Jelle Hellings", "Marc Gyssens", "Yuqing Wu", "Dirk Van Gucht", "Jan Van den Bussche", "Stijn Vansummeren", "George H. L. Fletcher"], "title": ["Comparing Downward Fragments of the Relational Calculus with Transitive\n  Closure on Trees"], "date": ["2018-03-04T17:38:51Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1803.01390v1"], "summary": ["  Motivated by the continuing interest in the tree data model, we study the\nexpressive power of downward navigational query languages on trees and chains.\nBasic navigational queries are built from the identity relation and edge\nrelations using composition and union. We study the effects on relative\nexpressiveness when we add transitive closure, projections, coprojections,\nintersection, and difference; this for boolean queries and path queries on\nlabeled and unlabeled structures. In all cases, we present the complete Hasse\ndiagram. In particular, we establish, for each query language fragment that we\nstudy on trees, whether it is closed under difference and intersection.\n"]},
{"authors": ["Saravanan Thirumuruganathan", "Nan Tang", "Mourad Ouzzani"], "title": ["Data Curation with Deep Learning [Vision]: Towards Self Driving Data\n  Curation"], "date": ["2018-03-04T17:08:45Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1803.01384v1"], "summary": ["  Past. Data curation - the process of discovering, integrating, and cleaning\ndata - is one of the oldest data management problems. Unfortunately, it is\nstill the most time consuming and least enjoyable work of data scientists. So\nfar, successful data curation stories are mainly ad-hoc solutions that are\neither domain-specific (for example, ETL rules) or task-specific (for example,\nentity resolution).\n  Present. The power of current data curation solutions are not keeping up with\nthe ever changing data ecosystem in terms of volume, velocity, variety and\nveracity, mainly due to the high human cost, instead of machine cost, needed\nfor providing the ad-hoc solutions mentioned above. Meanwhile, deep learning is\nmaking strides in achieving remarkable successes in areas such as image\nrecognition, natural language processing, and speech recognition. This is\nlargely due to its ability to understanding features that are neither\ndomain-specific nor task-specific.\n  Future. Data curation solutions need to keep the pace with the fast-changing\ndata ecosystem, where the main hope is to devise domain-agnostic and\ntask-agnostic solutions. To this end, we start a new research project, called\nAutoDC, to unleash the potential of deep learning towards self-driving data\ncuration. We will discuss how different deep learning concepts can be adapted\nand extended to solve various data curation problems. We showcase some\nlow-hanging fruits about the early encounters between deep learning and data\ncuration happening in AutoDC. We believe that the directions pointed out by\nthis work will not only drive AutoDC towards democratizing data curation, but\nalso serve as a cornerstone for researchers and practitioners to move to a new\nrealm of data curation solutions.\n"]},
{"authors": ["Giovanni Vincenti"], "title": ["Imprecise temporal associations and decision support systems"], "date": ["2018-03-03T21:40:33Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1803.01248v1"], "summary": ["  The quick and pervasive infiltration of decision support systems, artificial\nintelligence, and data mining in consumer electronics and everyday life in\ngeneral has been significant in recent years. Fields such as UX have been\nfacilitating the integration of such technologies into software and hardware,\nbut the back-end processing is still based on binary foundations. This article\ndescribes an approach to mining for imprecise temporal associations among\nevents in data streams, taking into account the very natural concept of\napproximation. This type of association analysis is likely to lead to more\nmeaningful and actionable decision support systems.\n"]},
{"authors": ["Abolfazl Asudeh", "Azade Nazi", "Nan Zhang", "Gautam Das", "H. V. Jagadish"], "title": ["RRR: Rank-Regret Representative"], "date": ["2018-02-28T08:24:02Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.10303v2"], "summary": ["  Selecting the best items in a dataset is a common task in data exploration.\nHowever, the concept of \"best\" lies in the eyes of the beholder: different\nusers may consider different attributes more important, and hence arrive at\ndifferent rankings. Nevertheless, one can remove \"dominated\" items and create a\n\"representative\" subset of the data set, comprising the \"best items\" in it. A\nPareto-optimal representative is guaranteed to contain the best item of each\npossible ranking, but it can be almost as big as the full data. Representative\ncan be found if we relax the requirement to include the best item for every\npossible user, and instead just limit the users' \"regret\". Existing work\ndefines regret as the loss in score by limiting consideration to the\nrepresentative instead of the full data set, for any chosen ranking function.\n  However, the score is often not a meaningful number and users may not\nunderstand its absolute value. Sometimes small ranges in score can include\nlarge fractions of the data set. In contrast, users do understand the notion of\nrank ordering. Therefore, alternatively, we consider the position of the items\nin the ranked list for defining the regret and propose the {\\em rank-regret\nrepresentative} as the minimal subset of the data containing at least one of\nthe top-$k$ of any possible ranking function. This problem is NP-complete. We\nuse the geometric interpretation of items to bound their ranks on ranges of\nfunctions and to utilize combinatorial geometry notions for developing\neffective and efficient approximation algorithms for the problem. Experiments\non real datasets demonstrate that we can efficiently find small subsets with\nsmall rank-regrets.\n"]},
{"authors": ["Georgios Santipantakis", "Christos Doulkeridis", "George A. Vouros", "Akrivi Vlachou"], "title": ["MaskLink: Efficient Link Discovery for Spatial Relations via Masking\n  Areas"], "date": ["2018-03-03T09:56:39Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1803.01135v1"], "summary": ["  In this paper, we study the problem of spatial link discovery (LD), focusing\nprimarily on topological and proximity relations between spatial objects. The\nproblem is timely due to the large number of sources that generate spatial\ndata, including streaming sources (e.g., surveillance of moving objects) but\nalso archival sources (such as static areas of interest). To address the\nproblem of integrating data from such diverse sources, link discovery\ntechniques are required to identify various spatial relations efficiently.\nExisting approaches typically adopt the filter and refine methodology by\nexploiting a blocking technique for effective filtering.\n  In this paper, we present a new spatial LD technique, called MaskLink, that\nimproves the effectiveness of the filtering step. We show that MaskLink\noutperforms the state-of-the-art algorithm for link discovery of topological\nrelations, while also addressing some of its limitations, such as applicability\nfor streaming data, low memory requirements, and parallelization. Furthermore,\nwe show that MaskLink can be extended and generalized to the case of\nproximity-based link discovery, which has not been studied before for spatial\ndata.\n  Our empirical study demonstrates the superiority of MaskLink against the\nstate-of-the-art in the case of topological relations, and its performance gain\ncompared to a baseline technique in the case of proximity-based LD.\n"]},
{"authors": ["Besim Bilalli", "Alberto Abell\u00f3", "Tom\u00e0s Aluja-Banet", "Robert Wrembel"], "title": ["PRESISTANT: Learning based assistant for data pre-processing"], "date": ["2018-03-02T19:50:30Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1803.01024v1"], "summary": ["  Data pre-processing is one of the most time consuming and relevant steps in a\ndata analysis process (e.g., classification task). A given data pre-processing\noperator (e.g., transformation) can have positive, negative or zero impact on\nthe final result of the analysis. Expert users have the required knowledge to\nfind the right pre-processing operators. However, when it comes to non-experts,\nthey are overwhelmed by the amount of pre-processing operators and it is\nchallenging for them to find operators that would positively impact their\nanalysis (e.g., increase the predictive accuracy of a classifier). Existing\nsolutions either assume that users have expert knowledge, or they recommend\npre-processing operators that are only \"syntactically\" applicable to a dataset,\nwithout taking into account their impact on the final analysis. In this work,\nwe aim at providing assistance to non-expert users by recommending data\npre-processing operators that are ranked according to their impact on the final\nanalysis. We developed a tool PRESISTANT, that uses Random Forests to learn the\nimpact of pre-processing operators on the performance (e.g., predictive\naccuracy) of 5 different classification algorithms, such as J48, Naive Bayes,\nPART, Logistic Regression, and Nearest Neighbor. Extensive evaluations on the\nrecommendations provided by our tool, show that PRESISTANT can effectively help\nnon-experts in order to achieve improved results in their analytical tasks.\n"]},
{"authors": ["Daniel Lemire", "Gregory Ssi-Yan-Kai", "Owen Kaser"], "title": ["Consistently faster and smaller compressed bitmaps with Roaring"], "date": ["2016-03-21T19:30:53Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1603.06549v4"], "summary": ["  Compressed bitmap indexes are used in databases and search engines. Many\nbitmap compression techniques have been proposed, almost all relying primarily\non run-length encoding (RLE). However, on unsorted data, we can get superior\nperformance with a hybrid compression technique that uses both uncompressed\nbitmaps and packed arrays inside a two-level tree. An instance of this\ntechnique, Roaring, has recently been proposed. Due to its good performance, it\nhas been adopted by several production platforms (e.g., Apache Lucene, Apache\nSpark, Apache Kylin and Druid).\n  Yet there are cases where run-length encoded bitmaps are smaller than the\noriginal Roaring bitmaps---typically when the data is sorted so that the\nbitmaps contain long compressible runs. To better handle these cases, we build\na new Roaring hybrid that combines uncompressed bitmaps, packed arrays and RLE\ncompressed segments. The result is a new Roaring format that compresses better.\n  Overall, our new implementation of Roaring can be several times faster (up to\ntwo orders of magnitude) than the implementations of traditional RLE-based\nalternatives (WAH, Concise, EWAH) while compressing better. We review the\ndesign choices and optimizations that make these good results possible.\n"]},
{"authors": ["Kai Mast", "Lequn Chen", "Emin G\u00fcn Sirer"], "title": ["Enabling Strong Database Integrity using Trusted Execution Environments"], "date": ["2018-01-05T03:04:50Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.01618v2"], "summary": ["  Many applications require the immutable and consistent sharing of data across\norganizational boundaries. Because conventional datastores cannot provide this\nfunctionality, blockchains have been proposed as one possible solution. Yet\npublic blockchains are energy inefficient, hard to scale and suffer from\nlimited throughput and high latencies, while permissioned blockchains depend on\nspecially designated nodes, potentially leak meta-information, and also suffer\nfrom scale and performance bottlenecks.\n  This paper presents CreDB, a datastore that provides blockchain-like\nguarantees of integrity using trusted execution environments. CreDB employs\nfour novel mechanisms to support a new class of applications. First, it creates\na permanent record of every transaction, known as a witness, that clients can\nthen use not only to audit the database but to prove to third parties that\ndesired actions took place. Second, it associates with every object an\ninseparable and inviolable policy, which not only performs access control but\nenables the datastore to implement state machines whose behavior is amenable to\nanalysis. Third, timeline inspection allows authorized parties to inspect and\nreason about the history of changes made to the data. Finally, CreDB provides a\nprotected function evaluation mechanism that allows integrity-protected\ncomputation over private data. The paper describes these mechanisms, and the\napplications they collectively enable, in detail. We have fully implemented a\nprototype of CreDB on Intel SGX. Evaluation shows that CreDB can serve as a\ndrop-in replacement for other NoSQL stores, such as MongoDB while providing\nstronger integrity guarantees.\n"]},
{"authors": ["Tobias Christiani", "Rasmus Pagh", "Johan Sivertsen"], "title": ["Scalable and robust set similarity join"], "date": ["2017-07-21T09:50:36Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1707.06814v3"], "summary": ["  Set similarity join is a fundamental and well-studied database operator. It\nis usually studied in the exact setting where the goal is to compute all pairs\nof sets that exceed a given similarity threshold (measured e.g. as Jaccard\nsimilarity). But set similarity join is often used in settings where 100%\nrecall may not be important --- indeed, where the exact set similarity join is\nitself only an approximation of the desired result set.\n  We present a new randomized algorithm for set similarity join that can\nachieve any desired recall up to 100%, and show theoretically and empirically\nthat it significantly improves on existing methods. The present\nstate-of-the-art exact methods are based on prefix-filtering, the performance\nof which depends on the data set having many rare tokens. Our method is robust\nagainst the absence of such structure in the data. At 90% recall our algorithm\nis often more than an order of magnitude faster than state-of-the-art exact\nmethods, depending on how well a data set lends itself to prefix filtering. Our\nexperiments on benchmark data sets also show that the method is several times\nfaster than comparable approximate methods. Our algorithm makes use of recent\ntheoretical advances in high-dimensional sketching and indexing that we believe\nto be of wider relevance to the data engineering community.\n"]},
{"authors": ["Zhongjun Jin", "Michael Cafarella", "H. V. Jagadish", "Sean Kandel", "Michael Minar"], "title": ["Unifacta: Profiling-driven String Pattern Standardization"], "date": ["2018-03-02T03:47:39Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1803.00701v1"], "summary": ["  Data cleaning is critical for effective data analytics on many real-world\ndata collected. One of the most challenging data cleaning tasks is pattern\nstandardization-reformatting ad hoc data, e.g., phone numbers, human names and\naddresses, in heterogeneous non-standard patterns (formats) into a standard\npattern-as it is tedious and effort-consuming, especially for large data sets\nwith diverse patterns. In this paper, we develop Unifacta, a technique that\nhelps the end user standardize ill-formatted ad hoc data. With minimum user\ninput, our proposed technique can effectively and efficiently help the end user\nsynthesize high quality explainable pattern standardization programs. We\nimplemented Unifacta, on Trifacta, and experimentally compared Unifacta with a\nprevious state-of-the-art string transformation tool, Flashfill, along with\nTrifacta and Blinkfill. Experimental results show that Unifacta produced\nprograms of comparable quality, but more explainable, while requiring\nsubstantially less user effort than Flashfill, and other related baseline\nsystems. In a user effort study, Unifacta saved 30\\% - 70\\% user effort\ncompared to the baseline systems. In an experiment testing the user's\nunderstanding of the synthesized transformation logic, Unifacta users achieved\na success rate about twice that of Flashfill users.\n"]},
{"authors": ["Sen Wu", "Luke Hsiao", "Xiao Cheng", "Braden Hancock", "Theodoros Rekatsinas", "Philip Levis", "Christopher R\u00e9"], "title": ["Fonduer: Knowledge Base Construction from Richly Formatted Data"], "date": ["2017-03-15T09:12:29Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.05028v2"], "summary": ["  We focus on knowledge base construction (KBC) from richly formatted data. In\ncontrast to KBC from text or tabular data, KBC from richly formatted data aims\nto extract relations conveyed jointly via textual, structural, tabular, and\nvisual expressions. We introduce Fonduer, a machine-learning-based KBC system\nfor richly formatted data. Fonduer presents a new data model that accounts for\nthree challenging characteristics of richly formatted data: (1) prevalent\ndocument-level relations, (2) multimodality, and (3) data variety. Fonduer uses\na new deep-learning model to automatically capture the representation (i.e.,\nfeatures) needed to learn how to extract relations from richly formatted data.\nFinally, Fonduer provides a new programming model that enables users to convert\ndomain expertise, based on multiple modalities of information, to meaningful\nsignals of supervision for training a KBC system. Fonduer-based KBC systems are\nin production for a range of use cases, including at a major online retailer.\nWe compare Fonduer against state-of-the-art KBC approaches in four different\ndomains. We show that Fonduer achieves an average improvement of 41 F1 points\non the quality of the output knowledge base---and in some cases produces up to\n1.87x the number of correct entries---compared to expert-curated public\nknowledge bases. We also conduct a user study to assess the usability of\nFonduer's new programming model. We show that after using Fonduer for only 30\nminutes, non-domain experts are able to design KBC systems that achieve on\naverage 23 F1 points higher quality than traditional machine-learning-based KBC\napproaches.\n"]},
{"authors": ["Wajdi Dhifli", "Abdoulaye Banir\u00e9 Diallo"], "title": ["Toward an Efficient Multi-class Classification in an Open Universe"], "date": ["2015-11-02T22:04:00Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1511.00725v3"], "summary": ["  Classification is a fundamental task in machine learning and data mining.\nExisting classification methods are designed to classify unknown instances\nwithin a set of previously known training classes. Such a classification takes\nthe form of a prediction within a closed-set of classes. However, a more\nrealistic scenario that fits real-world applications is to consider the\npossibility of encountering instances that do not belong to any of the training\nclasses, $i.e.$, an open-set classification. In such situation, existing\nclosed-set classifiers will assign a training label to these instances\nresulting in a misclassification. In this paper, we introduce Galaxy-X, a novel\nmulti-class classification approach for open-set recognition problems. For each\nclass of the training set, Galaxy-X creates a minimum bounding hyper-sphere\nthat encompasses the distribution of the class by enclosing all of its\ninstances. In such manner, our method is able to distinguish instances\nresembling previously seen classes from those that are of unknown ones. To\nadequately evaluate open-set classification, we introduce a novel evaluation\nprocedure. Experimental results on benchmark datasets show the efficiency of\nour approach in classifying novel instances from known as well as unknown\nclasses.\n"]},
{"authors": ["Ugur Turan", "Ismail H. Toroslu", "Murat Kantarcioglu"], "title": ["Graph Based Proactive Secure Decomposition Algorithm for Context\n  Dependent Attribute Based Inference Control Problem"], "date": ["2018-03-01T16:53:49Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1803.00497v1"], "summary": ["  Relational DBMSs continue to dominate the database market, and inference\nproblem on external schema of relational DBMS's is still an important issue in\nterms of data privacy.Especially for the last 10 years, external schema\nconstruction for application-specific database usage has increased its\nindependency from the conceptual schema, as the definitions and implementations\nof views and procedures have been optimized. This paper offers an optimized\ndecomposition strategy for the external schema, which concentrates on the\nprivacy policy and required associations of attributes for the intended user\nroles. The method proposed in this article performs a proactive decomposition\nof the external schema, in order to satisfy both the forbidden and required\nassociations of attributes.Functional dependency constraints of a database\nschema can be represented as a graph, in which vertices are attribute sets and\nedges are functional dependencies. In this representation, inference problem\ncan be defined as a process of searching a subtree in the dependency graph\ncontaining the attributes that need to be related. The optimized decomposition\nprocess aims to generate an external schema, which guarantees the prevention of\nthe inference of the forbidden attribute sets while guaranteeing the\nassociation of the required attribute sets with a minimal loss of possible\nassociation among other attributes, if the inhibited and required attribute\nsets are consistent with each other. Our technique is purely proactive, and can\nbe viewed as a normalization process. Due to the usage independency of external\nschema construction tools, it can be easily applied to any existing systems\nwithout rewriting data access layer of applications. Our extensive experimental\nanalysis shows the effectiveness of this optimized proactive strategy for a\nwide variety of logical schema volumes.\n"]},
{"authors": ["Milos Nikolic", "Dan Olteanu"], "title": ["Incremental View Maintenance with Triple Lock Factorization Benefits"], "date": ["2017-03-22T01:39:00Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.07484v2"], "summary": ["  We introduce F-IVM, a unified incremental view maintenance (IVM) approach for\na variety of tasks, including gradient computation for learning linear\nregression models over joins, matrix chain multiplication, and factorized\nevaluation of conjunctive queries.\n  F-IVM is a higher-order IVM algorithm that reduces the maintenance of the\ngiven task to the maintenance of a hierarchy of increasingly simpler views. The\nviews are functions mapping keys, which are tuples of input data values, to\npayloads, which are elements from a task-specific ring. Whereas the computation\nover the keys is the same for all tasks, the computation over the payloads\ndepends on the task. F-IVM achieves efficiency by factorizing the computation\nof the keys, payloads, and updates.\n  We implemented F-IVM as an extension of DBToaster. We show in a range of\nscenarios that it can outperform classical first-order IVM, DBToaster's fully\nrecursive higher-order IVM, and plain recomputation by orders of magnitude\nwhile using less memory.\n"]},
{"authors": ["Vasileios Kagklis", "Elias C. Stavropoulos", "Vassilios S. Verykios"], "title": ["A Frequent Itemset Hiding Toolbox"], "date": ["2018-02-28T17:23:33Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.10543v1"], "summary": ["  Advances in data collection and data storage technologies have given way to\nthe establishment of transactional databases among companies and organizations,\nas they allow enormous amounts of data to be stored efficiently. Useful\nknowledge can be mined from these data, which can be used in several ways\ndepending on the nature of the data. Quite often companies and organizations\nare willing to share data for the sake of mutual benefit. However, the sharing\nof such data comes with risks, as problems with privacy may arise. Sensitive\ndata, along with sensitive knowledge inferred from this data, must be protected\nfrom unintentional exposure to unauthorized parties. One form of the inferred\nknowledge is frequent patterns mined in the form of frequent itemsets from\ntransactional databases. The problem of protecting such patterns is known as\nthe frequent itemset hiding problem.\n  In this paper we present a toolbox, which provides several implementations of\nfrequent itemset hiding algorithms. Firstly, we summarize the most important\naspects of each algorithm. We then introduce the architecture of the toolbox\nand its novel features. Finally, we provide experimental results on real world\ndatasets, demonstrating the efficiency of the toolbox and the convenience it\noffers in comparing different algorithms.\n"]},
{"authors": ["Edmon Begoli", "Jes\u00fas Camacho Rodr\u00edguez", "Julian Hyde", "Michael J. Mior", "Daniel Lemire"], "title": ["Apache Calcite: A Foundational Framework for Optimized Query Processing\n  Over Heterogeneous Data Sources"], "date": ["2018-02-28T02:10:36Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.10233v1"], "summary": ["  Apache Calcite is a foundational software framework that provides query\nprocessing, optimization, and query language support to many popular\nopen-source data processing systems such as Apache Hive, Apache Storm, Apache\nFlink, Druid, and MapD. Calcite's architecture consists of a modular and\nextensible query optimizer with hundreds of built-in optimization rules, a\nquery processor capable of processing a variety of query languages, an adapter\narchitecture designed for extensibility, and support for heterogeneous data\nmodels and stores (relational, semi-structured, streaming, and geospatial).\nThis flexible, embeddable, and extensible architecture is what makes Calcite an\nattractive choice for adoption in big-data frameworks. It is an active project\nthat continues to introduce support for the new types of data sources, query\nlanguages, and approaches to query processing and optimization.\n"]},
{"authors": ["Yihan Gao", "Silu Huang", "Aditya Parameswaran"], "title": ["Navigating the Data Lake with Datamaran: Automatically Extracting\n  Structure from Log Datasets"], "date": ["2017-08-29T17:47:08Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1708.08905v3"], "summary": ["  Organizations routinely accumulate semi-structured log datasets generated as\nthe output of code; these datasets remain unused and uninterpreted, and occupy\nwasted space - this phenomenon has been colloquially referred to as \"data lake\"\nproblem. One approach to leverage these semi-structured datasets is to convert\nthem into a structured relational format, following which they can be analyzed\nin conjunction with other datasets. We present Datamaran, an tool that extracts\nstructure from semi-structured log datasets with no human supervision.\nDatamaran automatically identifies field and record endpoints, separates the\nstructured parts from the unstructured noise or formatting, and can tease apart\nmultiple structures from within a dataset, in order to efficiently extract\nstructured relational datasets from semi-structured log datasets, at scale with\nhigh accuracy. Compared to other unsupervised log dataset extraction tools\ndeveloped in prior work, Datamaran does not require the record boundaries to be\nknown beforehand, making it much more applicable to the noisy log files that\nare ubiquitous in data lakes. Datamaran can successfully extract structured\ninformation from all datasets used in prior work, and can achieve 95%\nextraction accuracy on automatically collected log datasets from GitHub - a\nsubstantial 66% increase of accuracy compared to unsupervised schemes from\nprior work. Our user study further demonstrates that the extraction results of\nDatamaran are closer to the desired structure than competing algorithms.\n"]},
{"authors": ["Ingo M\u00fcller", "Andrea Arteaga", "Torsten Hoefler", "Gustavo Alonso"], "title": ["Reproducible Floating-Point Aggregation in RDBMSs"], "date": ["2018-02-27T13:51:05Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.09883v1"], "summary": ["  Industry-grade database systems are expected to produce the same result if\nthe same query is repeatedly run on the same input. However, the numerous\nsources of non-determinism in modern systems make reproducible results\ndifficult to achieve. This is particularly true if floating-point numbers are\ninvolved, where the order of the operations affects the final result.\n  As part of a larger effort to extend database engines with data\nrepresentations more suitable for machine learning and scientific applications,\nin this paper we explore the problem of making relational GroupBy over\nfloating-point formats bit-reproducible, i.e., ensuring any execution of the\noperator produces the same result up to every single bit. To that aim, we first\npropose a numeric data type that can be used as drop-in replacement for other\nnumber formats and is---unlike standard floating-point formats---associative.\nWe use this data type to make state-of-the-art GroupBy operators reproducible,\nbut this approach incurs a slowdown between 4x and 12x compared to the same\noperator using conventional database number formats. We thus explore how to\nmodify existing GroupBy algorithms to make them bit-reproducible and efficient.\nBy using vectorized summation on batches and carefully balancing batch size,\ncache footprint, and preprocessing costs, we are able to reduce the slowdown\ndue to reproducibility to a factor between 1.9x and 2.4x of aggregation in\nisolation and to a mere 2.7% of end-to-end query performance even on\naggregation-intensive queries in MonetDB. We thereby provide a solid basis for\nsupporting more reproducible operations directly in relational engines.\n  This document is an extended version of an article currently in print for the\nproceedings of ICDE'18 with the same title and by the same authors. The main\nadditions are more implementation details and experiments.\n"]},
{"authors": ["Petr Luk\u00e1\u0161", "Radim Ba\u010da", "Michal Kr\u00e1tk\u00fd"], "title": ["Demythization of XML Query Processing: Technical Report"], "date": ["2017-03-28T12:33:45Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.09539v3"], "summary": ["  XML query can be modeled by twig pattern query (TPQ) specifying predicates on\nXML nodes and XPath relationships satisfied between them. A lot of TPQ types\nhave been proposed; this paper takes into account a TPQ model extended by a\nspecification of output and non-output query nodes since it complies with the\nXQuery semantics and, in many cases, it leads to a more efficient query\nprocessing. In general, there are two approaches to process the TPQ: holistic\njoins and binary joins. Whereas the binary join approach builds a query plan as\na tree of interconnected binary operators, the holistic join approach evaluates\na whole query using one operator (i.e., using one complex algorithm).\nSurprisingly, a thorough analytical and experimental comparison is still\nmissing despite an enormous research effort in this area. In this paper, we try\nto fill this gap; we analytically and experimentally show that the binary joins\nused in a fully-pipelined plan (i.e., the plan where each join operation does\nnot wait for the complete result of the previous operation and no explicit\nsorting is used) can often outperform the holistic joins, especially for TPQs\nwith a higher ratio of non-output query nodes. The main contributions of this\npaper can be summarized as follows: (i) we introduce several improvements of\nexisting binary join approaches allowing to build a fully-pipelined plan for a\nTPQ considering non-output query nodes, (ii) we prove that for a certain class\nof TPQs such a plan has the linear time complexity with respect to the size of\nthe input and output as well as the linear space complexity with respect to the\nXML document depth (i.e., the same complexity as the holistic join approaches),\n(iii) we show that our improved binary join approach outperforms the holistic\njoin approaches in many situations, and (iv) we propose a simple combined\napproach that uses advantages of both types of approaches.\n"]},
{"authors": ["Carl S. Adorf", "Paul M. Dodd", "Vyas Ramasubramani", "Sharon C. Glotzer"], "title": ["Simple Data and Workflow Management with the signac Framework"], "date": ["2016-11-10T23:34:13Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1611.03543v3"], "summary": ["  Researchers in the field of materials science, chemistry, and computational\nphysics are regularly posed with the challenge of managing large and\nheterogeneous data spaces. The amount of data increases in lockstep with\ncomputational efficiency multiplied by the amount of available computational\nresources, which shifts the bottleneck in the scientific process from data\nacquisition to data processing and analysis. We present a framework designed to\naid in the integration of various specialized data formats, tools and\nworkflows. The signac framework provides all basic components required to\ncreate a well-defined and thus collectively accessible and searchable data\nspace, simplifying data access and modification through a homogeneous data\ninterface that is largely agnostic to the data source, i.e., computation or\nexperiment. The framework's data model is designed to not require absolute\ncommitment to the presented implementation, simplifying adaption into existing\ndata sets and workflows. This approach not only increases the efficiency with\nwhich scientific results can be produced, but also significantly lowers\nbarriers for collaborations requiring shared data access.\n"]},
{"authors": ["Nasrin Mazaheri Soudani", "Ali Karami"], "title": ["All nearest neighbor calculation based on Delaunay graphs"], "date": ["2018-02-26T20:32:05Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.09594v1"], "summary": ["  When we have two data sets and want to find the nearest neighbour of each\npoint in the first dataset among points in the second one, we need the all\nnearest neighbour operator. This is an operator in spatial databases that has\nmany application in different fields such as GIS and VLSI circuit design.\nExisting algorithms for calculating this operator assume that there is no pre\ncomputation on these data sets. These algorithms has o(n*m*d) time complexity\nwhere n and m are the number of points in two data sets and d is the dimension\nof data points. With assumption of some pre computation on data sets algorithms\nwith lower time complexity can be obtained. One of the most common pre\ncomputation on spatial data is Delaunay graphs. In the Delaunay graph of a data\nset each point is linked to its nearest neighbours. In this paper, we introduce\nan algorithm for computing the all nearest neighbour operator on spatial data\nsets based on their Delaunay graphs. The performance of this algorithm is\ncompared with one of the best existing algorithms for computing ANN operator in\nterms of CPU time and the number of IOs. The experimental results show that\nthis algorithm has better performance than the other.\n"]},
{"authors": ["Andreas Kipf", "Harald Lang", "Varun Pandey", "Raul Alexandru Persa", "Peter Boncz", "Thomas Neumann", "Alfons Kemper"], "title": ["Adaptive Geospatial Joins for Modern Hardware"], "date": ["2018-02-26T18:11:36Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.09488v1"], "summary": ["  Geospatial joins are a core building block of connected mobility\napplications. An especially challenging problem are joins between streaming\npoints and static polygons. Since points are not known beforehand, they cannot\nbe indexed. Nevertheless, points need to be mapped to polygons with low\nlatencies to enable real-time feedback.\n  We present an adaptive geospatial join that uses true hit filtering to avoid\nexpensive geometric computations in most cases. Our technique uses a\nquadtree-based hierarchical grid to approximate polygons and stores these\napproximations in a specialized radix tree. We emphasize on an approximate\nversion of our algorithm that guarantees a user-defined precision. The exact\nversion of our algorithm can adapt to the expected point distribution by\nrefining the index. We optimized our implementation for modern hardware\narchitectures with wide SIMD vector processing units, including Intel's brand\nnew Knights Landing. Overall, our approach can perform up to two orders of\nmagnitude faster than existing techniques.\n"]},
{"authors": ["Malika Bendechache", "Nhien-An Le-Khac", "M-Tahar Kechadi"], "title": ["Efficient Large Scale Clustering based on Data Partitioning"], "date": ["2017-04-11T17:05:01Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1704.03421v2"], "summary": ["  Clustering techniques are very attractive for extracting and identifying\npatterns in datasets. However, their application to very large spatial datasets\npresents numerous challenges such as high-dimensionality data, heterogeneity,\nand high complexity of some algorithms. For instance, some algorithms may have\nlinear complexity but they require the domain knowledge in order to determine\ntheir input parameters. Distributed clustering techniques constitute a very\ngood alternative to the big data challenges (e.g.,Volume, Variety, Veracity,\nand Velocity). Usually these techniques consist of two phases. The first phase\ngenerates local models or patterns and the second one tends to aggregate the\nlocal results to obtain global models. While the first phase can be executed in\nparallel on each site and, therefore, efficient, the aggregation phase is\ncomplex, time consuming and may produce incorrect and ambiguous global clusters\nand therefore incorrect models. In this paper we propose a new distributed\nclustering approach to deal efficiently with both phases, generation of local\nresults and generation of global models by aggregation. For the first phase,\nour approach is capable of analysing the datasets located in each site using\ndifferent clustering techniques. The aggregation phase is designed in such a\nway that the final clusters are compact and accurate while the overall process\nis efficient in time and memory allocation. For the evaluation, we use two\nwell-known clustering algorithms, K-Means and DBSCAN. One of the key outputs of\nthis distributed clustering technique is that the number of global clusters is\ndynamic, no need to be fixed in advance. Experimental results show that the\napproach is scalable and produces high quality results.\n"]},
{"authors": ["K. Venkatesh Emani", "S. Sudarshan"], "title": ["Cobra: A Framework for Cost Based Rewriting of Database Applications"], "date": ["2018-01-15T17:58:18Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.04891v3"], "summary": ["  Database applications are typically written using a mixture of imperative\nlanguages and declarative frameworks for data processing. Application logic\ngets distributed across the declarative and imperative parts of a program.\nOften, there is more than one way to implement the same program, whose\nefficiency may depend on a number of parameters. In this paper, we propose a\nframework that automatically generates all equivalent alternatives of a given\nprogram using a given set of program transformations, and chooses the least\ncost alternative. We use the concept of program regions as an algebraic\nabstraction of a program and extend the Volcano/Cascades framework for\noptimization of algebraic expressions, to optimize programs. We illustrate the\nuse of our framework for optimizing database applications. We show through\nexperimental results, that our framework has wide applicability in real world\napplications and provides significant performance benefits.\n"]},
{"authors": ["Tomer Kaftan", "Magdalena Balazinska", "Alvin Cheung", "Johannes Gehrke"], "title": ["Cuttlefish: A Lightweight Primitive for Adaptive Query Processing"], "date": ["2018-02-26T06:50:43Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.09180v1"], "summary": ["  Modern data processing applications execute increasingly sophisticated\nanalysis that requires operations beyond traditional relational algebra. As a\nresult, operators in query plans grow in diversity and complexity. Designing\nquery optimizer rules and cost models to choose physical operators for all of\nthese novel logical operators is impractical. To address this challenge, we\ndevelop Cuttlefish, a new primitive for adaptively processing online query\nplans that explores candidate physical operator instances during query\nexecution and exploits the fastest ones using multi-armed bandit reinforcement\nlearning techniques. We prototype Cuttlefish in Apache Spark and adaptively\nchoose operators for image convolution, regular expression matching, and\nrelational joins. Our experiments show Cuttlefish-based adaptive convolution\nand regular expression operators can reach 72-99% of the throughput of an\nall-knowing oracle that always selects the optimal algorithm, even when\nindividual physical operators are up to 105x slower than the optimal.\nAdditionally, Cuttlefish achieves join throughput improvements of up to 7.5x\ncompared with Spark SQL's query optimizer.\n"]},
{"authors": ["Jingbo Zhou", "Qi Guo", "H. V. Jagadish", "Lubo\u0161 Kr\u010d\u00e1l", "Siyuan Liu", "Wenhao Luan", "Anthony K. H. Tung", "Yueji Yang", "Yuxin Zheng"], "title": ["A Generic Inverted Index Framework for Similarity Search on the GPU -\n  Technical Report"], "date": ["2016-03-28T14:44:34Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1603.08390v2"], "summary": ["  We propose a novel generic inverted index framework on the GPU (called\nGENIE), aiming to reduce the programming complexity of the GPU for parallel\nsimilarity search of different data types. Not every data type and similarity\nmeasure are supported by GENIE, but many popular ones are. We present the\nsystem design of GENIE, and demonstrate similarity search with GENIE on several\ndata types along with a theoretical analysis of search results. A new concept\nof locality sensitive hashing (LSH) named $\\tau$-ANN search, and a novel data\nstructure c-PQ on the GPU are also proposed for achieving this purpose.\nExtensive experiments on different real-life datasets demonstrate the\nefficiency and effectiveness of our framework. The implemented system has been\nreleased as open source.\n"]},
{"authors": ["Sejoon Oh", "Namyong Park", "Lee Sael", "U Kang"], "title": ["Scalable Tucker Factorization for Sparse Tensors - Algorithms and\n  Discoveries"], "date": ["2017-10-06T02:54:44Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.02261v2"], "summary": ["  Given sparse multi-dimensional data (e.g., (user, movie, time; rating) for\nmovie recommendations), how can we discover latent concepts/relations and\npredict missing values? Tucker factorization has been widely used to solve such\nproblems with multi-dimensional data, which are modeled as tensors. However,\nmost Tucker factorization algorithms regard and estimate missing entries as\nzeros, which triggers a highly inaccurate decomposition. Moreover, few methods\nfocusing on an accuracy exhibit limited scalability since they require huge\nmemory and heavy computational costs while updating factor matrices. In this\npaper, we propose P-Tucker, a scalable Tucker factorization method for sparse\ntensors. P-Tucker performs alternating least squares with a row-wise update\nrule in a fully parallel way, which significantly reduces memory requirements\nfor updating factor matrices. Furthermore, we offer two variants of P-Tucker: a\ncaching algorithm P-Tucker-Cache and an approximation algorithm\nP-Tucker-Approx, both of which accelerate the update process. Experimental\nresults show that P-Tucker exhibits 1.7-14.1x speed-up and 1.4-4.8x less error\ncompared to the state-of-the-art. In addition, P-Tucker scales near linearly\nwith the number of observable entries in a tensor and number of threads. Thanks\nto P-Tucker, we successfully discover hidden concepts and relations in a\nlarge-scale real-world tensor, while existing methods cannot reveal latent\nfeatures due to their limited scalability or low accuracy.\n"]},
{"authors": ["Yujing Ma", "Florin Rusu", "Martin Torres"], "title": ["Stochastic Gradient Descent on Highly-Parallel Architectures"], "date": ["2018-02-24T05:27:04Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.08800v1"], "summary": ["  There is an increased interest in building data analytics frameworks with\nadvanced algebraic capabilities both in industry and academia. Many of these\nframeworks, e.g., TensorFlow and BIDMach, implement their compute-intensive\nprimitives in two flavors---as multi-thread routines for multi-core CPUs and as\nhighly-parallel kernels executed on GPU. Stochastic gradient descent (SGD) is\nthe most popular optimization method for model training implemented extensively\non modern data analytics platforms. While the data-intensive properties of SGD\nare well-known, there is an intense debate on which of the many SGD variants is\nbetter in practice. In this paper, we perform a comprehensive study of parallel\nSGD for training generalized linear models. We consider the impact of three\nfactors -- computing architecture (multi-core CPU or GPU), synchronous or\nasynchronous model updates, and data sparsity -- on three measures---hardware\nefficiency, statistical efficiency, and time to convergence. In the process, we\ndesign an optimized asynchronous SGD algorithm for GPU that leverages warp\nshuffling and cache coalescing for data and model access. We draw several\ninteresting findings from our extensive experiments with logistic regression\n(LR) and support vector machines (SVM) on five real datasets. For synchronous\nSGD, GPU always outperforms parallel CPU---they both outperform a sequential\nCPU solution by more than 400X. For asynchronous SGD, parallel CPU is the\nsafest choice while GPU with data replication is better in certain situations.\nThe choice between synchronous GPU and asynchronous CPU depends on the task and\nthe characteristics of the data. As a reference, our best implementation\noutperforms TensorFlow and BIDMach consistently. We hope that our insights\nprovide a useful guide for applying parallel SGD to generalized linear models.\n"]},
{"authors": ["Francesco Belardinelli", "Umberto Grandi"], "title": ["Database Aggregation"], "date": ["2018-02-23T15:15:53Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.08586v1"], "summary": ["  Knowledge can be represented compactly in a multitude ways, from a set of\npropositional formulas, to a Kripke model, to a database. In this paper we\nstudy the aggregation of information coming from multiple sources, each source\nsubmitting a database modelled as a first-order relational structure. In the\npresence of an integrity constraint, we identify classes of aggregators that\nrespect it in the aggregated database, provided all individual databases\nsatisfy it. We also characterise languages for first-order queries on which the\nanswer to queries on the aggregated database coincides with the aggregation of\nthe answers to the query obtained on each individual database. This\ncontribution is meant to be a first step on the application of techniques from\nrational choice theory to knowledge representation in databases.\n"]},
{"authors": ["Peng Cheng", "Xiang Lian", "Xun Jian", "Lei Chen"], "title": ["FROG: A Fast and Reliable Crowdsourcing Framework (Technical Report)"], "date": ["2016-10-26T16:44:11Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1610.08411v2"], "summary": ["  For decades, the crowdsourcing has gained much attention from both academia\nand industry, which outsources a number of tasks to human workers. Existing\nworks considered improving the task accuracy through voting or learning\nmethods, they usually did not fully take into account reducing the latency of\nthe task completion. When a task requester posts a group of tasks (e.g.,\nsentiment analysis), and one can only obtain answers of all tasks after the\nlast task is accomplished. As a consequence, the time delay of even one task in\nthis group could delay the next step of the task requester's work from minutes\nto days, which is quite undesirable for the task requester.\n  Inspired by the importance of the task accuracy and latency, in this paper,\nwe will propose a novel crowdsourcing framework, namely Fast and Reliable\ncrOwdsourcinG framework (FROG), which intelligently assigns tasks to workers,\nsuch that the latencies of tasks are reduced and the expected accuracies of\ntasks are met. Specifically, our FROG framework consists of two important\ncomponents, task scheduler and notification modules. For the task scheduler\nmodule, we formalize a FROG task scheduling (FROG-TS) problem, in which the\nserver actively assigns workers with tasks with high reliability and low\nlatency. We prove that the FROG-TS problem is NP-hard. Thus, we design two\nheuristic approaches, request-based and batch-based scheduling. For the\nnotification module, we define an efficient worker notifying (EWN) problem,\nwhich only sends task invitations to those workers with high probabilities of\naccepting the tasks. To tackle the EWN problem, we propose a smooth kernel\ndensity estimation approach to estimate the probability that a worker accepts\nthe task invitation. Through extensive experiments, we demonstrate the\neffectiveness and efficiency of our proposed FROG platform on both real and\nsynthetic data sets.\n"]},
{"authors": ["Peng Cheng", "Xun Jian", "Lei Chen"], "title": ["Task Assignment on Spatial Crowdsourcing (Technical Report)"], "date": ["2016-05-31T15:35:18Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1605.09675v3"], "summary": ["  Recently, with the rapid development of mobile devices and the crowdsourcing\nplatforms, the spatial crowdsourcing has attracted much attention from the\ndatabase community. Specifically, spatial crowdsourcing refers to sending a\nlocation-based request to workers according to their positions, and workers\nneed to physically move to specified locations to conduct tasks. Many works\nhave studied task assignment problems in spatial crowdsourcing, however, their\nproblem settings are different from each other. Thus, it is hard to compare the\nperformances of existing algorithms on task assignment in spatial\ncrowdsourcing. In this paper, we present a comprehensive experimental\ncomparison of most existing algorithms on task assignment in spatial\ncrowdsourcing. Specifically, we first give general definitions about spatial\nworkers and spatial tasks based on definitions in the existing works such that\nthe existing algorithms can be applied on the same synthetic and real data\nsets. Then, we provide an uniform implementation for all the tested algorithms\nof task assignment problems in spatial crowdsourcing (open sourced). Finally,\nbased on the results on both synthetic and real data sets, we discuss the\nstrengths and weaknesses of tested algorithms, which can guide future research\non the same area and practical implementations of spatial crowdsourcing\nsystems.\n"]},
{"authors": ["Jeyhun Karimov", "Tilmann Rabl", "Asterios Katsifodimos", "Roman Samarev", "Henri Heiskanen", "Volker Markl"], "title": ["Benchmarking Distributed Stream Processing Engines"], "date": ["2018-02-23T12:09:08Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.08496v1"], "summary": ["  Over the last years, stream data processing has been gaining attention both\nin industry and in academia due to its wide range of applications. To fulfill\nthe need for scalable and efficient stream analytics, numerous open source\nstream data processing systems (SDPSs) have been developed, with high\nthroughput and low latency being their key performance targets. In this paper,\nwe propose a framework to evaluate the performance of three SDPSs, namely\nApache Storm, Apache Spark, and Apache Flink. Our evaluation focuses in\nparticular on measuring the throughput and latency of windowed operations. For\nthis benchmark, we design workloads based on real-life, industrial use-cases.\nThe main contribution of this work is threefold. First, we give a definition of\nlatency and throughput for stateful operators. Second, we completely separate\nthe system under test and driver, so that the measurement results are closer to\nactual system performance under real conditions. Third, we build the first\ndriver to test the actual sustainable performance of a system under test. Our\ndetailed evaluation highlights that there is no single winner, but rather, each\nsystem excels in individual use-cases.\n"]},
{"authors": ["Souvik Bhattacherjee", "Amol Deshpande"], "title": ["RStore: A Distributed Multi-version Document Store"], "date": ["2018-02-21T17:50:44Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.07693v2"], "summary": ["  We address the problem of compactly storing a large number of versions\n(snapshots) of a collection of keyed documents or records in a distributed\nenvironment, while efficiently answering a variety of retrieval queries over\nthose, including retrieving full or partial versions, and evolution histories\nfor specific keys. We motivate the increasing need for such a system in a\nvariety of application domains, carefully explore the design space for building\nsuch a system and the various storage-computation-retrieval trade-offs, and\ndiscuss how different storage layouts influence those trade-offs. We propose a\nnovel system architecture that satisfies the key desiderata for such a system,\nand offers simple tuning knobs that allow adapting to a specific data and query\nworkload. Our system is intended to act as a layer on top of a distributed\nkey-value store that houses the raw data as well as any indexes. We design\nnovel off-line storage layout algorithms for efficiently partitioning the data\nto minimize the storage costs while keeping the retrieval costs low. We also\npresent an online algorithm to handle new versions being added to system. Using\nextensive experiments on large datasets, we demonstrate that our system\noperates at the scale required in most practical scenarios and often\noutperforms standard baselines, including a delta-based storage engine, by\norders-of-magnitude.\n"]},
{"authors": ["Nikos Bikakis"], "title": ["Big Data Visualization Tools"], "date": ["2018-01-25T10:16:48Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.08336v2"], "summary": ["  Data visualization is the presentation of data in a pictorial or graphical\nformat, and a data visualization tool is the software that generates this\npresentation. Data visualization provides users with intuitive means to\ninteractively explore and analyze data, enabling them to effectively identify\ninteresting patterns, infer correlations and causalities, and supports\nsense-making activities.\n"]},
{"authors": ["Nazim Faour"], "title": ["Data Consistency Simulation Tool for NoSQL Database Systems"], "date": ["2018-02-22T14:24:00Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.08052v1"], "summary": ["  Various data consistency levels have an important part in the integrity of\ndata and also affect performance especially the data that is replicated many\ntimes across or over the cluster. Based on BASE and the theorem of CAP\ntradeoffs, most systems of NoSQL have more relaxed consistency guarantees than\nanother kind of databases which implement ACID. Most systems of NoSQL gave\ndifferent methods to adjust a required level of consistency to ensure the\nminimal numbering of the replicas accepted in each operation. Simulations are\nalways depending on a simplified model and ignore many details and facts about\nthe real system. Therefore, a simulation can only work as an estimation or an\nexplanation vehicle for observed behavior. So to create simulation tool, I have\nto characterize a model, identify influence factors and simply implement that\ndepending on a (modeled) workload. In this paper, I have a model of simulation\nto measure the consistency of the data and to detect the data consistency\nviolations in simulated network partition settings. So workloads are needed\nwith the set of users who make requests and then put the results for analysis.\n"]},
{"authors": ["Huiping Liu", "Cheqing Jin", "Bin Yang", "Aoying Zhou"], "title": ["Finding Top-k Optimal Sequenced Routes -- Full Version"], "date": ["2018-02-22T12:46:07Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.08014v1"], "summary": ["  Motivated by many practical applications in logistics and\nmobility-as-a-service, we study the top-k optimal sequenced routes (KOSR)\nquerying on large, general graphs where the edge weights may not satisfy the\ntriangle inequality, e.g., road network graphs with travel times as edge\nweights. The KOSR querying strives to find the top-k optimal routes (i.e., with\nthe top-k minimal total costs) from a given source to a given destination,\nwhich must visit a number of vertices with specific vertex categories (e.g.,\ngas stations, restaurants, and shopping malls) in a particular order (e.g.,\nvisiting gas stations before restaurants and then shopping malls).\n  To efficiently find the top-k optimal sequenced routes, we propose two\nalgorithms PruningKOSR and StarKOSR. In PruningKOSR, we define a dominance\nrelationship between two partially-explored routes. The partially-explored\nroutes that can be dominated by other partially-explored routes are postponed\nbeing extended, which leads to a smaller searching space and thus improves\nefficiency. In StarKOSR, we further improve the efficiency by extending routes\nin an A* manner. With the help of a judiciously designed heuristic estimation\nthat works for general graphs, the cost of partially explored routes to the\ndestination can be estimated such that the qualified complete routes can be\nfound early. In addition, we demonstrate the high extensibility of the proposed\nalgorithms by incorporating Hop Labeling, an effective label indexing technique\nfor shortest path queries, to further improve efficiency. Extensive experiments\non multiple real-world graphs demonstrate that the proposed methods\nsignificantly outperform the baseline method. Furthermore, when k=1, StarKOSR\nalso outperforms the state-of-the-art method for the optimal sequenced route\nqueries.\n"]},
{"authors": ["Sudeepa Roy", "Cynthia Rudin", "Alexander Volfovsky", "Tianyu Wang"], "title": ["FLAME: A Fast Large-scale Almost Matching Exactly Approach to Causal\n  Inference"], "date": ["2017-07-19T22:35:40Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1707.06315v3"], "summary": ["  A classical problem in causal inference is that of matching, where treatment\nunits need to be matched to control units. Some of the main challenges in\ndeveloping matching methods arise from the tension among (i) inclusion of as\nmany covariates as possible in defining the matched groups, (ii) having matched\ngroups with enough treated and control units for a valid estimate of Average\nTreatment Effect (ATE) in each group, and (iii) computing the matched pairs\nefficiently for large datasets. In this paper we propose a fast method for\napproximate and exact matching in causal analysis called FLAME (Fast\nLarge-scale Almost Matching Exactly). We define an optimization objective for\nmatch quality, which gives preferences to matching on covariates that can be\nuseful for predicting the outcome while encouraging as many matches as\npossible. FLAME aims to optimize our match quality measure, leveraging\ntechniques that are natural for query processing in the area of database\nmanagement. We provide two implementations of FLAME using SQL queries and\nbit-vector techniques.\n"]},
{"authors": ["Yanhong A. Liu"], "title": ["Logic Programming Applications: What Are the Abstractions and\n  Implementations?"], "date": ["2018-02-20T19:04:14Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.07284v1"], "summary": ["  This article presents an overview of applications of logic programming,\nclassifying them based on the abstractions and implementations of logic\nlanguages that support the applications. The three key abstractions are join,\nrecursion, and constraint. Their essential implementations are for-loops, fixed\npoints, and backtracking, respectively. The corresponding kinds of applications\nare database queries, inductive analysis, and combinatorial search,\nrespectively. We also discuss language extensions and programming paradigms,\nsummarize example application problems by application areas, and touch on\nexample systems that support variants of the abstractions with different\nimplementations.\n"]},
{"authors": ["Trong Duc Nguyen", "Ming-Hung Shih", "Divesh Srivastava", "Srikanta Tirthapura", "Bojian Xu"], "title": ["Variance-Optimal Offline and Streaming Stratified Random Sampling"], "date": ["2018-01-27T05:28:48Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.09039v3"], "summary": ["  Stratified random sampling (SRS) is a fundamental sampling technique that\nprovides accurate estimates for aggregate queries using a small size sample,\nand has been used widely for approximate query processing. A key question in\nSRS is how to partition a target sample size among different strata. While\nNeyman allocation provides a solution that minimizes the variance of an\nestimate using this sample, it works under the assumption that each stratum is\nabundant, i.e., has a large number of data points to choose from. This\nassumption may not hold in general: one or more strata may be bounded, and may\nnot contain a large number of data points, even though the total data size may\nbe large.\n  We first present VOILA, an offline method for allocating sample sizes to\nstrata in a variance-optimal manner, even for the case when one or more strata\nmay be bounded. We next consider SRS on streaming data that are continuously\narriving. We show a lower bound, that any streaming algorithm for SRS must have\n(in the worst case) a variance that is {\\Omega}(r) factor away from the\noptimal, where r is the number of strata. We present S-VOILA, a practical\nstreaming algorithm for SRS that is locally variance-optimal in its allocation\nof sample sizes to different strata. Our result from experiments on real and\nsynthetic data show that VOILA can have significantly (1.4 to 50.0 times)\nsmaller variance than Neyman allocation. The streaming algorithm S-VOILA\nresults in a variance that is typically close to VOILA, which was given the\nentire input beforehand.\n"]},
{"authors": ["Maduako N. Ikechukwu", "Francis I. Okeke"], "title": ["Towards Realisation of Heterogeneous Earth-Observation Sensor Database\n  Framework for the Sensor Observation Service based on PostGIS"], "date": ["2018-02-17T03:52:21Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.06183v1"], "summary": ["  Environmental monitoring and management systems in most cases deal with\nmodels and spatial analytics that involve the integration of in-situ and remote\nGeosensor observations. In-situ sensor observations and those gathered by\nremote sensors are usually provided by different databases and services in\nreal-time dynamic services such as the Geo-Web Services. Thus, data have to be\npulled from different databases and transferred over the network before they\nare fused and processed on the service middleware. This process is very massive\nand unnecessary communication-work load on the service middleware. Massive work\nload in large raster downloads from flat-file raster data sources each time a\nrequest is made and huge integration and geo-processing work load on the\nservice middleware which could actually be better leveraged at the database\nThis paper therefore proposes the realization of heterogeneous sensor database\nframework based on PostGIS for integration, geo-processing and spatial analysis\nof remote and in-situ sensor observations at the database level. Also discussed\nin this paper is how the framework can be integrated in the Sensor Observation\nService (SOS) to reduce communication and massive workload on the Geospatial\nWeb Services and as well make query request from the user end a lot more\nflexible. Keywords: Earth-Observation, Heterogeneous Earth-Observation Sensor\nDatabase, PostGIS , Sensor Observation Service.\n"]},
{"authors": ["Giorgio Stefanoni", "Boris Motik", "Egor V. Kostylev"], "title": ["Estimating the Cardinality of Conjunctive Queries over RDF Data Using\n  Graph Summarisation"], "date": ["2018-01-29T16:48:33Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.09619v2"], "summary": ["  Estimating the cardinality (i.e., the number of answers) of conjunctive\nqueries is particularly difficult in RDF systems: queries over RDF data are\nnavigational and thus tend to involve many joins. We present a new, principled\ncardinality estimation technique based on graph summarisation. We interpret a\nsummary of an RDF graph using a possible world semantics and formalise the\nestimation problem as computing the expected cardinality over all RDF graphs\nrepresented by the summary, and we present a closed-form formula for computing\nthe expectation of arbitrary queries. We also discuss approaches to RDF graph\nsummarisation. Finally, we show empirically that our cardinality technique is\nmore accurate and more consistent, often by orders of magnitude, than the state\nof the art.\n"]},
{"authors": ["Matteo Cossu", "Michael F\u00e4rber", "Georg Lausen"], "title": ["PRoST: Distributed Execution of SPARQL Queries Using Mixed Partitioning\n  Strategies"], "date": ["2018-02-16T11:25:15Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.05898v1"], "summary": ["  The rapidly growing size of RDF graphs in recent years necessitates\ndistributed storage and parallel processing strategies. To obtain efficient\nquery processing using computer clusters a wide variety of different approaches\nhave been proposed. Related to the approach presented in the current paper are\nsystems built on top of Hadoop HDFS, for example using Apache Accumulo or using\nApache Spark. We present a new RDF store called PRoST (Partitioned RDF on Spark\nTables) based on Apache Spark. PRoST introduces an innovative strategy that\ncombines the Vertical Partitioning approach with the Property Table, two\npreexisting models for storing RDF datasets. We demonstrate that our proposal\noutperforms state-of-the-art systems w.r.t. the runtime for a wide range of\nquery types and without any extensive precomputing phase.\n"]},
{"authors": ["Shumo Chu", "Alvin Cheung", "Dan Suciu"], "title": ["Axiomatic Foundations and Algorithms for Deciding Semantic Equivalences\n  of SQL Queries"], "date": ["2018-02-06T21:40:50Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.02229v2"], "summary": ["  Deciding the equivalence of SQL queries is a fundamental problem in data\nmanagement. As prior work has mainly focused on studying the theoretical\nlimitations of the problem, very few implementations for checking such\nequivalences exist. In this paper, we present a new formalism and\nimplementation for reasoning about the equivalences of SQL queries. Our\nformalism, U-semiring, extends SQL's semiring semantics with unbounded\nsummation and duplicate elimination. U-semiring is defined using only very few\naxioms and can thus be easily implemented using proof assistants such as Coq\nfor automated query reasoning. Yet, they are sufficient enough to enable us\nreason about sophisticated SQL queries that are evaluated over bags and sets,\nalong with various integrity constraints. To evaluate the effectiveness of\nU-semiring, we have used it to formally verify 39 query rewrite rules from both\nclassical data management research papers and real-world SQL engines, where\nmany of them have never been proven correct before.\n"]},
{"authors": ["Xin Yang", "Ju Fan"], "title": ["Influential User Subscription on Time-Decaying Social Streams"], "date": ["2018-02-14T20:08:35Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.05305v1"], "summary": ["  Influence maximization which asks for $k$-size seed set from a social network\nsuch that maximizing the influence over all other users (called influence\nspread) has widely attracted attention due to its significant applications in\nviral marketing and rumor control. In real world scenarios, people are\ninterested in the most influential users in particular topics, and want to\nsubscribe the topics-of-interests over social networks. In this paper, we\nformulate the problem of influential users subscription on time-decaying social\nstream, which asks for maintaining the $k$-size influential users sets for each\ntopic-aware subscription queries. We first analyze the widely adopted sliding\nwindow model and propose a newly time-decaying influence model to overcome the\nshortages when calculating the influence over social stream. Developed from\nsieve based streaming algorithm, we propose an efficient algorithm to support\nthe calculation of time-decaying influence over dynamically updating social\nnetworks. Using information among subscriptions, we then construct the Prefix\nTree Structure to allow us minimizing the times of calculating influence of\neach update and easily maintained. Pruning techniques are also applied to the\nPrefix Tree to optimize the performance of social stream update. Our approach\nensures a $\\frac{1}{2}-\\epsilon$ approximation ratio. Experimental results show\nthat our approach significantly outperforms the baseline approaches in\nefficiency and result quality.\n"]},
{"authors": ["Sheng Wang", "Tien Tuan Anh Dinh", "Qian Lin", "Zhongle Xie", "Meihui Zhang", "Qingchao Cai", "Gang Chen", "Wanzeng Fu", "Beng Chin Ooi", "Pingcheng Ruan"], "title": ["ForkBase: An Efficient Storage Engine for Blockchain and Forkable\n  Applications"], "date": ["2018-02-14T04:07:34Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.04949v1"], "summary": ["  Existing data storage systems offer a wide range of functionalities to\naccommodate an equally diverse range of applications. However, new classes of\napplications have emerged, e.g., blockchain and collaborative analytics,\nfeaturing data versioning, fork semantics, tamper-evidence or any combination\nthereof. They present new opportunities for storage systems to efficiently\nsupport such applications by embedding the above requirements into the storage.\n  In this paper, we present ForkBase, a storage engine specifically designed to\nprovide efficient support for blockchain and forkable applications. By\nintegrating the core application properties into the storage, ForkBase not only\ndelivers high performance but also reduces development effort. Data in ForkBase\nis multi-versioned, and each version uniquely identifies the data content and\nits history. Two variants of fork semantics are supported in ForkBase to\nfacilitate any collaboration workflows. A novel index structure is introduced\nto efficiently identify and eliminate duplicate content across data objects.\nConsequently, ForkBase is not only efficient in performance, but also in space\nrequirement. We demonstrate the performance of ForkBase using three\napplications: a blockchain platform, a wiki engine and a collaborative\nanalytics application. We conduct extensive experimental evaluation of these\napplications against respective state-of-the-art system. The results show that\nForkBase achieves superior performance while significantly lowering the\ndevelopment cost.\n"]},
{"authors": ["David Dao", "Dan Alistarh", "Claudiu Musat", "Ce Zhang"], "title": ["DataBright: Towards a Global Exchange for Decentralized Data Ownership\n  and Trusted Computation"], "date": ["2018-02-13T18:20:07Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.04780v1"], "summary": ["  It is safe to assume that, for the foreseeable future, machine learning,\nespecially deep learning will remain both data- and computation-hungry. In this\npaper, we ask: Can we build a global exchange where everyone can contribute\ncomputation and data to train the next generation of machine learning\napplications?\n  We present an early, but running prototype of DataBright, a system that turns\nthe creation of training examples and the sharing of computation into an\ninvestment mechanism. Unlike most crowdsourcing platforms, where the\ncontributor gets paid when they submit their data, DataBright pays dividends\nwhenever a contributor's data or hardware is used by someone to train a machine\nlearning model. The contributor becomes a shareholder in the dataset they\ncreated. To enable the measurement of usage, a computation platform that\ncontributors can trust is also necessary. DataBright thus merges both a data\nmarket and a trusted computation market.\n  We illustrate that trusted computation can enable the creation of an AI\nmarket, where each data point has an exact value that should be paid to its\ncreator. DataBright allows data creators to retain ownership of their\ncontribution and attaches to it a measurable value. The value of the data is\ngiven by its utility in subsequent distributed computation done on the\nDataBright computation market. The computation market allocates tasks and\nsubsequent payments to pooled hardware. This leads to the creation of a\ndecentralized AI cloud. Our experiments show that trusted hardware such as\nIntel SGX can be added to the usual ML pipeline with no additional costs. We\nuse this setting to orchestrate distributed computation that enables the\ncreation of a computation market. DataBright is available for download at\nhttps://github.com/ds3lab/databright.\n"]},
{"authors": ["Tommaso Soru", "Andr\u00e9 Valdestilhas", "Edgard Marx", "Axel-Cyrille Ngonga Ngomo"], "title": ["Beyond Markov Logic: Efficient Mining of Prediction Rules in Large\n  Graphs"], "date": ["2018-02-10T18:46:54Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.03638v2"], "summary": ["  Graph representations of large knowledge bases may comprise billions of\nedges. Usually built upon human-generated ontologies, several knowledge bases\ndo not feature declared ontological rules and are far from being complete.\nCurrent rule mining approaches rely on schemata or store the graph in-memory,\nwhich can be unfeasible for large graphs. In this paper, we introduce\nHornConcerto, an algorithm to discover Horn clauses in large graphs without the\nneed of a schema. Using a standard fact-based confidence score, we can mine\nclose Horn rules having an arbitrary body size. We show that our method can\noutperform existing approaches in terms of runtime and memory consumption and\nmine high-quality rules for the link prediction task, achieving\nstate-of-the-art results on a widely-used benchmark. Moreover, we find that\nrules alone can perform inference significantly faster than embedding-based\nmethods and achieve accuracies on link prediction comparable to\nresource-demanding approaches such as Markov Logic Networks.\n"]},
{"authors": ["Wojtek Kazana", "Luc Segoufin"], "title": ["First-order queries on classes of structures with bounded expansion"], "date": ["2018-02-13T13:29:50Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.04613v1"], "summary": ["  We consider the evaluation of first-order queries over classes of databases\nwith bounded expansion. The notion of bounded expansion is fairly broad and\ngeneralizes bounded degree, bounded treewidth and exclusion of at least one\nminor. It was known that over a class of databases with bounded expansion,\nfirst-order sentences could be evaluated in time linear in the size of the\ndatabase. We give a different proof of this result. Moreover, we show that\nanswers to first-order queries can be enumerated with constant delay after a\nlinear time preprocessing. We also show that counting the number of answers to\na query can be done in time linear in the size of the database.\n"]},
{"authors": ["Zhongyuan Xu", "Scott D. Stoller"], "title": ["Mining Attribute-Based Access Control Policies from Logs"], "date": ["2014-03-23T03:09:28Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1403.5715v5"], "summary": ["  Attribute-based access control (ABAC) provides a high level of flexibility\nthat promotes security and information sharing. ABAC policy mining algorithms\nhave potential to significantly reduce the cost of migration to ABAC, by\npartially automating the development of an ABAC policy from information about\nthe existing access-control policy and attribute data. This paper presents an\nalgorithm for mining ABAC policies from operation logs and attribute data. To\nthe best of our knowledge, it is the first algorithm for this problem.\n"]},
{"authors": ["Kijung Shin", "Euiwoong Lee", "Jinoh Oh", "Mohammad Hammoud", "Christos Faloutsos"], "title": ["DiSLR: Distributed Sampling with Limited Redundancy For Triangle\n  Counting in Graph Streams"], "date": ["2018-02-12T18:57:57Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.04249v1"], "summary": ["  Given a web-scale graph that grows over time, how should its edges be stored\nand processed on multiple machines for rapid and accurate estimation of the\ncount of triangles? The count of triangles (i.e., cliques of size three) has\nproven useful in many applications, including anomaly detection, community\ndetection, and link recommendation. For triangle counting in large and dynamic\ngraphs, recent work has focused largely on streaming algorithms and distributed\nalgorithms. To achieve the advantages of both approaches, we propose DiSLR, a\ndistributed streaming algorithm that estimates the counts of global triangles\nand local triangles associated with each node. Making one pass over the input\nstream, DiSLR carefully processes and stores the edges across multiple machines\nso that the redundant use of computational and storage resources is minimized.\nCompared to its best competitors, DiSLR is (a) Accurate: giving up to 39X\nsmaller estimation error, (b) Fast: up to 10.4X faster, scaling linearly with\nthe number of edges in the input stream, and (c) Theoretically sound: yielding\nunbiased estimates with variances decreasing faster as the number of machines\nis scaled up.\n"]},
{"authors": ["Harsh Thakkar", "Dharmen Punjani", "Yashwant Keswani", "Jens Lehmann", "S\u00f6ren Auer"], "title": ["A Stitch in Time Saves Nine -- SPARQL querying of Property Graphs using\n  Gremlin Traversals"], "date": ["2018-01-09T12:25:19Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.02911v2"], "summary": ["  Knowledge graphs have become popular over the past years and frequently rely\non the Resource Description Framework (RDF) or Property Graphs (PG) as\nunderlying data models. However, the query languages for these two data models\n-- SPARQL for RDF and Gremlin for property graph traversal -- are lacking\ninteroperability. We present Gremlinator, a novel SPARQL to Gremlin translator.\nGremlinator translates SPARQL queries to Gremlin traversals for executing graph\npattern matching queries over graph databases. This allows to access and query\na wide variety of Graph Data Management Systems (DMS) using the W3C\nstandardized SPARQL query language and avoid the learning curve of a new Graph\nQuery Language. Gremlin is a system-agnostic traversal language covering both\nOLTP graph database or OLAP graph processors, thus making it a desirable choice\nfor supporting interoperability wrt. querying Graph DMSs. We present a\ncomprehensive empirical evaluation of Gremlinator and demonstrate its validity\nand applicability by executing SPARQL queries on top of the leading graph\nstores Neo4J, Sparksee, and Apache TinkerGraph and compare the performance with\nthe RDF stores Virtuoso, 4Store and JenaTDB. Our evaluation demonstrates the\nsubstantial performance gain obtained by the Gremlin counterparts of the SPARQL\nqueries, especially for star-shaped and complex queries.\n"]},
{"authors": ["Davide Mottin", "Bastian Grasnick", "Axel Kroschk", "Patrick Siegler", "Emmanuel Mueller"], "title": ["Notable Characteristics Search through Knowledge Graphs"], "date": ["2018-02-12T14:24:55Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.04060v1"], "summary": ["  Query answering routinely employs knowledge graphs to assist the user in the\nsearch process. Given a knowledge graph that represents entities and\nrelationships among them, one aims at complementing the search with intuitive\nbut effective mechanisms. In particular, we focus on the comparison of two or\nmore entities and the detection of unexpected, surprising properties, called\nnotable characteristics. Such characteristics provide intuitive explanations of\nthe peculiarities of the selected entities with respect to similar entities. We\npropose a solid probabilistic approach that first retrieves entity nodes\nsimilar to the query nodes provided by the user, and then exploits\ndistributional properties to understand whether a certain attribute is\ninteresting or not. Our preliminary experiments demonstrate the solidity of our\napproach and show that we are able to discover notable characteristics that are\nindeed interesting and relevant for the user.\n"]},
{"authors": ["Giacomo Kahn", "Alexandre Bazin"], "title": ["Average Size of Implicational Bases"], "date": ["2018-02-12T13:31:27Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.04032v1"], "summary": ["  Implicational bases are objects of interest in formal concept analysis and\nits applications. Unfortunately, even the smallest base, the Duquenne-Guigues\nbase, has an exponential size in the worst case. In this paper, we use results\non the average number of minimal transversals in random hypergraphs to show\nthat the base of proper premises is, on average, of quasi-polynomial size.\n"]},
{"authors": ["Giacomo Kahn", "Alexandre Bazin"], "title": ["Introducer Concepts in n-Dimensional Contexts"], "date": ["2018-02-12T13:29:46Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.04030v1"], "summary": ["  Concept lattices are well-known conceptual structures that organise\ninteresting patterns-the concepts-extracted from data. In some applications,\nsuch as software engineering or data mining, the size of the lattice can be a\nproblem, as it is often too large to be efficiently computed, and too complex\nto be browsed. For this reason, the Galois Sub-Hierarchy, a restriction of the\nconcept lattice to introducer concepts, has been introduced as a smaller\nalternative. In this paper, we generalise the Galois Sub-Hierarchy to\nn-lattices, conceptual structures obtained from multidimensional data in the\nsame way that concept lattices are obtained from binary relations.\n"]},
{"authors": ["Feichen Shen", "Yugyung Lee"], "title": ["MedTQ: Dynamic Topic Discovery and Query Generation for Medical\n  Ontologies"], "date": ["2018-02-12T01:22:10Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.03855v1"], "summary": ["  Biomedical ontology refers to a shared conceptualization for a biomedical\ndomain of interest that has vastly improved data management and data sharing\nthrough the open data movement. The rapid growth and availability of biomedical\ndata make it impractical and computationally expensive to perform manual\nanalysis and query processing with the large scale ontologies. The lack of\nability in analyzing ontologies from such a variety of sources, and supporting\nknowledge discovery for clinical practice and biomedical research should be\novercome with new technologies. In this study, we developed a Medical Topic\ndiscovery and Query generation framework (MedTQ), which was composed by a\nseries of approaches and algorithms. A predicate neighborhood pattern-based\napproach introduced has the ability to compute the similarity of predicates\n(relations) in ontologies. Given a predicate similarity metric, machine\nlearning algorithms have been developed for automatic topic discovery and query\ngeneration. The topic discovery algorithm, called the hierarchical K-Means\nalgorithm was designed by extending an existing supervised algorithm (K-means\nclustering) for the construction of a topic hierarchy. In the hierarchical\nK-Means algorithm, a level-by-level optimization strategy was selected for\nconsistent with the strongly association between elements within a topic.\nAutomatic query generation was facilitated for discovered topic that could be\nguided users for interactive query design and processing. Evaluation was\nconducted to generate topic hierarchy for DrugBank ontology as a case study.\nResults demonstrated that the MedTQ framework can enhance knowledge discovery\nby capturing underlying structures from domain specific data and ontologies.\n"]},
{"authors": ["Khaled Ammar", "Frank McSherry", "Semih Salihoglu", "Manas Joglekar"], "title": ["Distributed Evaluation of Subgraph Queries Using Worstcase Optimal\n  LowMemory Dataflows"], "date": ["2018-02-11T16:08:47Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.03760v1"], "summary": ["  We study the problem of finding and monitoring fixed-size subgraphs in a\ncontinually changing large-scale graph. We present the first approach that (i)\nperforms worst-case optimal computation and communication, (ii) maintains a\ntotal memory footprint linear in the number of input edges, and (iii) scales\ndown per-worker computation, communication, and memory requirements linearly as\nthe number of workers increases, even on adversarially skewed inputs.\n  Our approach is based on worst-case optimal join algorithms, recast as a\ndata-parallel dataflow computation. We describe the general algorithm and\nmodifications that make it robust to skewed data, prove theoretical bounds on\nits resource requirements in the massively parallel computing model, and\nimplement and evaluate it on graphs containing as many as 64 billion edges. The\nunderlying algorithm and ideas generalize from finding and monitoring subgraphs\nto the more general problem of computing and maintaining relational equi-joins\nover dynamic relations.\n"]},
{"authors": ["Kyung-Joong Kim", "DuMim Yoon", "JiHoon Jeon", "Seong-il Yang", "Sang-Kwang Lee", "EunJo Lee", "Yoonjae Jang", "Dae-Wook Kim", "Pei Pei Chen", "Anna Guitart", "Paul Bertens", "\u00c1frica Peri\u00e1\u00f1ez", "Fabian Hadiji", "Marc M\u00fcller", "Youngjun Joo", "Jiyeon Lee", "Inchon Hwang"], "title": ["Game Data Mining Competition on Churn Prediction and Survival Analysis\n  using Commercial Game Log Data"], "date": ["2018-02-07T04:20:24Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.02301v2"], "summary": ["  Usually, game companies avoid sharing their game data with external\nresearchers and only a few number of research groups were granted limited\naccess to gam data so far. Such reluctance of the companies to make data\npublicly available closes doors on the wide use and development of the data\nmining techniques and AI research specific to the game industry. In this work,\nwe propose an international competition on game data mining using the\ncommercial game log data from one of the major game companies in Korea: NCSOFT.\nIt provides opportunities to many researchers who wish to develop and apply\nstate-of-the-art data mining techniques to game log data by making the data\nopen. The data has been collected from Blade & Soul, Action Role Playing Game,\nfrom NCSoft. The data comprises of approximately 100GB of game logs from 10,000\nplayers. The main aim of the competition was to predict whether a player would\nchurn and when the player would churn during two different periods between\nwhich its business model was changed to the free-to-play model from monthly\nfixed charged one. The final result of the competition reveals that the highly\nranked competitors used deep learning, tree boosting, and linear regression.\n"]},
{"authors": ["Gabriel Tanase", "Toyotaro Suzumura", "Jinho Lee", " Chun-Fu", " Chen", "Jason Crawford", "Hiroki Kanezashi", "Song Zhang", "Warut D. Vijitbenjaronk"], "title": ["System G Distributed Graph Database"], "date": ["2018-02-08T21:42:28Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.03057v1"], "summary": ["  Motivated by the need to extract knowledge and value frominterconnected data,\ngraph analytics on big data is a veryactive area of research in both industry\nand academia. Tosupport graph analytics efficiently a large number of in\nmem-ory graph libraries, graph processing systems and graphdatabases have\nemerged. Projects in each of these cate-gories focus on particular aspects such\nas static versus dy-namic graphs, off line versus on line processing, small\nversuslarge graphs, etc.While there has been much advance in graph processingin\nthe past decades, there is still a need for a fast graph pro-cessing, using a\ncluster of machines with distributed storage.In this paper, we discuss a novel\ndistributed graph databasecalled System G designed for efficient graph data\nstorage andprocessing on modern computing architectures. In particu-lar we\ndescribe a single node graph database and a runtimeand communication layer that\nallows us to compose a dis-tributed graph database from multiple single node\ninstances.From various industry requirements, we find that fast inser-tions and\nlarge volume concurrent queries are critical partsof the graph databases and we\noptimize our database forsuch features. We experimentally show the efficiency\nofSystem G for storing data and processing graph queries onstate-of-the-art\nplatforms.\n"]},
{"authors": ["Christian Bessiere", "Nadjib Lazaar", "Yahia Lebbah", "Mehdi Maamar"], "title": ["Users Constraints in Itemset Mining"], "date": ["2017-12-31T19:55:52Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.00345v2"], "summary": ["  Discovering significant itemsets is one of the fundamental problems in data\nmining. It has recently been shown that constraint programming is a flexible\nway to tackle data mining tasks. With a constraint programming approach, we can\neasily express and efficiently answer queries with users constraints on items.\nHowever, in many practical cases it is possible that queries also express users\nconstraints on the dataset itself. For instance, asking for a particular\nitemset in a particular part of the dataset. This paper presents a general\nconstraint programming model able to handle any kind of query on the items or\nthe dataset for itemset mining.\n"]},
{"authors": ["George Christodoulides"], "title": ["Praaline: Integrating Tools for Speech Corpus Research"], "date": ["2018-02-08T15:15:51Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.02914v1"], "summary": ["  This paper presents Praaline, an open-source software system for managing,\nannotating, analysing and visualising speech corpora. Researchers working with\nspeech corpora are often faced with multiple tools and formats, and they need\nto work with ever-increasing amounts of data in a collaborative way. Praaline\nintegrates and extends existing time-proven tools for spoken corpora analysis\n(Praat, Sonic Visualiser and a bridge to the R statistical package) in a\nmodular system, facilitating automation and reuse. Users are exposed to an\nintegrated, user-friendly interface from which to access multiple tools. Corpus\nmetadata and annotations may be stored in a database, locally or remotely, and\nusers can define the metadata and annotation structure. Users may run a\ncustomisable cascade of analysis steps, based on plug-ins and scripts, and\nupdate the database with the results. The corpus database may be queried, to\nproduce aggregated data-sets. Praaline is extensible using Python or C++\nplug-ins, while Praat and R scripts may be executed against the corpus data. A\nseries of visualisations, editors and plug-ins are provided. Praaline is free\nsoftware, released under the GPL license.\n"]},
{"authors": ["Marie Le Guilly", "Jean-Marc Petit", "Vasile-Marian Scuturici"], "title": ["SQL Query Completion for Data Exploration"], "date": ["2018-02-08T14:20:30Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.02872v1"], "summary": ["  Within the big data tsunami, relational databases and SQL are still there and\nremain mandatory in most of cases for accessing data. On the one hand, SQL is\neasy-to-use by non specialists and allows to identify pertinent initial data at\nthe very beginning of the data exploration process. On the other hand, it is\nnot always so easy to formulate SQL queries: nowadays, it is more and more\nfrequent to have several databases available for one application domain, some\nof them with hundreds of tables and/or attributes. Identifying the pertinent\nconditions to select the desired data, or even identifying relevant attributes\nis far from trivial. To make it easier to write SQL queries, we propose the\nnotion of SQL query completion: given a query, it suggests additional\nconditions to be added to its WHERE clause. This completion is semantic, as it\nrelies on the data from the database, unlike current completion tools that are\nmostly syntactic. Since the process can be repeated over and over again --\nuntil the data analyst reaches her data of interest --, SQL query completion\nfacilitates the exploration of databases. SQL query completion has been\nimplemented in a SQL editor on top of a database management system. For the\nevaluation, two questions need to be studied: first, does the completion speed\nup the writing of SQL queries? Second , is the completion easily adopted by\nusers? A thorough experiment has been conducted on a group of 70 computer\nscience students divided in two groups (one with the completion and the other\none without) to answer those questions. The results are positive and very\npromising.\n"]},
{"authors": ["Thed van Leeuwen", "Ingeborg Meijer", "Alfredo Yegros-Yegros", "Rodrigo Costas"], "title": ["Developing indicators on Open Access by combining evidence from diverse\n  data sources"], "date": ["2018-02-08T12:36:57Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.02827v1"], "summary": ["  In the last couple of years, the role of Open Access (OA) publishing has\nbecome central in science management and research policy. In the UK and the\nNetherlands, national OA mandates require the scientific community to seriously\nconsider publishing research outputs in OA forms. At the same time, other\nelements of Open Science are becoming also part of the debate, thus including\nnot only publishing research outputs but also other related aspects of the\nchain of scientific knowledge production such as open peer review and open\ndata. From a research management point of view, it is important to keep track\nof the progress made in the OA publishing debate. Until now, this has been\nquite problematic, given the fact that OA as a topic is hard to grasp by\nbibliometric methods, as most databases supporting bibliometric data lack\nexhaustive and accurate open access labelling of scientific publications. In\nthis study, we present a methodology that systematically creates OA labels for\nlarge sets of publications processed in the Web of Science database. The\nmethodology is based on the combination of diverse data sources that provide\nevidence of publications being OA\n"]},
{"authors": ["Chang Ge", "Ihab F. Ilyas", "Xi He", "Ashwin Machanavajjhala"], "title": ["Private Exploration Primitives for Data Cleaning"], "date": ["2017-12-29T16:09:37Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1712.10266v3"], "summary": ["  Data cleaning, or the process of detecting and repairing inaccurate or\ncorrupt records in the data, is inherently human-driven. State of the art\nsystems assume cleaning experts can access the data (or a sample of it) to tune\nthe cleaning process. However, in many cases, privacy constraints disallow\nunfettered access to the data. To address this challenge, we observe and\nprovide empirical evidence that data cleaning can be achieved without access to\nthe sensitive data, but with access to a (noisy) query interface that supports\na small set of linear counting query primitives. Motivated by this, we present\nDPClean, a first of a kind system that allows engineers tune data cleaning\nworkflows while ensuring differential privacy. In DPClean, a cleaning engineer\ncan pose sequences of aggregate counting queries with error tolerances. A\nprivacy engine translates each query into a differentially private mechanism\nthat returns an answer with error matching the specified tolerance, and allows\nthe data owner track the overall privacy loss. With extensive experiments using\nhuman and simulated cleaning engineers on blocking and matching tasks, we\ndemonstrate that our approach is able to achieve high cleaning quality while\nensuring a reasonable privacy loss.\n"]},
{"authors": ["Ping Zhang", "Zhifeng Bao", "Yuchen Li", "Guoliang Li", "Yipeng Zhang", "Zhiyong Peng"], "title": ["Trajectory-driven Influential Billboard Placement"], "date": ["2018-02-06T23:05:02Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.02254v1"], "summary": ["  In this paper we propose and study the problem of trajectory-driven\ninfluential billboard placement: given a set of billboards $U$ (each associated\nwith a location and a cost), a database of trajectories $\\mathcal{T}$ and a\nbudget $L$, the goal is to find a set of billboards within the budget so that\nthe placed ad can influence the largest number of trajectories. One core\nchallenge is that multiple billboards have influence overlap on trajectories\nand it is critical to identify and reduce the influence overlap. Two more\nconstraints on top of this challenge, i.e., the budget constraint and\nnon-uniform costs associated with each billboard, make this optimization\nproblem more intricate. We show that this problem is NP-hard and propose an\nenumeration based algorithm with $(1-1/e)$ approximation ratio and\n$O(|\\mathcal{T}|\\cdot|U|^{5})$ time complexity, where $|\\mathcal{T}|$ and $|U|$\nare the number of trajectories and billboards respectively. By exploiting the\nlocality property of billboards influence, we propose a partition-based\nframework \\psel. \\psel partitions $U$ into a set of small clusters, computes\nthe locally influential billboards for each cluster, and merges the local\nbillboards to generate the globally influential billboards of $U$. As a result,\nthe computation cost is reduced to $O(|\\mathcal{T}|\\cdot|C_m|^5)$, where\n$|C_m|$ is the cardinality of the largest partition in $U$. Furthermore, we\npropose a \\bbsel method to further prune billboards with low marginal\ninfluence; \\bbsel significantly reduces the practical cost of \\psel while\nachieving the same approximation ratio as \\psel. Experiments on real datasets\nshow that our method achieves high influence and efficiency.\n"]},
{"authors": ["Kijung Shin", "Bryan Hooi", "Jisu Kim", "Christos Faloutsos"], "title": ["Out-of-Core and Distributed Algorithms for Dense Subtensor Mining"], "date": ["2018-02-04T03:06:53Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.01065v2"], "summary": ["  How can we detect fraudulent lockstep behavior in large-scale multi-aspect\ndata (i.e., tensors)? Can we detect it when data are too large to fit in memory\nor even on a disk? Past studies have shown that dense subtensors in real-world\ntensors (e.g., social media, Wikipedia, TCP dumps, etc.) signal anomalous or\nfraudulent behavior such as retweet boosting, bot activities, and network\nattacks. Thus, various approaches, including tensor decomposition and search,\nhave been proposed for detecting dense subtensors rapidly and accurately.\nHowever, existing methods have low accuracy, or they assume that tensors are\nsmall enough to fit in main memory, which is unrealistic in many real-world\napplications such as social media and web. To overcome these limitations, we\npropose D-CUBE, a disk-based dense-subtensor detection method, which also can\nrun in a distributed manner across multiple machines. Compared to\nstate-of-the-art methods, D-CUBE is (1) Memory Efficient: requires up to 1,600X\nless memory and handles 1,000X larger data (2.6TB), (2) Fast: up to 7X faster\ndue to its near-linear scalability, (3) Provably Accurate: gives a guarantee on\nthe densities of the detected subtensors, and (4) Effective: spotted network\nattacks from TCP dumps and synchronized behavior in rating data most\naccurately.\n"]},
{"authors": ["Maryam Fanaeepour", "Benjamin I. P. Rubinstein"], "title": ["Differentially-Private Counting of Users' Spatial Regions"], "date": ["2016-09-26T14:24:53Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1609.07983v2"], "summary": ["  Mining of spatial data is an enabling technology for mobile services,\nInternet-connected cars, and the Internet of Things. But the very\ndistinctiveness of spatial data that drives utility, can cost user privacy.\nPast work has focused upon points and trajectories for differentially-private\nrelease. In this work, we continue the tradition of privacy-preserving spatial\nanalytics, focusing not on point or path data, but on planar spatial regions.\nSuch data represents the area of a user's most frequent visitation---such as\n\"around home and nearby shops\". Specifically we consider the\ndifferentially-private release of data structures that support range queries\nfor counting users' spatial regions. Counting planar regions leads to unique\nchallenges not faced in existing work. A user's spatial region that straddles\nmultiple data structure cells can lead to duplicate counting at query time. We\nprovably avoid this pitfall by leveraging the Euler characteristic for the\nfirst time with differential privacy. To address the increased sensitivity of\nrange queries to spatial region data, we calibrate privacy-preserving noise\nusing bounded user region size and a constrained inference that uses robust\nleast absolute deviations. Our novel constrained inference reduces noise and\npromotes covertness by (privately) imposing consistency. We provide a full\nend-to-end theoretical analysis of both differential privacy and\nhigh-probability utility for our approach using concentration bounds. A\ncomprehensive experimental study on several real-world datasets establishes\npractical validity.\n"]},
{"authors": ["Grzegorz G\u0142uch", "Jerzy Marcinkowski", "Piotr Ostropolski-Nalewaja"], "title": ["Can One Escape Red Chains? Regular Path Queries Determinacy is\n  Undecidable"], "date": ["2018-02-05T18:33:16Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.01554v1"], "summary": ["  For a given set of queries (which are expressions in some query language)\n$\\mathcal{Q}=\\{Q_1$, $Q_2, \\ldots Q_k\\}$ and for another query $Q_0$ we say\nthat $\\mathcal{Q}$ determines $Q_0$ if -- informally speaking -- for every\ndatabase $\\mathbb D$, the information contained in the views\n$\\mathcal{Q}({\\mathbb D})$ is sufficient to compute $Q_0({\\mathbb D})$. Query\nDeterminacy Problem is the problem of deciding, for given $\\mathcal{Q}$ and\n$Q_0$, whether $\\mathcal{Q}$ determines $Q_0$. Many versions of this problem,\nfor different query languages, were studied in database theory. In this paper\nwe solve a problem stated in [CGLV02] and show that Query Determinacy Problem\nis undecidable for the Regular Path Queries -- the paradigmatic query language\nof graph databases.\n"]},
{"authors": ["Dominik D. Freydenberger", "Markus L. Schmid"], "title": ["Deterministic Regular Expressions With Back-References"], "date": ["2018-02-05T16:47:45Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.01508v1"], "summary": ["  Most modern libraries for regular expression matching allow back-references\n(i.e., repetition operators) that substantially increase expressive power, but\nalso lead to intractability. In order to find a better balance between\nexpressiveness and tractability, we combine these with the notion of\ndeterminism for regular expressions used in XML DTDs and XML Schema. This\nincludes the definition of a suitable automaton model, and a generalization of\nthe Glushkov construction. We demonstrate that, compared to their\nnon-deterministic superclass, these deterministic regular expressions with\nback-references have desirable algorithmic properties (i.e., efficiently\nsolvable membership problem and some decidable problems in static analysis),\nwhile, at the same time, their expressive power exceeds that of deterministic\nregular expressions without back-references.\n"]},
{"authors": ["Shrainik Jain", "Bill Howe", "Jiaqi Yan", "Thierry Cruanes"], "title": ["Query2Vec: An Evaluation of NLP Techniques for Generalized Workload\n  Analytics"], "date": ["2018-01-17T10:21:49Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.05613v2"], "summary": ["  We consider methods for learning vector representations of SQL queries to\nsupport generalized workload analytics tasks, including workload summarization\nfor index selection and predicting queries that will trigger memory errors. We\nconsider vector representations of both raw SQL text and optimized query plans,\nand evaluate these methods on synthetic and real SQL workloads. We find that\ngeneral algorithms based on vector representations can outperform existing\napproaches that rely on specialized features. For index recommendation, we\ncluster the vector representations to compress large workloads with no loss in\nperformance from the recommended index. For error prediction, we train a\nclassifier over learned vectors that can automatically relate subtle syntactic\npatterns with specific errors raised during query execution. Surprisingly, we\nalso find that these methods enable transfer learning, where a model trained on\none SQL corpus can be applied to an unrelated corpus and still enable good\nperformance. We find that these general approaches, when trained on a large\ncorpus of SQL queries, provides a robust foundation for a variety of workload\nanalysis tasks and database features, without requiring application-specific\nfeature engineering.\n"]},
{"authors": ["Davide Lanti", "Guohui Xiao", "Diego Calvanese"], "title": ["Cost-Driven Ontology-Based Data Access (Extended Version)"], "date": ["2017-07-21T17:08:59Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1707.06974v2"], "summary": ["  In ontology-based data access (OBDA), users are provided with a conceptual\nview of a (relational) data source that abstracts away details about data\nstorage. This conceptual view is realized through an ontology that is connected\nto the data source through declarative mappings, and query answering is carried\nout by translating the user queries over the conceptual view into SQL queries\nover the data source. Standard translation techniques in OBDA try to transform\nthe user query into a union of conjunctive queries (UCQ), following the\nheuristic argument that UCQs can be efficiently evaluated by modern relational\ndatabase engines. In this work, we show that translating to UCQs is not always\nthe best choice, and that, under certain conditions on the interplay between\nthe ontology, the map- pings, and the statistics of the data, alternative\ntranslations can be evaluated much more efficiently. To find the best\ntranslation, we devise a cost model together with a novel cardinality\nestimation that takes into account all such OBDA components. Our experiments\nconfirm that (i) alternatives to the UCQ translation might produce queries that\nare orders of magnitude more efficient, and (ii) the cost model we propose is\nfaithful to the actual query evaluation cost, and hence is well suited to\nselect the best translation.\n"]},
{"authors": ["Diego Didona", "Willy Zwaenepoel"], "title": ["Size-aware Sharding For Improving Tail Latencies in In-memory Key-value\n  Stores"], "date": ["2018-02-02T14:23:00Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.00696v1"], "summary": ["  This paper introduces the concept of size-aware sharding to improve tail\nlatencies for in-memory key-value stores, and describes its implementation in\nthe Minos key-value store. Tail latencies are crucial in distributed\napplications with high fan-out ratios, because overall response time is\ndetermined by the slowest response. Size-aware sharding distributes requests\nfor keys to cores according to the size of the item associated with the key. In\nparticular, requests for small and large items are sent to disjoint subsets of\ncores. Size-aware sharding improves tail latencies by avoiding head-of-line\nblocking, in which a request for a small item gets queued behind a request for\na large item. Alternative size-unaware approaches to sharding, such as\nkeyhash-based sharding, request dispatching and stealing do not avoid\nhead-of-line blocking, and therefore exhibit worse tail latencies. The\nchallenge in implementing size-aware sharding is to maintain high throughput by\navoiding the cost of software dispatching and by achieving load balancing\nbetween different cores. Minos uses hardware dispatch for all requests for\nsmall items, which form the very large majority of all requests. It achieves\nload balancing by adapting the number of cores handling requests for small and\nlarge items to their relative presence in the workload. We compare Minos to\nthree state-of-the-art designs of in-memory KV stores. Compared to its closest\ncompetitor, Minos achieves a 99th percentile latency that is up to two orders\nof magnitude lower. Put differently, for a given value for the 99th percentile\nlatency equal to 10 times the mean service time, Minos achieves a throughput\nthat is up to 7.4 times higher.\n"]},
{"authors": ["Malika Bendechache", "M-Tahar Kechadi"], "title": ["Distributed Clustering Algorithm for Spatial Data Mining"], "date": ["2018-02-01T14:41:33Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.00304v1"], "summary": ["  Distributed data mining techniques and mainly distributed clustering are\nwidely used in the last decade because they deal with very large and\nheterogeneous datasets which cannot be gathered centrally. Current distributed\nclustering approaches are normally generating global models by aggregating\nlocal results that are obtained on each site. While this approach mines the\ndatasets on their locations the aggregation phase is complex, which may produce\nincorrect and ambiguous global clusters and therefore incorrect knowledge. In\nthis paper we propose a new clustering approach for very large spatial datasets\nthat are heterogeneous and distributed. The approach is based on K-means\nAlgorithm but it generates the number of global clusters dynamically. Moreover,\nthis approach uses an elaborated aggregation phase. The aggregation phase is\ndesigned in such a way that the overall process is efficient in time and memory\nallocation. Preliminary results show that the proposed approach produces high\nquality results and scales up well. We also compared it to two popular\nclustering algorithms and show that this approach is much more efficient.\n"]},
{"authors": ["Malika Bendechache", "Nhien-An Le-Khac", "M-Tahar Kechadi"], "title": ["Hierarchical Aggregation Approach for Distributed clustering of spatial\n  datasets"], "date": ["2018-02-01T12:50:59Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.00688v1"], "summary": ["  In this paper, we present a new approach of distributed clustering for\nspatial datasets, based on an innovative and efficient aggregation technique.\nThis distributed approach consists of two phases: 1) local clustering phase,\nwhere each node performs a clustering on its local data, 2) aggregation phase,\nwhere the local clusters are aggregated to produce global clusters. This\napproach is characterised by the fact that the local clusters are represented\nin a simple and efficient way. And The aggregation phase is designed in such a\nway that the final clusters are compact and accurate while the overall process\nis efficient in both response time and memory allocation. We evaluated the\napproach with different datasets and compared it to well-known clustering\ntechniques. The experimental results show that our approach is very promising\nand outperforms all those algorithms\n"]},
{"authors": ["Timo Schindler"], "title": ["Anomaly Detection in Log Data using Graph Databases and Machine Learning\n  to Defend Advanced Persistent Threats"], "date": ["2018-02-01T12:17:54Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.00259v1"], "summary": ["  Advanced Persistent Threats (APTs) are a main impendence in cyber security of\ncomputer networks. In 2015, a successful breach remains undetected 146 days on\naverage, reported by [Fi16].With our work we demonstrate a feasible and fast\nway to analyse real world log data to detect breaches or breach attempts. By\nadapting well-known kill chain mechanisms and a combine of a time series\ndatabase and an abstracted graph approach, it is possible to create flexible\nattack profiles. Using this approach, it can be demonstrated that the graph\nanalysis successfully detects simulated attacks by analysing the log data of a\nsimulated computer network. Considering another source for log data, the\nframework is capable to deliver sufficient performance for analysing real-world\ndata in short time. By using the computing power of the graph database it is\npossible to identify the attacker and furthermore it is feasible to detect\nother affected system components. We believe to significantly reduce the\ndetection time of breaches with this approach and react fast to new attack\nvectors.\n"]},
{"authors": ["Dan Kondratyuk", "Jake Rodden", "Elmer Duran"], "title": ["Integrity Coded Databases: An Evaluation of Performance, Efficiency, and\n  Practicality"], "date": ["2018-02-01T10:30:20Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1802.00230v1"], "summary": ["  In recent years, cloud database storage has become an inexpensive and\nconvenient option for businesses and individuals to store information. While\nits positive aspects make the cloud extremely attractive for data storage, it\nis a relatively new area of service, making it vulnerable to cyber-attacks and\nsecurity breaches. Storing data in a foreign location also requires the owner\nto relinquish control of their information to system administrators of these\nonline database services. This opens the possibility for malicious, internal\nattacks on the data that may involve the manipulation, omission, or addition of\ndata. The retention of the data as it was intended to be stored is referred to\nas the database's integrity. Our research tests a potential solution for\nmaintaining the integrity of these cloud-storage databases by converting the\noriginal databases to Integrity Coded Databases (ICDB). ICDBs utilize Integrity\nCodes: cryptographic codes created alongside the data by a private key that\nonly the data owner has access to. When the database is queried, an integrity\ncode is returned along with the queried information. The owner is then able to\nverify that the information is correct, complete, and fresh. Consequently,\nICDBs also incur performance and memory penalties. In our research, we explore,\ntest, and benchmark ICDBs to determine the costs and benefits of maintaining an\nICDB versus a standard database.\n"]},
{"authors": ["Shlomi Dolev", "Yin Li", "Shantanu Sharma"], "title": ["Privacy-Preserving Secret Shared Computations using MapReduce"], "date": ["2018-01-31T07:02:10Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.10323v1"], "summary": ["  Data outsourcing allows data owners to keep their data at untrusted clouds\nthat do not ensure the privacy of data and/or computations. One useful\nframework for fault-tolerant data processing in a distributed fashion is\nMapReduce, which was developed for trusted private clouds. This paper presents\nalgorithms for data outsourcing based on Shamir's secret-sharing scheme and for\nexecuting privacy-preserving SQL queries such as count, selection including\nrange selection, projection, and join while using MapReduce as an underlying\nprogramming model. The proposed algorithms prevent the untrusted cloud to know\nthe database or the query while also preventing output size and access-pattern\nattacks. Interestingly, our algorithms do not need the database owner, which\nonly creates and distributes secret-shares once, to be involved to answer any\nquery, and hence, the database owner also cannot learn the query. We evaluate\nthe efficiency of the algorithms on parameters: (i) the number of communication\nrounds (between a user and a cloud), (ii) the total amount of bit flow (between\na user and a cloud), and (iii) the computational load at the user-side and the\ncloud-side.\n"]},
{"authors": ["Yuhang Zhang", "Tania Churchill", "Kee Siong Ng"], "title": ["Exploiting Redundancy, Recurrence and Parallelism: How to Link Millions\n  of Addresses with Ten Lines of Code in Ten Minutes"], "date": ["2017-08-04T07:26:20Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1708.01402v4"], "summary": ["  Accurate and efficient record linkage is an open challenge of particular\nrelevance to Australian Government Agencies, who recognise that so-called\nwicked social problems are best tackled by forming partnerships founded on\nlarge-scale data fusion. Names and addresses are the most common attributes on\nwhich data from different government agencies can be linked. In this paper, we\nfocus on the problem of address linking. Linkage is particularly problematic\nwhen the data has significant quality issues. The most common approach for\ndealing with quality issues is to standardise raw data prior to linking. If a\nmistake is made in standardisation, however, it is usually impossible to\nrecover from it to perform linkage correctly. This paper proposes a novel\nalgorithm for address linking that is particularly practical for linking large\ndisparate sets of addresses, being highly scalable, robust to data quality\nissues and simple to implement. It obviates the need for labour intensive and\nproblematic address standardisation. We demonstrate the efficacy of the\nalgorithm by matching two large address datasets from two government agencies\nwith good accuracy and computational efficiency.\n"]},
{"authors": ["Alex Galakatos", "Michael Markovitch", "Carsten Binnig", "Rodrigo Fonseca", "Tim Kraska"], "title": ["A-Tree: A Bounded Approximate Index Structure"], "date": ["2018-01-30T20:22:53Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.10207v1"], "summary": ["  Index structures are one of the most important tools that DBAs leverage in\norder to improve the performance of analytics and transactional workloads.\nHowever, with the explosion of data that is constantly being generated in a\nwide variety of domains including autonomous vehicles, Internet of Things (IoT)\ndevices, and E-commerce sites, building several indexes can often become\nprohibitive and consume valuable system resources. In fact, a recent study has\nshown that indexes created as part of the TPC-C benchmark can account for 55%\nof the total memory available in a state-of-the-art in-memory DBMS. This\noverhead consumes valuable and expensive main memory, and limits the amount of\nspace that a database has available to store new data or process existing data.\n  In this paper, we present a novel approximate index structure called A-Tree.\nAt the core of our index is a tunable error parameter that allows a DBA to\nbalance lookup performance and space consumption. To navigate this tradeoff, we\nprovide a cost model that helps the DBA choose an appropriate error parameter\ngiven either (1) a lookup latency requirement (e.g., 500ns) or (2) a storage\nbudget (e.g., 100MB). Using a variety of real-world datasets, we show that our\nindex structure is able to provide performance that is comparable to full index\nstructures while reducing the storage footprint by orders of magnitude.\n"]},
{"authors": ["Maaz Bin Safeer Ahmad", "Alvin Cheung"], "title": ["Automatically Leveraging MapReduce Frameworks for Data-Intensive\n  Applications"], "date": ["2018-01-30T00:02:57Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.09802v1"], "summary": ["  MapReduce is a popular programming paradigm for running large-scale\ndata-intensive computation. Recently, many frameworks that implement that\nparadigm have been developed. To leverage such frameworks, however, developers\nneed to familiarize with each framework's API and rewrite their code. We\npresent CORA, a new tool that automatically translates sequential Java programs\nto the MapReduce paradigm. Rather than building a compiler by tediously\ndesigning pattern-matching rules to identify code fragments to translate from\nthe input, CORA translates the input program in two steps: first, CORA uses\nprogram synthesis to identify input code fragments and search for a program\nsummary (i.e., a functional specification) of each fragment. The summary is\nexpressed using a high-level intermediate language resembling the MapReduce\nparadigm. Next, each found summary is verified to be semantically equivalent to\nthe original using a theorem prover. CORA then generates executable code from\nthe summary, using either the Hadoop, Spark, or Flink API. We have evaluated\nCORA by automatically converting real-world sequential Java benchmarks to\nMapReduce. The resulting benchmarks perform up to 32.2x faster compared to the\noriginal, and are all translated without designing any pattern-matching rules.\n"]},
{"authors": ["Brian Hentschel", "Peter J. Haas", "Yuanyuan Tian"], "title": ["Temporally-Biased Sampling for Online Model Management"], "date": ["2018-01-29T19:10:15Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.09709v1"], "summary": ["  To maintain the accuracy of supervised learning models in the presence of\nevolving data streams, we provide temporally-biased sampling schemes that\nweight recent data most heavily, with inclusion probabilities for a given data\nitem decaying exponentially over time. We then periodically retrain the models\non the current sample. This approach speeds up the training process relative to\ntraining on all of the data. Moreover, time-biasing lets the models adapt to\nrecent changes in the data while -- unlike in a sliding-window approach --\nstill keeping some old data to ensure robustness in the face of temporary\nfluctuations and periodicities in the data values. In addition, the\nsampling-based approach allows existing analytic algorithms for static data to\nbe applied to dynamic streaming data essentially without change. We provide and\nanalyze both a simple sampling scheme (T-TBS) that probabilistically maintains\na target sample size and a novel reservoir-based scheme (R-TBS) that is the\nfirst to provide both complete control over the decay rate and a guaranteed\nupper bound on the sample size, while maximizing both expected sample size and\nsample-size stability. The latter scheme rests on the notion of a \"fractional\nsample\" and, unlike T-TBS, allows for data arrival rates that are unknown and\ntime varying. R-TBS and T-TBS are of independent interest, extending the known\nset of unequal-probability sampling schemes. We discuss distributed\nimplementation strategies; experiments in Spark illuminate the performance and\nscalability of the algorithms, and show that our approach can increase machine\nlearning robustness in the face of evolving data.\n"]},
{"authors": ["Genqiang Wu", "Xianyao Xia", "Yeping He"], "title": ["Analytic Theory to Differential Privacy"], "date": ["2017-02-09T06:34:19Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.02721v3"], "summary": ["  The purpose of this paper is to develop a mathematical analysis theory to\nsolve differential privacy problems. The heart of our approaches is to use\nanalytic tools to characterize the correlations among the outputs of different\ndatasets, which makes it feasible to represent a differentially private\nmechanism with minimal number of parameters. These results are then used to\nconstruct differentially private mechanisms analytically. Furthermore, our\napproaches are universal to almost all query functions. We believe that the\napproaches and results of this paper are indispensable complements to the\ncurrent studies of differential privacy that are ruled by the ad hoc and\nalgorithmic approaches.\n"]},
{"authors": ["Hui Li", "Sizhe Peng", "Jian Li", "Jingjing Li", "Jiangtao Cui", "Jianfeng Ma"], "title": ["ONCE and ONCE+: Counting the Frequency of Time-constrained Serial\n  Episodes in a Streaming Sequence"], "date": ["2018-01-29T17:26:07Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.09639v1"], "summary": ["  As a representative sequential pattern mining problem, counting the frequency\nof serial episodes from a streaming sequence has drawn continuous attention in\nacademia due to its wide application in practice, e.g., telecommunication\nalarms, stock market, transaction logs, bioinformatics, etc. Although a number\nof serial episodes mining algorithms have been developed recently, most of them\nare neither stream-oriented, as they require multi-pass of dataset, nor\ntime-aware, as they fail to take into account the time constraint of serial\nepisodes. In this paper, we propose two novel one-pass algorithms, ONCE and\nONCE+, each of which can respectively compute two popular frequencies of given\nepisodes satisfying predefined time-constraint as signals in a stream arrives\none-after-another. ONCE is only used for non-overlapped frequency where the\noccurrences of a serial episode in sequence are not intersected. ONCE+ is\ndesigned for the distinct frequency where the occurrences of a serial episode\ndo not share any event. Theoretical study proves that our algorithm can\ncorrectly mine the frequency of target time constraint serial episodes in a\ngiven stream. Experimental study over both real-world and synthetic datasets\ndemonstrates that the proposed algorithm can work, with little time and space,\nin signal-intensive streams where millions of signals arrive within a single\nsecond. Moreover, the algorithm has been applied in a real stream processing\nsystem, where the efficacy and efficiency of this work is tested in practical\napplications.\n"]},
{"authors": ["Ilya Kolchinsky", "Assaf Schuster"], "title": ["Join Query Optimization Techniques for Complex Event Processing\n  Applications"], "date": ["2018-01-29T09:23:36Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.09413v1"], "summary": ["  Complex event processing (CEP) is a prominent technology used in many modern\napplications for monitoring and tracking events of interest in massive data\nstreams. CEP engines inspect real-time information flows and attempt to detect\ncombinations of occurrences matching predefined patterns. This is done by\ncombining basic data items, also called primitive events, according to a\npattern detection plan, in a manner similar to the execution of multi-join\nqueries in traditional data management systems. Despite this similarity, little\nwork has been done on utilizing existing join optimization methods to improve\nthe performance of CEP-based systems. In this paper, we provide the first\ntheoretical and experimental study of the relationship between these two\nresearch areas. We formally prove that the CEP Plan Generation problem is\nequivalent to the Join Query Plan Generation problem for a restricted class of\npatterns and can be reduced to it for a considerably wider range of classes.\nThis result implies the NP-completeness of the CEP Plan Generation problem. We\nfurther show how join query optimization techniques developed over the last\ndecades can be adapted and utilized to provide practically efficient solutions\nfor complex event detection. Our experiments demonstrate the superiority of\nthese techniques over existing strategies for CEP optimization in terms of\nthroughput, latency, and memory consumption.\n"]},
{"authors": ["Youhuan Li", "Lei Zou", "M. Tamer Ozsu", "Dongyan Zhao"], "title": ["Time Constrained Continuous Subgraph Search over Streaming Graphs"], "date": ["2018-01-28T14:43:04Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.09240v1"], "summary": ["  The growing popularity of dynamic applications such as social networks\nprovides a promising way to detect valuable information in real time. Efficient\nanalysis over high-speed data from dynamic applications is of great\nsignificance. Data from these dynamic applications can be easily modeled as\nstreaming graph. In this paper, we study the subgraph (isomorphism) search over\nstreaming graph data that obeys timing order constraints over the occurrence of\nedges in the stream. We propose a data structure and algorithm to efficiently\nanswer subgraph search and introduce optimizations to greatly reduce the space\ncost, and propose concurrency management to improve system throughput.\nExtensive experiments on real network traffic data and synthetic social\nstreaming data confirms the efficiency and effectiveness of our solution.\n"]},
{"authors": ["Yu Zhou", "Jianbin Huang", "Heli Sun"], "title": ["A Semantic-Rich Similarity Measure in Heterogeneous Information Networks"], "date": ["2018-01-02T01:22:48Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.00783v3"], "summary": ["  Measuring the similarities between objects in information networks has\nfundamental importance in recommendation systems, clustering and web search.\nThe existing metrics depend on the meta path or meta structure specified by\nusers. In this paper, we propose a stratified meta structure based similarity\n$SMSS$ in heterogeneous information networks. The stratified meta structure can\nbe constructed automatically and capture rich semantics. Then, we define the\ncommuting matrix of the stratified meta structure by virtue of the commuting\nmatrices of meta paths and meta structures. As a result, $SMSS$ is defined by\nvirtue of these commuting matrices. Experimental evaluations show that the\nproposed $SMSS$ on the whole outperforms the state-of-the-art metrics in terms\nof ranking and clustering.\n"]},
{"authors": ["Antoine Amarilli", "Mouhamadou Lamine Ba", "Daniel Deutch", "Pierre Senellart"], "title": ["Possible and Certain Answers for Queries over Order-Incomplete Data"], "date": ["2017-07-22T22:10:49Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1707.07222v2"], "summary": ["  To combine and query ordered data from multiple sources, one needs to handle\nuncertainty about the possible orderings. Examples of such \"order-incomplete\"\ndata include integrated event sequences such as log entries, lists of\nproperties (e.g., hotels and restaurants) ranked by an unknown function\nreflecting relevance or customer ratings, and documents edited concurrently\nwith an uncertain order on edits. This paper introduces a query language for\norder-incomplete data, based on the positive relational algebra with\norder-aware accumulation. We use partial orders to represent order-incomplete\ndata, and study possible and certain answers for queries in this context. We\nshow that these problems are respectively NP-complete and coNP-complete, but\nidentify many tractable cases depending on the query operators or input partial\norders.\n"]},
{"authors": ["Harsh Thakkar", "Dharmen Punjani", "Jens Lehmann", "S\u00f6ren Auer"], "title": ["Killing Two Birds with One Stone -- Querying Property Graphs using\n  SPARQL via GREMLINATOR"], "date": ["2018-01-25T23:15:36Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.09556v1"], "summary": ["  Knowledge graphs have become popular over the past decade and frequently rely\non the Resource Description Framework (RDF) or Property Graph (PG) databases as\ndata models. However, the query languages for these two data models -- SPARQL\nfor RDF and the PG traversal language Gremlin -- are lacking interoperability.\nWe present Gremlinator, the first translator from SPARQL -- the W3C\nstandardized language for RDF -- and Gremlin -- a popular property graph\ntraversal language. Gremlinator translates SPARQL queries to Gremlin path\ntraversals for executing graph pattern matching queries over graph databases.\nThis allows a user, who is well versed in SPARQL, to access and query a wide\nvariety of Graph Data Management Systems (DMSs) avoiding the steep learning\ncurve for adapting to a new Graph Query Language (GQL). Gremlin is a graph\ncomputing system-agnostic traversal language (covering both OLTP graph database\nor OLAP graph processors), making it a desirable choice for supporting\ninteroperability for querying Graph DMSs. Gremlinator currently supports the\ntranslation of a subset of SPARQL 1.0, specifically the SPARQL SELECT queries.\n"]},
{"authors": ["Ilya Kolchinsky", "Assaf Schuster"], "title": ["Efficient Adaptive Detection of Complex Event Patterns"], "date": ["2018-01-25T20:17:18Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.08588v1"], "summary": ["  Complex event processing (CEP) is widely employed to detect occurrences of\npredefined combinations (patterns) of events in massive data streams. As new\nevents are accepted, they are matched using some type of evaluation structure,\ncommonly optimized according to the statistical properties of the data items in\nthe input stream. However, in many real-life scenarios the data characteristics\nare never known in advance or are subject to frequent on-the-fly changes. To\nmodify the evaluation structure as a reaction to such changes, adaptation\nmechanisms are employed. These mechanisms typically function by monitoring a\nset of properties and applying a new evaluation plan when significant deviation\nfrom the initial values is observed. This strategy often leads to missing\nimportant input changes or it may incur substantial computational overhead by\nover-adapting. In this paper, we present an efficient and precise method for\ndynamically deciding whether and how the evaluation structure should be\nreoptimized. This method is based on a small set of constraints to be satisfied\nby the monitored values, defined such that a better evaluation plan is\nguaranteed if any of the constraints is violated. To the best of our knowledge,\nour proposed mechanism is the first to provably avoid false positives on\nreoptimization decisions. We formally prove this claim and demonstrate how our\nmethod can be applied on known algorithms for evaluation plan generation. Our\nextensive experimental evaluation on real-world datasets confirms the\nsuperiority of our strategy over existing methods in terms of performance and\naccuracy.\n"]},
{"authors": ["Daniel Kaltenthaler", "Johannes-Y. Lohrer"], "title": ["The Historic Development of the Zooarchaeological Database OssoBook and\n  the xBook Framework for Scientific Databases"], "date": ["2018-01-24T16:07:39Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.08052v1"], "summary": ["  In this technical report, we describe the historic development of the\nzooarchaeological database OssoBook and the resulting framework xBook, a\ngeneric infrastructure for distributed, relational data management that is\nmainly designed for the needs of scientific data. We describe the concepts of\nthe architecture and its most important features. We especially point out the\nServer-Client architecture, the synchronization process, the Launcher\napplication, and the structure and features of the application.\n"]},
{"authors": ["Eugene Siow", "Thanassis Tiropanis", "Xin Wang", "Wendy Hall"], "title": ["TritanDB: Time-series Rapid Internet of Things Analytics"], "date": ["2018-01-24T12:10:46Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.07947v1"], "summary": ["  The efficient management of data is an important prerequisite for realising\nthe potential of the Internet of Things (IoT). Two issues given the large\nvolume of structured time-series IoT data are, addressing the difficulties of\ndata integration between heterogeneous Things and improving ingestion and query\nperformance across databases on both resource-constrained Things and in the\ncloud. In this paper, we examine the structure of public IoT data and discover\nthat the majority exhibit unique flat, wide and numerical characteristics with\na mix of evenly and unevenly-spaced time-series. We investigate the advances in\ntime-series databases for telemetry data and combine these findings with\nmicrobenchmarks to determine the best compression techniques and storage data\nstructures to inform the design of a novel solution optimised for IoT data. A\nquery translation method with low overhead even on resource-constrained Things\nallows us to utilise rich data models like the Resource Description Framework\n(RDF) for interoperability and data integration on top of the optimised\nstorage. Our solution, TritanDB, shows an order of magnitude performance\nimprovement across both Things and cloud hardware on many state-of-the-art\ndatabases within IoT scenarios. Finally, we describe how TritanDB supports\nvarious analyses of IoT time-series data like forecasting.\n"]},
{"authors": ["Zijian Li", "Xun Jian", "Xiang Lian", "Lei Chen"], "title": ["An Efficient Probabilistic Approach for Graph Similarity Search"], "date": ["2017-06-17T05:25:10Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1706.05476v2"], "summary": ["  Graph similarity search is a common and fundamental operation in graph\ndatabases. One of the most popular graph similarity measures is the Graph Edit\nDistance (GED) mainly because of its broad applicability and high\ninterpretability. Despite its prevalence, exact GED computation is proved to be\nNP-hard, which could result in unsatisfactory computational efficiency on large\ngraphs. However, exactly accurate search results are usually unnecessary for\nreal-world applications especially when the responsiveness is far more\nimportant than the accuracy. Thus, in this paper, we propose a novel\nprobabilistic approach to efficiently estimate GED, which is further leveraged\nfor the graph similarity search. Specifically, we first take branches as\nelementary structures in graphs, and introduce a novel graph similarity measure\nby comparing branches between graphs, i.e., Graph Branch Distance (GBD), which\ncan be efficiently calculated in polynomial time. Then, we formulate the\nrelationship between GED and GBD by considering branch variations as the result\nascribed to graph edit operations, and model this process by probabilistic\napproaches. By applying our model, the GED between any two graphs can be\nefficiently estimated by their GBD, and these estimations are finally utilized\nin the graph similarity search. Extensive experiments show that our approach\nhas better accuracy, efficiency and scalability than other comparable methods\nin the graph similarity search over real and synthetic data sets.\n"]},
{"authors": ["Timm Fitschen", "Alexander Schlemmer", "Daniel Hornung", "Henrik tom W\u00f6rden", "Ulrich Parlitz", "Stefan Luther"], "title": ["CaosDB - Research Data Management for Complex, Changing, and Automated\n  Research Workflows"], "date": ["2018-01-23T16:46:18Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.07653v1"], "summary": ["  Here we present CaosDB, a Research Data Management System (RDMS) designed to\nensure seamless integration of inhomogeneous data sources and repositories of\nlegacy data. Its primary purpose is the management of data from biomedical\nsciences, both from simulations and experiments during the complete research\ndata lifecycle. An RDMS for this domain faces particular challenges: Research\ndata arise in huge amounts, from a wide variety of sources, and traverse a\nhighly branched path of further processing. To be accepted by its users, an\nRDMS must be built around workflows of the scientists and practices and thus\nsupport changes in workflow and data structure. Nevertheless it should\nencourage and support the development and observation of standards and\nfurthermore facilitate the automation of data acquisition and processing with\nspecialized software. The storage data model of an RDMS must reflect these\ncomplexities with appropriate semantics and ontologies while offering simple\nmethods for finding, retrieving, and understanding relevant data. We show how\nCaosDB responds to these challenges and give an overview of the CaosDB Server,\nits data model and its easy-to-learn CaosDB Query Language. We briefly discuss\nthe status of the implementation, how we currently use CaosDB, and how we plan\nto use and extend it.\n"]},
{"authors": ["Fotis Psallidas", "Eugene Wu"], "title": ["Smoke: Fine-grained Lineage at Interactive Speed"], "date": ["2018-01-22T18:39:18Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.07237v1"], "summary": ["  Data lineage describes the relationship between individual input and output\ndata items of a workflow, and has served as an integral ingredient for both\ntraditional (e.g., debugging, auditing, data integration, and security) and\nemergent (e.g., interactive visualizations, iterative analytics, explanations,\nand cleaning) applications. The core, long-standing problem that lineage\nsystems need to address---and the main focus of this paper---is to capture the\nrelationships between input and output data items across a workflow with the\ngoal to streamline queries over lineage. Unfortunately, current lineage systems\neither incur high lineage capture overheads, or lineage query processing costs,\nor both. As a result, applications, that in principle can express their logic\ndeclaratively in lineage terms, resort to hand-tuned implementations. To this\nend, we introduce Smoke, an in-memory database engine that neither lineage\ncapture overhead nor lineage query processing needs to be compromised. To do\nso, Smoke introduces tight integration of the lineage capture logic into\nphysical database operators; efficient, write-optimized lineage representations\nfor storage; and optimizations when future lineage queries are known up-front.\nOur experiments on microbenchmarks and realistic workloads show that Smoke\nreduces the lineage capture overhead and streamlines lineage queries by\nmultiple orders of magnitude compared to state-of-the-art alternatives. Our\nexperiments on real-world applications highlight that Smoke can meet the\nlatency requirements of interactive visualizations (e.g., <150ms) and\noutperform hand-written implementations of data profiling primitives.\n"]},
{"authors": ["Chao Yan", "Bo Li", "Yevgeniy Vorobeychik", "Aron Laszka", "Daniel Fabbri", "Bradley Malin"], "title": ["Get Your Workload in Order: Game Theoretic Prioritization of Database\n  Auditing"], "date": ["2018-01-22T17:42:32Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.07215v1"], "summary": ["  For enhancing the privacy protections of databases, where the increasing\namount of detailed personal data is stored and processed, multiple mechanisms\nhave been developed, such as audit logging and alert triggers, which notify\nadministrators about suspicious activities; however, the two main limitations\nin common are: 1) the volume of such alerts is often substantially greater than\nthe capabilities of resource-constrained organizations, and 2) strategic\nattackers may disguise their actions or carefully choosing which records they\ntouch, making incompetent the statistical detection models. For solving them,\nwe introduce a novel approach to database auditing that explicitly accounts for\nadversarial behavior by 1) prioritizing the order in which types of alerts are\ninvestigated and 2) providing an upper bound on how much resource to allocate\nfor each type. We model the interaction between a database auditor and\npotential attackers as a Stackelberg game in which the auditor chooses an\nauditing policy and attackers choose which records to target. A corresponding\napproach combining linear programming, column generation, and heuristic search\nis proposed to derive an auditing policy. For testing the policy-searching\nperformance, a publicly available credit card application dataset are adopted,\non which it shows that our methods produce high-quality mixed strategies as\ndatabase audit policies, and our general approach significantly outperforms\nnon-game-theoretic baselines.\n"]},
{"authors": ["Abdul Rahman Sherzad"], "title": ["Data is the Fuel of Organizations: Opportunities and Challenges in\n  Afghanistan"], "date": ["2018-01-22T13:23:54Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.07789v1"], "summary": ["  In this paper, the author at first briefly outlines the value of data in\norganizations and the opportunities and challenges in Afghanistan. Then the\nauthor takes the Kankor (National University Entrance Exam) data, particularly\nnames of participants, locations, high schools and higher education\ninstitutions into account and explains how these data, that organizations in\nAfghanistan do not use for anything, can be useful in several cases and areas.\nThe application of these data is shown through cases such as Auto filling\nmissing values, identifying names of people, locations, and institutions from\nunstructured text, generating fake data to benchmark the database and web\napplication performance and appearance, comparing and matching high school data\nwith Kankor data, producing the top-n male and female names very common in\nAfghanistan or province-wise, and the data mining application in education and\nhigher education institutions.\n"]},
{"authors": ["Mathias Weber", "Annette Bieniusa"], "title": ["ACGreGate: A Framework for Practical Access Control for Applications\n  using Weakly Consistent Databases"], "date": ["2018-01-22T09:23:42Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.07005v1"], "summary": ["  Scalable and highly available systems often require data stores that offer\nweaker consistency guarantees than traditional relational databases systems.\nThe correctness of these applications highly depends on the resilience of the\napplication model against data inconsistencies. In particular regarding\napplication security, it is difficult to determine which inconsistencies can be\ntolerated and which might lead to security breaches.\n  In this paper, we discuss the problem of how to develop an access control\nlayer for applications using weakly consistent data stores without loosing the\nperformance benefits gained by using weaker consistency models. We present\nACGreGate, a Java framework for implementing correct access control layers for\napplications using weakly consistent data stores. Under certain requirements on\nthe data store, ACGreGate ensures that the access control layer operates\ncorrectly with respect to dynamically adaptable security policies. We used\nACGreGate to implement the access control layer of a student management system.\nThis case study shows that practically useful security policies can be\nimplemented with the framework incurring little overhead. A comparison with a\nsetup using a centralized server shows the benefits of using ACGreGate for\nscalability of the service to geo-scale.\n"]},
{"authors": ["Thapana Boonchoo", "Xiang Ao", "Qing He"], "title": ["An Efficient Density-based Clustering Algorithm for Higher-Dimensional\n  Data"], "date": ["2018-01-22T06:35:17Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.06965v1"], "summary": ["  DBSCAN is a typically used clustering algorithm due to its clustering ability\nfor arbitrarily-shaped clusters and its robustness to outliers. Generally, the\ncomplexity of DBSCAN is O(n^2) in the worst case, and it practically becomes\nmore severe in higher dimension. Grid-based DBSCAN is one of the recent\nimproved algorithms aiming at facilitating efficiency. However, the performance\nof grid-based DBSCAN still suffers from two problems: neighbour explosion and\nredundancies in merging, which make the algorithms infeasible in\nhigh-dimensional space. In this paper, we propose a novel algorithm named GDPAM\nattempting to extend Grid-based DBSCAN to higher data dimension. In GDPAM, a\nbitmap indexing is utilized to manage non-empty grids so that the neighbour\ngrid queries can be performed efficiently. Furthermore, we adopt an efficient\nunion-find algorithm to maintain the clustering information in order to reduce\nredundancies in the merging. The experimental results on both real-world and\nsynthetic datasets demonstrate that the proposed algorithm outperforms the\nstate-of-the-art exact/approximate DBSCAN and suggests a good scalability.\n"]},
{"authors": ["Mohammad Hossain Namaki", "F A Rezaur Rahman Chowdhury", "Md Rakibul Islam", "Janardhan Rao Doppa", "Yinghui Wu"], "title": ["Learning to Speed Up Query Planning in Graph Databases"], "date": ["2018-01-21T04:49:23Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.06766v1"], "summary": ["  Querying graph structured data is a fundamental operation that enables\nimportant applications including knowledge graph search, social network\nanalysis, and cyber-network security. However, the growing size of real-world\ndata graphs poses severe challenges for graph databases to meet the\nresponse-time requirements of the applications. Planning the computational\nsteps of query processing - Query Planning - is central to address these\nchallenges. In this paper, we study the problem of learning to speedup query\nplanning in graph databases towards the goal of improving the\ncomputational-efficiency of query processing via training queries.We present a\nLearning to Plan (L2P) framework that is applicable to a large class of query\nreasoners that follow the Threshold Algorithm (TA) approach. First, we define a\ngeneric search space over candidate query plans, and identify target search\ntrajectories (query plans) corresponding to the training queries by performing\nan expensive search. Subsequently, we learn greedy search control knowledge to\nimitate the search behavior of the target query plans. We provide a concrete\ninstantiation of our L2P framework for STAR, a state-of-the-art graph query\nreasoner. Our experiments on benchmark knowledge graphs including DBpedia,\nYAGO, and Freebase show that using the query plans generated by the learned\nsearch control knowledge, we can significantly improve the speed of STAR with\nnegligible loss in accuracy.\n"]},
{"authors": ["Xin Wang", "Eugene Siow", "Aastha Madaan", "Thanassis Tiropanis"], "title": ["PRESTO: Probabilistic Cardinality Estimation for RDF Queries Based on\n  Subgraph Overlapping"], "date": ["2018-01-19T14:11:45Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.06408v1"], "summary": ["  In query optimisation accurate cardinality estimation is essential for\nfinding optimal query plans. It is especially challenging for RDF due to the\nlack of explicit schema and the excessive occurrence of joins in RDF queries.\nExisting approaches typically collect statistics based on the counts of triples\nand estimate the cardinality of a query as the product of its join components,\nwhere errors can accumulate even when the estimation of each component is\naccurate. As opposed to existing methods, we propose PRESTO, a cardinality\nestimation method that is based on the counts of subgraphs instead of triples\nand uses a probabilistic method to estimate cardinalities of RDF queries as a\nwhole. PRESTO avoids some major issues of existing approaches and is able to\naccurately estimate arbitrary queries under a bound memory constraint. We\nevaluate PRESTO with YAGO and show that PRESTO is more accurate for both simple\nand complex queries.\n"]},
{"authors": ["Jithin Vachery", "Akhil Arora", "Sayan Ranu", "Arnab Bhattacharya"], "title": ["CGQ: Relationship-Aware Contextual Graph Querying in Large Networks"], "date": ["2018-01-19T13:50:47Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.06402v1"], "summary": ["  The phenomenal growth of graph data from a wide-variety of real-world\napplications has rendered graph querying to be a problem of paramount\nimportance. Traditional techniques use structural as well as node similarities\nto find matches of a given query graph in a (large) target graph. However,\nalmost all previous research has tacitly ignored the presence of relationships\nand context (usually manifested in the form of node/edge label distributions)\nin the query. In this paper, we propose CGQ -- Relationship-Aware Contextual\nGraph Querying} for real-world graphs. Given a query graph and a target graph,\nCGQ identifies the (top-k) maximal common subgraphs between the query and the\ntarget graphs with the highest contextual similarity. We prove that the problem\nis NP-hard and APX-Hard. To overcome this computational bottleneck, we propose\na hierarchical index, CGQ-Tree, with its associated CGQ search algorithm.\nEmpirically, the CGQ search algorithm is capable of achieving speed-ups of up\nto three orders of magnitude over a baseline strategy. Our experiments show\nthat CGQ is effective, efficient and scalable.\n"]},
{"authors": ["Antoine Amarilli", "Mouhamadou Lamine Ba", "Daniel Deutch", "Pierre Senellart"], "title": ["Computing Possible and Certain Answers over Order-Incomplete Data"], "date": ["2018-01-19T13:20:56Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.06396v1"], "summary": ["  This paper studies the complexity of query evaluation for databases whose\nrelations are partially ordered; the problem commonly arises when combining\nordered data from multiple sources. We focus on queries in a useful fragment of\nSQL, namely positive relational algebra with aggregates, whose bag semantics we\nextend to the partially ordered setting. Our semantics leads to the study of\ntwo main computational problems, namely the possibility and certainty of query\nanswers. We show that these problems are respectively NP-complete and\ncoNP-complete, but identify tractable cases depending on the query operators or\ninput partial orders. We further introduce a duplicate elimination operator and\nstudy its effect on the complexity results.\n"]},
{"authors": ["Marc Shapiro", "Annette Bieniusa", "Nuno Pregui\u00e7a", "Valter Balegas", "Christopher Meiklejohn"], "title": ["Just-Right Consistency: reconciling availability and safety"], "date": ["2018-01-19T09:32:54Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.06340v1"], "summary": ["  By the CAP Theorem, a distributed data storage system can ensure either\nConsistency under Partition (CP) or Availability under Partition (AP), but not\nboth. This has led to a split between CP databases, in which updates are\nsynchronous, and AP databases, where they are asynchronous. However, there is\nno inherent reason to treat all updates identically: simply, the system should\nbe as available as possible, and synchronised just enough for the application\nto be correct. We offer a principled Just-Right Consistency approach to\ndesigning such applications, reconciling correctness with availability and\nperformance, based on the following insights:(i) The Conflict-Free Replicated\nData Type (CRDTs) data model supports asynchronous updates in an intuitive and\nprincipled way.(ii) Invariants involving joint or mutually-ordered updates are\ncompatible with AP and can be guaranteed by Transactional Causal Consistency,\nthe strongest consistency model that does not compromise availability.\nRegarding the remaining, \"CAP-sensitive\" invariants:(iii) For the common\npattern of Bounded Counters, we provide encapsulated data type that is proven\ncorrect and is efficient; (iv) in the general case, static analysis can\nidentify when synchronisation is not necessary for correctness.Our Antidote\ncloud database system supports CRDTs, Transactional Causal Consistency and the\nBounded Counter data type. Support tools help design applications by static\nanalysis and proof of CAP-sensitive invariants. This system supports\nindustrial-grade applications and has been tested experimentally with hundreds\nof servers across several geo-distributed data centres.\n"]},
{"authors": ["Tana Wattanawaroon", "Stephen Macke", "Aditya Parameswaran"], "title": ["Towards a Theory of Data-Diff: Optimal Synthesis of Succinct Data\n  Modification Scripts"], "date": ["2018-01-19T00:02:34Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.06258v1"], "summary": ["  This paper addresses the Data-Diff problem: given a dataset and a subsequent\nversion of the dataset, find the shortest sequence of operations that\ntransforms the dataset to the subsequent version, under a restricted family of\noperations. We consider operations similar to SQL UPDATE, each with a condition\n(WHERE) that matches a subset of tuples and a modifier (SET) that makes changes\nto those matched tuples. We characterize the problem based on different\nconstraints on the attributes and the allowed conditions and modifiers,\nproviding complexity classification and algorithms in each case.\n"]},
{"authors": ["Ali Pesaranghader", "Herna Viktor", "Eric Paquet"], "title": ["McDiarmid Drift Detection Methods for Evolving Data Streams"], "date": ["2017-10-05T14:02:28Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.02030v2"], "summary": ["  Increasingly, Internet of Things (IoT) domains, such as sensor networks,\nsmart cities, and social networks, generate vast amounts of data. Such data are\nnot only unbounded and rapidly evolving. Rather, the content thereof\ndynamically evolves over time, often in unforeseen ways. These variations are\ndue to so-called concept drifts, caused by changes in the underlying data\ngeneration mechanisms. In a classification setting, concept drift causes the\npreviously learned models to become inaccurate, unsafe and even unusable.\nAccordingly, concept drifts need to be detected, and handled, as soon as\npossible. In medical applications and emergency response settings, for example,\nchange in behaviours should be detected in near real-time, to avoid potential\nloss of life. To this end, we introduce the McDiarmid Drift Detection Method\n(MDDM), which utilizes McDiarmid's inequality in order to detect concept drift.\nThe MDDM approach proceeds by sliding a window over prediction results, and\nassociate window entries with weights. Higher weights are assigned to the most\nrecent entries, in order to emphasize their importance. As instances are\nprocessed, the detection algorithm compares a weighted mean of elements inside\nthe sliding window with the maximum weighted mean observed so far. A\nsignificant difference between the two weighted means, upper-bounded by the\nMcDiarmid inequality, implies a concept drift. Our extensive experimentation\nagainst synthetic and real-world data streams show that our novel method\noutperforms the state-of-the-art. Specifically, MDDM yields shorter detection\ndelays as well as lower false negative rates, while maintaining high\nclassification accuracies.\n"]},
{"authors": ["Remi Cura", "Julien Perret", "Nicolas Paparoditis"], "title": ["Interactive in-base street model edit: how common GIS software and a\n  database can serve as a custom Graphical User Interface"], "date": ["2018-01-17T18:55:43Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.05800v1"], "summary": ["  Our modern world produces an increasing quantity of data, and especially\ngeospatial data, with advance of sensing technologies, and growing complexity\nand organisation of vector data. Tools are needed to efficiently create and\nedit those vector geospatial data. Procedural generation has been a tool of\nchoice to generate strongly organised data, yet it may be hard to control.\nBecause those data may be involved to take consequence-full real life\ndecisions, user interactions are required to check data and edit it. The\nclassical process to do so would be to build an adhoc Graphical User Interface\n(GUI) tool adapted for the model and method being used. This task is difficult,\ntakes a large amount of resources, and is very specific to one model, making it\nhard to share and re-use.\n  Besides, many common generic GUI already exists to edit vector data, each\nhaving its specialities. We propose a change of paradigm; instead of building a\nspecific tool for one task, we use common GIS software as GUIs, and deport the\nspecific interactions from the software to within the database. In this\nparadigm, GIS software simply modify geometry and attributes of database\nlayers, and those changes are used by the database to perform automated task.\n  This new paradigm has many advantages. The first one is genericity. With\nin-base interaction, any GIS software can be used to perform edition, whatever\nthe software is a Desktop sofware or a web application. The second is\nconcurrency and coherency. Because interaction is in-base, use of database\nfeatures allows seamless multi-user work, and can guarantee that the data is in\na coherent state. Last we propose tools to facilitate multi-user edits, both\nduring the edit phase (each user knows what areas are edited by other users),\nand before and after edit (planning of edit, analyse of edited areas).\n"]},
{"authors": ["Lior Kogan"], "title": ["V1: A Visual Query Language for Property Graphs"], "date": ["2017-10-12T12:21:17Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.04470v2"], "summary": ["  V1 is a declarative visual query language for schema-based property graphs.\nV1 supports property graphs with mixed (both directed and undirected) edges and\nhalf-edges, with multivalued and composite properties, and with empty property\nvalues. V1 supports temporal data types, operators, and functions, and can be\nextended to support additional data types, operators, and functions (one\nspatiotemporal model is presented). V1 is generic, concise, has rich expressive\npower, and is highly receptive and productive.\n"]},
{"authors": ["Ankur Sharma", "Felix Martin Schuhknecht", "Jens Dittrich"], "title": ["The Case for Automatic Database Administration using Deep Reinforcement\n  Learning"], "date": ["2018-01-17T12:51:01Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.05643v1"], "summary": ["  Like any large software system, a full-fledged DBMS offers an overwhelming\namount of configuration knobs. These range from static initialisation\nparameters like buffer sizes, degree of concurrency, or level of replication to\ncomplex runtime decisions like creating a secondary index on a particular\ncolumn or reorganising the physical layout of the store. To simplify the\nconfiguration, industry grade DBMSs are usually shipped with various advisory\ntools, that provide recommendations for given workloads and machines. However,\nreality shows that the actual configuration, tuning, and maintenance is usually\nstill done by a human administrator, relying on intuition and experience.\nRecent work on deep reinforcement learning has shown very promising results in\nsolving problems, that require such a sense of intuition. For instance, it has\nbeen applied very successfully in learning how to play complicated games with\nenormous search spaces. Motivated by these achievements, in this work we\nexplore how deep reinforcement learning can be used to administer a DBMS.\nFirst, we will describe how deep reinforcement learning can be used to\nautomatically tune an arbitrary software system like a DBMS by defining a\nproblem environment. Second, we showcase our concept of NoDBA at the concrete\nexample of index selection and evaluate how well it recommends indexes for\ngiven workloads.\n"]},
{"authors": ["Sebastian Herbst", "Johannes Tenschert", "Andreas M. Wahl", "Klaus Meyer-Wegener"], "title": ["Sequences, yet Functions: The Dual Nature of Data-Stream Processing"], "date": ["2018-01-16T11:05:12Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.05206v1"], "summary": ["  Data-stream processing has continuously risen in importance as the amount of\navailable data has been steadily increas- ing over the last decade. Besides\ntraditional domains such as data-center monitoring and click analytics, there\nis an increasing number of network-enabled production machines that generate\ncontinuous streams of data. Due to their continuous nature, queries on\ndata-streams can be more complex, and distinctly harder to understand then\ndatabase queries. As users have to consider operational details, maintenance\nand debugging become challenging. Current approaches model data-streams as\nsequences, be- cause this is the way they are physically received. These models\nresult in an implementation-focused perspective. We explore an alternate way of\nmodeling data-streams by focusing on time-slicing semantics. This focus results\nin a model based on functions, which is better suited for reasoning about query\nsemantics. By adapting the definitions of relevant concepts in stream\nprocessing to our model, we illustrate the practical useful- ness of our\napproach. Thereby, we link data-streams and query primitives to concepts in\nfunctional programming and mathematics. Most noteworthy, we prove that\ndata-streams are monads, and show how to derive monad definitions for current\ndata-stream models. We provide an abstract, yet practical perspective on data-\nstream related subjects based on a sound, consistent query model. Our work can\nserve as solid foundation for future data-stream query-languages.\n"]},
{"authors": ["Chemseddine Nabti", "Hamida Seba"], "title": ["Compact Neighborhood Index for Subgraph Queries in Massive Graphs"], "date": ["2017-03-16T10:20:31Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.05547v3"], "summary": ["  Subgraph queries also known as subgraph isomorphism search is a fundamental\nproblem in querying graph-like structured data. It consists to enumerate the\nsubgraphs of a data graph that match a query graph. This problem arises in many\nreal-world applications related to query processing or pattern recognition such\nas computer vision, social network analysis, bioinformatic and big data\nanalytic. Subgraph isomorphism search knows a lot of investigations and\nsolutions mainly because of its importance and use but also because of its\nNP-completeness. Existing solutions use filtering mechanisms and optimise the\norder within witch the query vertices are matched on the data vertices to\nobtain acceptable processing times. However, existing approaches are iterative\nand generate several intermediate results. They also require that the data\ngraph is loaded in main memory and consequently are not adapted to large graphs\nthat do not fit into memory or are accessed by streams. To tackle this problem,\nwe propose a new approach based on concepts widely different from existing\nworks. Our approach distills the semantic and topological information that\nsurround a vertex into a simple integer. This simple vertex encoding that can\nbe computed and updated incrementally reduces considerably intermediate results\nand avoid to load the entire data graph into main memory. We evaluate our\napproach on several real-word datasets. The experimental results show that our\napproach is efficient and scalable.\n"]},
{"authors": ["Sergi Nadal", "Oscar Romero", "Alberto Abell\u00f3", "Panos Vassiliadis", "Stijn Vansummeren"], "title": ["An Integration-Oriented Ontology to Govern Evolution in Big Data\n  Ecosystems"], "date": ["2018-01-16T08:55:41Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.05161v1"], "summary": ["  Big Data architectures allow to flexibly store and process heterogeneous\ndata, from multiple sources, in their original format. The structure of those\ndata, commonly supplied by means of REST APIs, is continuously evolving. Thus\ndata analysts need to adapt their analytical processes after each API release.\nThis gets more challenging when performing an integrated or historical\nanalysis. To cope with such complexity, in this paper, we present the Big Data\nIntegration ontology, the core construct to govern the data integration process\nunder schema evolution by systematically annotating it with information\nregarding the schema of the sources. We present a query rewriting algorithm\nthat, using the annotated ontology, converts queries posed over the ontology to\nqueries over the sources. To cope with syntactic evolution in the sources, we\npresent an algorithm that semi-automatically adapts the ontology upon new\nreleases. This guarantees ontology-mediated queries to correctly retrieve data\nfrom the most recent schema version as well as correctness in historical\nqueries. A functional and performance evaluation on real-world APIs is\nperformed to validate our approach.\n"]},
{"authors": ["Mohammad Roohitavaf", "Sandeep Kulkarni"], "title": ["DKVF: A Framework for Rapid Prototyping and Evaluating Distributed\n  Key-value Stores"], "date": ["2018-01-15T23:09:59Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.05064v1"], "summary": ["  We present our framework DKVF that enables one to quickly prototype and\nevaluate new protocols for key-value stores and compare them with existing\nprotocols based on selected benchmarks. Due to limitations of CAP theorem, new\nprotocols must be developed that achieve the desired trade-off between\nconsistency and availability for the given application at hand. Hence, both\nacademic and industrial communities focus on developing new protocols that\nidentify a different (and hopefully better in one or more aspect) point on this\ntrade-off curve. While these protocols are often based on a simple intuition,\nevaluating them to ensure that they indeed provide increased availability,\nconsistency, or performance is a tedious task. Our framework, DKVF, enables one\nto quickly prototype a new protocol as well as identify how it performs\ncompared to existing protocols for pre-specified benchmarks. Our framework\nrelies on YCSB (Yahoo! Cloud Servicing Benchmark) for benchmarking. We\ndemonstrate DKVF by implementing four existing protocols --eventual\nconsistency, COPS, GentleRain and CausalSpartan-- with it. We compare the\nperformance of these protocols against different loading conditions. We find\nthat the performance is similar to our implementation of these protocols from\nscratch. And, the comparison of these protocols is consistent with what has\nbeen reported in the literature. Moreover, implementation of these protocols\nwas much more natural as we only needed to translate the pseudocode into Java\n(and add the necessary error handling). Hence, it was possible to achieve this\nin just 1-2 days per protocol. Finally, our framework is extensible. It is\npossible to replace individual components in the framework (e.g., the storage\ncomponent).\n"]},
{"authors": ["Edward Raff", "Charles Nicholas"], "title": ["Toward Metric Indexes for Incremental Insertion and Querying"], "date": ["2018-01-12T16:25:16Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.05055v1"], "summary": ["  In this work we explore the use of metric index structures, which accelerate\nnearest neighbor queries, in the scenario where we need to interleave\ninsertions and queries during deployment. This use-case is inspired by a\nreal-life need in malware analysis triage, and is surprisingly understudied.\nExisting literature tends to either focus on only final query efficiency, often\ndoes not support incremental insertion, or does not support arbitrary distance\nmetrics. We modify and improve three algorithms to support our scenario of\nincremental insertion and querying with arbitrary metrics, and evaluate them on\nmultiple datasets and distance metrics while varying the value of $k$ for the\ndesired number of nearest neighbors. In doing so we determine that our improved\nVantage-Point tree of Minimum-Variance performs best for this scenario.\n"]},
{"authors": ["Xuelian Lin", "Jiahao Jiang", "Shuai Ma", "Yimeng Zuo", "Chunming Hu"], "title": ["One-Pass Trajectory Simplification Using the Synchronous Euclidean\n  Distance"], "date": ["2018-01-12T09:36:09Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.05360v1"], "summary": ["  Various mobile devices have been used to collect, store and transmit\ntremendous trajectory data, and it is known that raw trajectory data seriously\nwastes the storage, network band and computing resource. To attack this issue,\none-pass line simplification (LS) algorithms have are been developed, by\ncompressing data points in a trajectory to a set of continuous line segments.\nHowever, these algorithms adopt the perpendicular Euclidean distance, and none\nof them uses the synchronous Euclidean distance (SED), and cannot support\nspatio-temporal queries. To do this, we develop two one-pass error bounded\ntrajectory simplification algorithms (CISED-S and CISED-W) using SED, based on\na novel spatio-temporal cone intersection technique. Using four real-life\ntrajectory datasets, we experimentally show that our approaches are both\nefficient and effective. In terms of running time, algorithms CISED-S and\nCISED-W are on average 3 times faster than SQUISH-E (the most efficient\nexisting LS algorithm using SED). In terms of compression ratios, algorithms\nCISED-S and CISED-W are comparable with and 19.6% better than DPSED (the most\neffective existing LS algorithm using SED) on average, respectively, and are\n21.1% and 42.4% better than SQUISH-E on average, respectively.\n"]},
{"authors": ["Fuat Bas\u0131k", "Bu\u011fra Gedik", "\u00c7a\u011fr\u0131 Etemo\u011flu", "Hakan Ferhatosmano\u011flu"], "title": ["Spatio-Temporal Linkage over Location Enhanced Services"], "date": ["2018-01-12T09:32:45Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.04101v1"], "summary": ["  We are witnessing an enormous growth in the volume of data generated by\nvarious online services. An important portion of this data contains geographic\nreferences, since many of these services are \\emph{location-enhanced} and thus\nproduce spatio-temporal records of their usage. We postulate that the\nspatio-temporal usage records belonging to the same real-world entity can be\nmatched across records from different location-enhanced services. Linking\nspatio-temporal records enables data analysts and service providers to obtain\ninformation that they cannot derive by analyzing only one set of usage records.\nIn this paper, we develop a new \\emph{linkage model} that can be used to match\nentities from two sets of spatio-temporal usage records belonging to two\ndifferent location-enhanced services. This linkage model is based on the\nconcept of $k$-$l$ \\emph{diversity} --- that we developed to capture both\nspatial and temporal aspects of the linkage. To realize this linkage model in\npractice, we develop a scalable linking algorithm called \\emph{ST-Link}, which\nmakes use of effective spatial and temporal filtering mechanisms that\nsignificantly reduce the search space for matching users. Furthermore,\n\\emph{ST-Link} utilizes sequential scan procedures to avoid random disk access\nand thus scales to large datasets. We evaluated our work with respect to\naccuracy and performance using several datasets. Experiments show that\n\\emph{ST-Link} is effective in practice for performing spatio-temporal linkage\nand can scale to large datasets.\n"]},
{"authors": ["Maria Luiza Mondelli", "Thiago Magalh\u00e3es", "Guilherme Loss", "Michael Wilde", "Ian Foster", "Marta Mattoso", "Daniel S. Katz", "Helio J. C. Barbosa", "Ana Tereza R. Vasconcelos", "Kary Oca\u00f1a", "Luiz M. R. Gadelha Jr"], "title": ["BioWorkbench: A High-Performance Framework for Managing and Analyzing\n  Bioinformatics Experiments"], "date": ["2018-01-11T18:35:50Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.03915v1"], "summary": ["  Advances in sequencing techniques have led to exponential growth in\nbiological data, demanding the development of large-scale bioinformatics\nexperiments. Because these experiments are computation- and data-intensive,\nthey require high-performance computing (HPC) techniques and can benefit from\nspecialized technologies such as Scientific Workflow Management Systems (SWfMS)\nand databases. In this work, we present BioWorkbench, a framework for managing\nand analyzing bioinformatics experiments. This framework automatically collects\nprovenance data, including both performance data from workflow execution and\ndata from the scientific domain of the workflow application. Provenance data\ncan be analyzed through a web application that abstracts a set of queries to\nthe provenance database, simplifying access to provenance information. We\nevaluate BioWorkbench using three case studies: SwiftPhylo, a phylogenetic tree\nassembly workflow; SwiftGECKO, a comparative genomics workflow; and RASflow, a\nRASopathy analysis workflow. We analyze each workflow from both computational\nand scientific domain perspectives, by using queries to a provenance and\nannotation database. Some of these queries are available as a pre-built feature\nof the BioWorkbench web application. Through the provenance data, we show that\nthe framework is scalable and achieves high-performance, reducing up to 98% of\nthe case studies execution time. We also show how the application of machine\nlearning techniques can enrich the analysis process.\n"]},
{"authors": ["Jiri Nadvornik", "Petr Skoda", "Dave Morris", "Pavel Tvrdik"], "title": ["Time Series Cube Data Model"], "date": ["2017-02-05T12:20:52Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.01393v2"], "summary": ["  The purpose of this document is to create a data model and its serialization\nfor expressing generic time series data. Already existing IVOA data models are\nreused as much as possible. The model is also made as generic as possible to be\nopen to new extensions but at the same time closed for modifications. This\nenables maintaining interoperability throughout different versions of the data\nmodel. We define the necessary building blocks for metadata discovery,\nserialization of time series data and understanding it by clients. We present\nseveral categories of time series science cases with examples of\nimplementation. We also take into account the most pressing topics for time\nseries providers like tracking original images for every individual point of a\nlight curve or time-derived axes like frequency for gravitational wave\nanalysis. The main motivation for the creation of a new model is to provide a\nunified time series data publishing standard - not only for light curves but\nalso more generic time series data, e.g., radial velocity curves, power\nspectra, hardness ratio, provenance linkage, etc. The flexibility is the most\ncrucial part of our model - we are not dependent on any physical domain or\nframe models. While images or spectra are already stable and standardized\nproducts, the time series related domains are still not completely evolved and\nnew ones will likely emerge in near future. That is why we need to keep models\nlike Time Series Cube DM independent of any underlying physical models. In our\nopinion, this is the only correct and sustainable way for future development of\nIVOA standards.\n"]},
{"authors": ["J. W. Zhang", "Y. C. Tay"], "title": ["A tool framework for tweaking features in synthetic datasets"], "date": ["2018-01-11T07:18:16Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.03645v1"], "summary": ["  Researchers and developers use benchmarks to compare their algorithms and\nproducts. A database benchmark must have a dataset D. To be\napplication-specific, this dataset D should be empirical. However, D may be too\nsmall, or too large, for the benchmarking experiments. D must, therefore, be\nscaled to the desired size.\n  To ensure the scaled D' is similar to D, previous work typically specifies or\nextracts a fixed set of features F = {F_1, F_2, . . . , F_n} from D, then uses\nF to generate synthetic data for D'. However, this approach (D -> F -> D')\nbecomes increasingly intractable as F gets larger, so a new solution is\nnecessary.\n  Different from existing approaches, this paper proposes ASPECT to scale D to\nenforce similarity. ASPECT first uses a size-scaler (S0) to scale D to D'. Then\nthe user selects a set of desired features F'_1, . . . , F'_n. For each desired\nfeature F'_k, there is a tweaking tool T_k that tweaks D' to make sure D' has\nthe required feature F'_k. ASPECT coordinates the tweaking of T_1,...,T_n to\nD', so T_n(...(T_1(D'))...) has the required features F'_1,...,F'_n.\n  By shifting from D -> F -> D' to D -> D' -> F', data scaling becomes\nflexible. The user can customise the scaled dataset with their own interested\nfeatures. Extensive experiments on real datasets show that ASPECT can enforce\nsimilarity in the dataset effectively and efficiently.\n"]},
{"authors": ["Stefan Sprenger", "Patrick Sch\u00e4fer", "Ulf Leser"], "title": ["Multidimensional Range Queries on Modern Hardware"], "date": ["2018-01-11T07:03:23Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.03644v1"], "summary": ["  Range queries over multidimensional data are an important part of database\nworkloads in many applications. Their execution may be accelerated by using\nmultidimensional index structures (MDIS), such as kd-trees or R-trees. As for\nmost index structures, the usefulness of this approach depends on the\nselectivity of the queries, and common wisdom told that a simple scan beats\nMDIS for queries accessing more than 15%-20% of a dataset. However, this wisdom\nis largely based on evaluations that are almost two decades old, performed on\ndata being held on disks, applying IO-optimized data structures, and using\nsingle-core systems. The question is whether this rule of thumb still holds\nwhen multidimensional range queries (MDRQ) are performed on modern\narchitectures with large main memories holding all data, multi-core CPUs and\ndata-parallel instruction sets. In this paper, we study the question whether\nand how much modern hardware influences the performance ratio between index\nstructures and scans for MDRQ. To this end, we conservatively adapted three\npopular MDIS, namely the R*-tree, the kd-tree, and the VA-file, to exploit\nfeatures of modern servers and compared their performance to different flavors\nof parallel scans using multiple (synthetic and real-world) analytical\nworkloads over multiple (synthetic and real-world) datasets of varying size,\ndimensionality, and skew. We find that all approaches benefit considerably from\nusing main memory and parallelization, yet to varying degrees. Our evaluation\nshows that, on current machines, the new rule of thumb for the threshold from\nwhich on scanning should be favored over parallel versions of classical MDIS\nshould be set rather around 1%.\n"]},
{"authors": ["Ahmet Kara", "Dan Olteanu"], "title": ["Covers of Query Results"], "date": ["2017-09-05T21:31:02Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.01600v2"], "summary": ["  We introduce succinct lossless representations of query results called\ncovers. They are subsets of the query results that correspond to minimal edge\ncovers in the hypergraphs of these results.\n  We first study covers whose structures are given by fractional hypertree\ndecompositions of join queries. For any decomposition of a query, we give\nasymptotically tight size bounds for the covers of the query result over that\ndecomposition and show that such covers can be computed in worst-case optimal\ntime up to a logarithmic factor in the database size. For acyclic join queries,\nwe can compute covers compositionally using query plans with a new operator\ncalled cover-join. The tuples in the query result can be enumerated from any of\nits covers with linearithmic pre-computation time and constant delay.\n  We then generalize covers from joins to functional aggregate queries that\nexpress a host of computational problems such as aggregate-join queries,\nin-database optimization, matrix chain multiplication, and inference in\nprobabilistic graphical models.\n"]},
{"authors": ["Kevin Hsieh", "Ganesh Ananthanarayanan", "Peter Bodik", "Paramvir Bahl", "Matthai Philipose", "Phillip B. Gibbons", "Onur Mutlu"], "title": ["Focus: Querying Large Video Datasets with Low Latency and Low Cost"], "date": ["2018-01-10T18:52:25Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.03493v1"], "summary": ["  Large volumes of videos are continuously recorded from cameras deployed for\ntraffic control and surveillance with the goal of answering \"after the fact\"\nqueries: identify video frames with objects of certain classes (cars, bags)\nfrom many days of recorded video. While advancements in convolutional neural\nnetworks (CNNs) have enabled answering such queries with high accuracy, they\nare too expensive and slow. We build Focus, a system for low-latency and\nlow-cost querying on large video datasets. Focus uses cheap ingestion\ntechniques to index the videos by the objects occurring in them. At\ningest-time, it uses compression and video-specific specialization of CNNs.\nFocus handles the lower accuracy of the cheap CNNs by judiciously leveraging\nexpensive CNNs at query-time. To reduce query time latency, we cluster similar\nobjects and hence avoid redundant processing. Using experiments on video\nstreams from traffic, surveillance and news channels, we see that Focus uses\n58X fewer GPU cycles than running expensive ingest processors and is 37X faster\nthan processing all the video at query time.\n"]},
{"authors": ["Mohammadreza Esfandiari", "Senjuti Basu Roy", "Sihem Amer-Yahia"], "title": ["Eliciting Worker Preference for Task Completion"], "date": ["2018-01-10T03:55:09Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.03233v1"], "summary": ["  Current crowdsourcing platforms provide little support for worker feedback.\nWorkers are sometimes invited to post free text describing their experience and\npreferences in completing tasks. They can also use forums such as Turker\nNation1 to exchange preferences on tasks and requesters. In fact, crowdsourcing\nplatforms rely heavily on observing workers and inferring their preferences\nimplicitly. In this work, we believe that asking workers to indicate their\npreferences explicitly improve their experience in task completion and hence,\nthe quality of their contributions. Explicit elicitation can indeed help to\nbuild more accurate worker models for task completion that captures the\nevolving nature of worker preferences. We design a worker model whose accuracy\nis improved iteratively by requesting preferences for task factors such as\nrequired skills, task payment, and task relevance. We propose a generic\nframework, develop efficient solutions in realistic scenarios, and run\nextensive experiments that show the benefit of explicit preference elicitation\nover implicit ones with statistical significance.\n"]},
{"authors": ["Charalampos E. Tsourakakis", "Shreyas Sekar", "Johnson Lam", "Liu Yang"], "title": ["Risk-Averse Matchings over Uncertain Graph Databases"], "date": ["2018-01-09T23:41:28Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.03190v1"], "summary": ["  A large number of applications such as querying sensor networks, and\nanalyzing protein-protein interaction (PPI) networks, rely on mining uncertain\ngraph and hypergraph databases. In this work we study the following problem:\ngiven an uncertain, weighted (hyper)graph, how can we efficiently find a\n(hyper)matching with high expected reward, and low risk?\n  This problem naturally arises in the context of several important\napplications, such as online dating, kidney exchanges, and team formation. We\nintroduce a novel formulation for finding matchings with maximum expected\nreward and bounded risk under a general model of uncertain weighted\n(hyper)graphs that we introduce in this work. Our model generalizes\nprobabilistic models used in prior work, and captures both continuous and\ndiscrete probability distributions, thus allowing to handle privacy related\napplications that inject appropriately distributed noise to (hyper)edge\nweights. Given that our optimization problem is NP-hard, we turn our attention\nto designing efficient approximation algorithms. For the case of uncertain\nweighted graphs, we provide a $\\frac{1}{3}$-approximation algorithm, and a\n$\\frac{1}{5}$-approximation algorithm with near optimal run time. For the case\nof uncertain weighted hypergraphs, we provide a\n$\\Omega(\\frac{1}{k})$-approximation algorithm, where $k$ is the rank of the\nhypergraph (i.e., any hyperedge includes at most $k$ nodes), that runs in\nalmost (modulo log factors) linear time.\n  We complement our theoretical results by testing our approximation algorithms\non a wide variety of synthetic experiments, where we observe in a controlled\nsetting interesting findings on the trade-off between reward, and risk. We also\nprovide an application of our formulation for providing recommendations of\nteams that are likely to collaborate, and have high impact.\n"]},
{"authors": ["Divya Mahajan", "Joon Kyung Kim", "Jacob Sacks", "Adel Ardalan", "Arun Kumar", "Hadi Esmaeilzadeh"], "title": ["In-RDBMS Hardware Acceleration of Advanced Analytics"], "date": ["2018-01-08T19:04:13Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.06027v1"], "summary": ["  The data revolution is fueled by advances in several areas, including\ndatabases, high-performance computer architecture, and machine learning.\nAlthough timely, there is a void of solutions that brings these disjoint\ndirections together. This paper sets out to be the initial step towards such a\nunion. The aim is to devise a solution for the in-Database Acceleration of\nAdvanced Analytics (DAnA). DAnA empowers database users to leap beyond\ntraditional data summarization techniques and seamlessly utilize\nhardware-accelerated machine learning. Deploying specialized hardware, such as\nFPGAs, for in-database analytics currently requires hand-designing the hardware\nand manually routing the data. Instead, DAnA automatically maps a high-level\nspecification of in-database analytics queries to the FPGA accelerator. The\naccelerator implementation is generated from a User Defined Function (UDF),\nexpressed as part of a SQL query in a Python-embedded Domain Specific Language\n(DSL). To realize efficient in-database integration, DAnA accelerators contain\na novel hardware structure, Striders, that directly interface with the buffer\npool of the database. DAnA obtains the schema and page layout information from\nthe database catalog to configure the Striders. In turn, Striders extract,\ncleanse, and process the training data tuples, which are consumed by a\nmulti-threaded FPGA engine that executes the analytics algorithm. We integrated\nDAnA with PostgreSQL to generate hardware accelerators for a range of\nreal-world and synthetic datasets running diverse ML algorithms. Results show\nthat DAnA-enhanced PostgreSQL provides, on average, 11.3x end-to-end speedup\nthan MADLib and 5.4x faster than multi-threaded MADLib running on Greenplum.\nDAnA provides these benefits while hiding the complexity of hardware design\nfrom data scientists and allowing them to express the algorithm in 30-60 lines\nof Python.\n"]},
{"authors": ["Ali Vakilian", "Yodsawalai Chodpathumwan", "Arash Termehchy", "Amir Nayyeri"], "title": ["Cost-Effective Conceptual Design Using Taxonomies"], "date": ["2015-03-19T06:23:47Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1503.05656v2"], "summary": ["  It is known that annotating named entities in unstructured and\nsemi-structured data sets by their concepts improves the effectiveness of\nanswering queries over these data sets. As every enterprise has a limited\nbudget of time or computational resources, it has to annotate a subset of\nconcepts in a given domain whose costs of annotation do not exceed the budget.\nWe call such a subset of concepts a {\\it conceptual design} for the annotated\ndata set. We focus on finding a conceptual design that provides the most\neffective answers to queries over the annotated data set, i.e., a {\\it\ncost-effective conceptual design}. Since, it is often less time-consuming and\ncostly to annotate general concepts than specific concepts, we use information\non superclass/subclass relationships between concepts in taxonomies to find a\ncost-effective conceptual design. We quantify the amount by which a conceptual\ndesign with concepts from a taxonomy improves the effectiveness of answering\nqueries over an annotated data set. If the taxonomy is a tree, we prove that\nthe problem is NP-hard and propose an efficient approximation and\npseudo-polynomial time algorithms for the problem. We further prove that if the\ntaxonomy is a directed acyclic graph, given some generally accepted hypothesis,\nit is not possible to find any approximation algorithm with reasonably small\napproximation ratio for the problem. Our empirical study using real-world data\nsets, taxonomies, and query workloads shows that our framework effectively\nquantifies the amount by which a conceptual design improves the effectiveness\nof answering queries. It also indicates that our algorithms are efficient for a\ndesign-time task with pseudo-polynomial algorithm being generally more\neffective than the approximation algorithm.\n"]},
{"authors": ["Antoine Amarilli", "Mika\u00ebl Monet", "Pierre Senellart"], "title": ["Connecting Width and Structure in Knowledge Compilation (Extended\n  Version)"], "date": ["2017-09-18T22:15:11Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.06188v2"], "summary": ["  Several query evaluation tasks can be done via knowledge compilation: the\nquery result is compiled as a lineage circuit from which the answer can be\ndetermined. For such tasks, it is important to leverage some width parameters\nof the circuit, such as bounded treewidth or pathwidth, to convert the circuit\nto structured classes, e.g., deterministic structured NNFs (d-SDNNFs) or OBDDs.\nIn this work, we show how to connect the width of circuits to the size of their\nstructured representation, through upper and lower bounds. For the upper bound,\nwe show how bounded-treewidth circuits can be converted to a d-SDNNF, in time\nlinear in the circuit size. Our bound, unlike existing results, is constructive\nand only singly exponential in the treewidth. We show a related lower bound on\nmonotone DNF or CNF formulas, assuming a constant bound on the arity (size of\nclauses) and degree (number of occurrences of each variable). Specifically, any\nd-SDNNF (resp., SDNNF) for such a DNF (resp., CNF) must be of exponential size\nin its treewidth; and the same holds for pathwidth when compiling to OBDDs. Our\nlower bounds, in contrast with most previous work, apply to any formula of this\nclass, not just a well-chosen family. Hence, for our language of DNF and CNF,\npathwidth and treewidth respectively characterize the efficiency of compiling\nto OBDDs and (d-)SDNNFs, that is, compilation is singly exponential in the\nwidth parameter. We conclude by applying our lower bound results to the task of\nquery evaluation.\n"]},
{"authors": ["Antoine Amarilli", "Pierre Bourhis", "Stefan Mengel"], "title": ["Enumeration on Trees under Relabelings"], "date": ["2017-09-18T22:08:44Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.06185v2"], "summary": ["  We study how to evaluate MSO queries with free variables on trees, within the\nframework of enumeration algorithms. Previous work has shown how to enumerate\nanswers with linear-time preprocessing and delay linear in the size of each\noutput, i.e., constant-delay for free first-order variables. We extend this\nresult to support relabelings, a restricted kind of update operations on trees\nwhich allows us to change the node labels. Our main result shows that we can\nenumerate the answers of MSO queries on trees with linear-time preprocessing\nand delay linear in each answer, while supporting node relabelings in\nlogarithmic time. To prove this, we reuse the circuit-based enumeration\nstructure from our earlier work, and develop techniques to maintain its index\nunder node relabelings. We also show how enumeration under relabelings can be\napplied to evaluate practical query languages, such as aggregate, group-by, and\nparameterized queries.\n"]},
{"authors": ["Robert J. Rovetto"], "title": ["An Ontology for Satellite Databases"], "date": ["2018-01-06T16:59:49Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.02940v1"], "summary": ["  This paper demonstrates the development of ontology for satellite databases.\nFirst, I create a computational ontology for the Union of Concerned Scientists\n(UCS) Satellite Database (UCSSD for short), called the UCS Satellite Ontology\n(or UCSSO). Second, in developing UCSSO I show that The Space Situational\nAwareness Ontology (SSAO) (Rovetto and Kelso 2016)--an existing space domain\nreference ontology--and related ontology work by the author (Rovetto 2015,\n2016) can be used either (i) with a database-specific local ontology such as\nUCSSO, or (ii) in its stead. In case (i), local ontologies such as UCSSO can\nreuse SSAO terms, perform term mappings, or extend it. In case (ii), the\nauthor's orbital space ontology work, such as the SSAO, is usable by the UCSSD\nand organizations with other space object catalogs, as a reference ontology\nsuite providing a common semantically-rich domain model. The SSAO, UCSSO, and\nthe broader Orbital Space Environment Domain Ontology project is online at\nhttp://purl.org/space-ontology and GitHub. This ontology effort aims, in part,\nto provide accurate formal representations of the domain for various\napplications. Ontology engineering has the potential to facilitate the sharing\nand integration of satellite data from federated databases and sensors for\nsafer spaceflight.\n"]},
{"authors": ["Daniel Lemire", "Owen Kaser", "Nathan Kurz", "Luca Deri", "Chris O'Hara", "Fran\u00e7ois Saint-Jacques", "Gregory Ssi-Yan-Kai"], "title": ["Roaring Bitmaps: Implementation of an Optimized Software Library"], "date": ["2017-09-22T15:56:49Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.07821v3"], "summary": ["  Compressed bitmap indexes are used in systems such as Git or Oracle to\naccelerate queries. They represent sets and often support operations such as\nunions, intersections, differences, and symmetric differences. Several\nimportant systems such as Elasticsearch, Apache Spark, Netflix's Atlas,\nLinkedIn's Pivot, Metamarkets' Druid, Pilosa, Apache Hive, Apache Tez,\nMicrosoft Visual Studio Team Services and Apache Kylin rely on a specific type\nof compressed bitmap index called Roaring. We present an optimized software\nlibrary written in C implementing Roaring bitmaps: CRoaring. It benefits from\nseveral algorithms designed for the single-instruction-multiple-data (SIMD)\ninstructions available on commodity processors. In particular, we present\nvectorized algorithms to compute the intersection, union, difference and\nsymmetric difference between arrays. We benchmark the library against a wide\nrange of competitive alternatives, identifying weaknesses and strengths in our\nsoftware. Our work is available under a liberal open-source license.\n"]},
{"authors": ["Stephen Macke", "Yiming Zhang", "Silu Huang", "Aditya Parameswaran"], "title": ["Adaptive Sampling for Rapidly Matching Histograms"], "date": ["2017-08-20T01:08:27Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1708.05918v2"], "summary": ["  In exploratory data analysis, analysts often have a need to identify\nhistograms that possess a specific distribution, among a large class of\ncandidate histograms, e.g., find histograms of countries whose income\ndistribution is most similar to that of Greece. This distribution could be a\nnew one that the user is curious about, or a known distribution from an\nexisting histogram visualization. At present, this process of identification is\nbrute-force, requiring the manual generation and evaluation of a large number\nof histogram. We present FastMatch: an end-to-end approach for interactively\nretrieving the histogram visualizations that are most similar to a\nuser-specified target, from a large collection of histograms. The primary\ntechnical contribution underlying FastMatch is a probabilistic algorithm,\nHistSim, a theoretically sound sampling-based approach to identify the top-$k$\nclosest histograms under $\\ell_1$ distance. While HistSim can be used\nindependently, within FastMatch we couple HistSim with a novel system\narchitecture that is aware of practical considerations, employing asynchronous\nblock-based sampling policies, building on lightweight sampling engines\ndeveloped in recent work. In our experiments on several real-world datasets,\nFastMatch obtains near-perfect accuracy with up to $20\\times$ speedups over\nless sophisticated approaches.\n"]},
{"authors": ["Abolfazl Asudeh", "H. V. Jagadish", "Julia Stoyanovich", "Gautam Das"], "title": ["Designing Fair Ranking Schemes"], "date": ["2017-12-28T04:56:06Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1712.09752v2"], "summary": ["  Items from a database are often ranked based on a combination of multiple\ncriteria. A user may have the flexibility to accept combinations that weigh\nthese criteria differently, within limits. On the other hand, this choice of\nweights can greatly affect the fairness of the produced ranking. In this paper,\nwe develop a system that helps users choose criterion weights that lead to\ngreater fairness.\n  We consider ranking functions that compute the score of each item as a\nweighted sum of (numeric) attribute values, and then sort items on their score.\nEach ranking function can be expressed as a vector of weights, or as a point in\na multi-dimensional space. For a broad range of fairness criteria, we show how\nto efficiently identify regions in this space that satisfy these criteria.\nUsing this identification method, our system is able to tell users whether\ntheir proposed ranking function satisfies the desired fairness criteria and, if\nit does not, to suggest the smallest modification that does. We develop\nuser-controllable approximation that and indexing techniques that are applied\nduring preprocessing, and support sub-second response times during the online\nphase. Our extensive experiments on real datasets demonstrate that our methods\nare able to find solutions that satisfy fairness criteria effectively and\nefficiently.\n"]},
{"authors": ["Tarique Siddiqui", "Albert Kim", "John Lee", "Karrie Karahalios", "Aditya Parameswaran"], "title": ["Effortless Data Exploration with zenvisage: An Expressive and\n  Interactive Visual Analytics System"], "date": ["2016-04-12T21:00:46Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1604.03583v3"], "summary": ["  Data visualization is by far the most commonly used mechanism to explore\ndata, especially by novice data analysts and data scientists. And yet, current\nvisual analytics tools are rather limited in their ability to guide data\nscientists to interesting or desired visualizations: the process of visual data\nexploration remains cumbersome and time-consuming. We propose zenvisage, a\nplatform for effortlessly visualizing interesting patterns, trends, or insights\nfrom large datasets. We describe zenvisage's general purpose visual query\nlanguage, ZQL (\"zee-quel\") for specifying the desired visual trend, pattern, or\ninsight - ZQL draws from use-cases in a variety of domains, including biology,\nmechanical engineering, climate science, and commerce. We formalize the\nexpressiveness of ZQL via a visual exploration algebra, and demonstrate that\nZQL is at least as expressive as that algebra. While analysts are free to use\nZQL directly, we also expose ZQL via a visual specification interface that we\ndescribe in this paper. We then describe our architecture and optimizations,\npreliminary experiments in supporting and optimizing for ZQL queries in our\ninitial zenvisage prototype, and a user study to evaluate whether data\nscientists are able to effectively use zenvisage for real applications.\n"]},
{"authors": ["Shuai Ma", "Jia Li", "Chunming Hu", "Xudong Liu", "Jinpeng Huai"], "title": ["Graph Pattern Matching for Dynamic Team Formation"], "date": ["2018-01-03T14:24:08Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.01012v1"], "summary": ["  Finding a list of k teams of experts, referred to as top-k team formation,\nwith the required skills and high collaboration compatibility has been\nextensively studied. However, existing methods have not considered the specific\ncollaboration relationships among different team members, i.e., structural\nconstraints, which are typically needed in practice. In this study, we first\npropose a novel graph pattern matching approach for top-k team formation, which\nincorporates both structural constraints and capacity bounds. Second, we\nformulate and study the dynamic top-k team formation problem due to the growing\nneed of a dynamic environment. Third, we develop an unified incremental\napproach, together with an optimization technique, to handle continuous pattern\nand data updates, separately and simultaneously, which has not been explored\nbefore. Finally, using real-life and synthetic data, we conduct an extensive\nexperimental study to show the effectiveness and efficiency of our graph\npattern matching approach for (dynamic) top-k team formation.\n"]},
{"authors": ["Scott Yang", "Silvia Lopez", "Meysam Golmohammadi", "Iyad Obeid", "Joseph Picone"], "title": ["Semi-automated Annotation of Signal Events in Clinical EEG Data"], "date": ["2018-01-03T03:47:20Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.02476v1"], "summary": ["  To be effective, state of the art machine learning technology needs large\namounts of annotated data. There are numerous compelling applications in\nhealthcare that can benefit from high performance automated decision support\nsystems provided by deep learning technology, but they lack the comprehensive\ndata resources required to apply sophisticated machine learning models.\nFurther, for economic reasons, it is very difficult to justify the creation of\nlarge annotated corpora for these applications. Hence, automated annotation\ntechniques become increasingly important. In this study, we investigated the\neffectiveness of using an active learning algorithm to automatically annotate a\nlarge EEG corpus. The algorithm is designed to annotate six types of EEG\nevents. Two model training schemes, namely threshold-based and volume-based,\nare evaluated. In the threshold-based scheme the threshold of confidence scores\nis optimized in the initial training iteration, whereas for the volume-based\nscheme only a certain amount of data is preserved after each iteration.\nRecognition performance is improved 2% absolute and the system is capable of\nautomatically annotating previously unlabeled data. Given that the\ninterpretation of clinical EEG data is an exceedingly difficult task, this\nstudy provides some evidence that the proposed method is a viable alternative\nto expensive manual annotation.\n"]},
{"authors": ["Matthias Boehm", "Berthold Reinwald", "Dylan Hutchison", "Alexandre V. Evfimievski", "Prithviraj Sen"], "title": ["On Optimizing Operator Fusion Plans for Large-Scale Machine Learning in\n  SystemML"], "date": ["2018-01-02T20:40:19Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.00829v1"], "summary": ["  Many large-scale machine learning (ML) systems allow specifying custom ML\nalgorithms by means of linear algebra programs, and then automatically generate\nefficient execution plans. In this context, optimization opportunities for\nfused operators---in terms of fused chains of basic operators---are ubiquitous.\nThese opportunities include (1) fewer materialized intermediates, (2) fewer\nscans of input data, and (3) the exploitation of sparsity across chains of\noperators. Automatic operator fusion eliminates the need for hand-written fused\noperators and significantly improves performance for complex or previously\nunseen chains of operations. However, existing fusion heuristics struggle to\nfind good fusion plans for complex DAGs or hybrid plans of local and\ndistributed operations. In this paper, we introduce an optimization framework\nfor systematically reason about fusion plans that considers materialization\npoints in DAGs, sparsity exploitation, different fusion template types, as well\nas local and distributed operations. In detail, we contribute algorithms for\n(1) candidate exploration of valid fusion plans, (2) cost-based candidate\nselection, and (3) code generation of local and distributed operations over\ndense, sparse, and compressed data. Our experiments in SystemML show end-to-end\nperformance improvements with optimized fusion plans of up to 21x compared to\nhand-written fused operators, with negligible optimization and code generation\noverhead.\n"]},
{"authors": ["Rachana Nget", "Yang Cao", "Masatoshi Yoshikawa"], "title": ["How to Balance Privacy and Money through Pricing Mechanism in Personal\n  Data Market"], "date": ["2017-05-08T17:28:01Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1705.02982v2"], "summary": ["  A personal data market is a platform including three participants: data\nowners (individuals), data buyers and market maker. Data owners who provide\npersonal data are compensated according to their privacy loss. Data buyers can\nsubmit a query and pay for the result according to their desired accuracy.\nMarket maker coordinates between data owner and buyer. This framework has been\npreviously studied based on differential privacy. However, the previous study\nassumes data owners can accept any level of privacy loss and data buyers can\nconduct the transaction without regard to the financial budget. In this paper,\nwe propose a practical personal data trading framework that is able to strike a\nbalance between money and privacy. In order to gain insights on user\npreferences, we first conducted an online survey on human attitude to- ward\nprivacy and interest in personal data trading. Second, we identify the 5 key\nprinciples of personal data market, which is important for designing a\nreasonable trading frame- work and pricing mechanism. Third, we propose a\nreason- able trading framework for personal data which provides an overview of\nhow the data is traded. Fourth, we propose a balanced pricing mechanism which\ncomputes the query price for data buyers and compensation for data owners\n(whose data are utilized) as a function of their privacy loss. The main goal is\nto ensure a fair trading for both parties. Finally, we will conduct an\nexperiment to evaluate the output of our proposed pricing mechanism in\ncomparison with other previously proposed mechanism.\n"]},
{"authors": ["El Kindi Rezig", "Mourad Ouzzani", "Ahmed K. Elmagarmid", "Walid G. Aref"], "title": ["Human-Centric Data Cleaning [Vision]"], "date": ["2017-12-24T22:28:18Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1712.08971v2"], "summary": ["  Data Cleaning refers to the process of detecting and fixing errors in the\ndata. Human involvement is instrumental at several stages of this process,\ne.g., to identify and repair errors, to validate computed repairs, etc. There\nis currently a plethora of data cleaning algorithms addressing a wide range of\ndata errors (e.g., detecting duplicates, violations of integrity constraints,\nmissing values, etc.). Many of these algorithms involve a human in the loop,\nhowever, this latter is usually coupled to the underlying cleaning algorithms.\nThere is currently no end-to-end data cleaning framework that systematically\ninvolves humans in the cleaning pipeline regardless of the underlying cleaning\nalgorithms. In this paper, we highlight key challenges that need to be\naddressed to realize such a framework. We present a design vision and discuss\nscenarios that motivate the need for such a framework to judiciously assist\nhumans in the cleaning process. Finally, we present directions to implement\nsuch a framework.\n"]},
{"authors": ["Renzo Angles", "Claudio Gutierrez"], "title": ["An introduction to Graph Data Management"], "date": ["2017-12-29T21:02:12Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1801.00036v1"], "summary": ["  A graph database is a database where the data structures for the schema\nand/or instances are modeled as a (labeled)(directed) graph or generalizations\nof it, and where querying is expressed by graph-oriented operations and type\nconstructors. In this article we present the basic notions of graph databases,\ngive an historical overview of its main development, and study the main current\nsystems that implement them.\n"]},
{"authors": ["Francisco Maturana", "Cristian Riveros", "Domagoj Vrgo\u010d"], "title": ["Document Spanners for Extracting Incomplete Information: Expressiveness\n  and Complexity"], "date": ["2017-07-04T06:41:17Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1707.00827v2"], "summary": ["  Rule-based information extraction has lately received a fair amount of\nattention from the database community, with several languages appearing in the\nlast few years. Although information extraction systems are intended to deal\nwith semistructured data, all language proposals introduced so far are designed\nto output relations, thus making them incapable of handling incomplete\ninformation. To remedy the situation, we propose to extend information\nextraction languages with the ability to use mappings, thus allowing us to work\nwith documents which have missing or optional parts. Using this approach, we\nsimplify the semantics of regex formulas and extraction rules, two previously\ndefined methods for extracting information, extend them with the ability to\nhandle incomplete data, and study how they compare in terms of expressive\npower. We also study computational properties of these languages, focusing on\nthe query enumeration problem, as well as satisfiability and containment.\n"]},
{"authors": ["Varunya Attasena", "J\u00e9r\u00f4me Darmont", "Nouria Harbi"], "title": ["Secret Sharing for Cloud Data Security"], "date": ["2017-12-29T09:10:21Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1712.10155v1"], "summary": ["  Cloud computing helps reduce costs, increase business agility and deploy\nsolutions with a high return on investment for many types of applications.\nHowever, data security is of premium importance to many users and often\nrestrains their adoption of cloud technologies. Various approaches, i.e., data\nencryption, anonymization, replication and verification, help enforce different\nfacets of data security. Secret sharing is a particularly interesting\ncryptographic technique. Its most advanced variants indeed simultaneously\nenforce data privacy, availability and integrity, while allowing computation on\nencrypted data. The aim of this paper is thus to wholly survey secret sharing\nschemes with respect to data security, data access and costs in the\npay-as-you-go paradigm.\n"]},
{"authors": ["Robert J. Rovetto"], "title": ["An Ontological Architecture for Orbital Debris Data"], "date": ["2017-04-01T21:26:37Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1704.01014v2"], "summary": ["  The orbital debris problem presents an opportunity for inter-agency and\ninternational cooperation toward the mutually beneficial goals of debris\nprevention, mitigation, remediation, and improved space situational awareness\n(SSA). Achieving these goals requires sharing orbital debris and other SSA\ndata. Toward this, I present an ontological architecture for the orbital debris\ndomain, taking steps in the creation of an orbital debris ontology (ODO). The\npurpose of this ontological system is to (I) represent general orbital debris\nand SSA domain knowledge, (II) structure, and standardize where needed, orbital\ndata and terminology, and (III) foster semantic interoperability and\ndata-sharing. In doing so I hope to (IV) contribute to solving the orbital\ndebris problem, improving peaceful global SSA, and ensuring safe space travel\nfor future generations.\n"]},
{"authors": ["Nicolas Le Scouarnec"], "title": ["Cuckoo++ Hash Tables: High-Performance Hash Tables for Networking\n  Applications"], "date": ["2017-12-27T16:40:15Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1712.09624v1"], "summary": ["  Hash tables are an essential data-structure for numerous networking\napplications (e.g., connection tracking, firewalls, network address\ntranslators). Among these, cuckoo hash tables provide excellent performance by\nallowing lookups to be processed with very few memory accesses (2 to 3 per\nlookup). Yet, for large tables, cuckoo hash tables remain memory bound and each\nmemory access impacts performance. In this paper, we propose algorithmic\nimprovements to cuckoo hash tables allowing to eliminate some unnecessary\nmemory accesses; these changes are conducted without altering the properties of\nthe original cuckoo hash table so that all existing theoretical analysis remain\napplicable. On a single core, our hash table achieves 37M lookups per second\nfor positive lookups (i.e., when the key looked up is present in the table),\nand 60M lookups per second for negative lookups, a 50% improvement over the\nimplementation included into the DPDK. On a 18-core, with mostly positive\nlookups, our implementation achieves 496M lookups per second, a 45% improvement\nover DPDK.\n"]},
{"authors": ["Yuqing Zhu", "Jianfeng Zhan", "Chuliang Weng", "Raghunath Nambiar", "Jinchao Zhang", "Xingzhen Chen", "Lei Wang"], "title": ["BigOP: Generating Comprehensive Big Data Workloads as a Benchmarking\n  Framework"], "date": ["2014-01-26T08:41:50Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1401.6628v2"], "summary": ["  Big Data is considered proprietary asset of companies, organizations, and\neven nations. Turning big data into real treasure requires the support of big\ndata systems. A variety of commercial and open source products have been\nunleashed for big data storage and processing. While big data users are facing\nthe choice of which system best suits their needs, big data system developers\nare facing the question of how to evaluate their systems with regard to general\nbig data processing needs. System benchmarking is the classic way of meeting\nthe above demands. However, existent big data benchmarks either fail to\nrepresent the variety of big data processing requirements, or target only one\nspecific platform, e.g. Hadoop.\n  In this paper, with our industrial partners, we present BigOP, an end-to-end\nsystem benchmarking framework, featuring the abstraction of representative\nOperation sets, workload Patterns, and prescribed tests. BigOP is part of an\nopen-source big data benchmarking project, BigDataBench. BigOP's abstraction\nmodel not only guides the development of BigDataBench, but also enables\nautomatic generation of tests with comprehensive workloads.\n  We illustrate the feasibility of BigOP by implementing an automatic test\ngeneration tool and benchmarking against three widely used big data processing\nsystems, i.e. Hadoop, Spark and MySQL Cluster. Three tests targeting three\ndifferent application scenarios are prescribed. The tests involve relational\ndata, text data and graph data, as well as all operations and workload\npatterns. We report results following test specifications.\n"]},
{"authors": ["Joseph Paul Cohen", "Wei Ding", "Abraham Bagherjeiran"], "title": ["XTreePath: A generalization of XPath to handle real world structural\n  variation"], "date": ["2015-05-06T10:08:12Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1505.01303v3"], "summary": ["  We discuss a key problem in information extraction which deals with wrapper\nfailures due to changing content templates. A good proportion of wrapper\nfailures are due to HTML templates changing to cause wrappers to become\nincompatible after element inclusion or removal in a DOM (Tree representation\nof HTML). We perform a large-scale empirical analyses of the causes of shift\nand mathematically quantify the levels of domain difficulty based on entropy.\nWe propose the XTreePath annotation method to captures contextual node\ninformation from the training DOM. We then utilize this annotation in a\nsupervised manner at test time with our proposed Recursive Tree Matching method\nwhich locates nodes most similar in context recursively using the tree edit\ndistance. The search is based on a heuristic function that takes into account\nthe similarity of a tree compared to the structure that was present in the\ntraining data. We evaluate XTreePath using 117,422 pages from 75 diverse\nwebsites in 8 vertical markets. Our XTreePath method consistently outperforms\nXPath and a current commercial system in terms of successful extractions in a\nblackbox test. We make our code and datasets publicly available online.\n"]},
{"authors": ["El Kindi Rezig", "Mourad Ouzzani", "Walid G. Aref", "Ahmed K. Elmagarmid", "Ahmed R. Mahmood"], "title": ["Pattern-Driven Data Cleaning"], "date": ["2017-12-26T22:06:58Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1712.09437v1"], "summary": ["  Data is inherently dirty and there has been a sustained effort to come up\nwith different approaches to clean it. A large class of data repair algorithms\nrely on data-quality rules and integrity constraints to detect and repair the\ndata. A well-studied class of integrity constraints is Functional Dependencies\n(FDs, for short) that specify dependencies among attributes in a relation. In\nthis paper, we address three major challenges in data repairing: (1) Accuracy:\nMost existing techniques strive to produce repairs that minimize changes to the\ndata. However, this process may produce incorrect combinations of attribute\nvalues (or patterns). In this work, we formalize the interaction of FD-induced\npatterns and select repairs that result in preserving frequent patterns found\nin the original data. This has the potential to yield a better repair quality\nboth in terms of precision and recall. (2) Interpretability of repairs: Current\ndata repair algorithms produce repairs in the form of data updates that are not\nnecessarily understandable. This makes it hard to debug repair decisions and\ntrace the chain of steps that produced them. To this end, we define a new\nformalism to declaratively express repairs that are easy for users to reason\nabout. (3) Scalability: We propose a linear-time algorithm to compute repairs\nthat outperforms state-of-the-art FD repairing algorithms by orders of\nmagnitude in repair time. Our experiments using both real-world and synthetic\ndata demonstrate that our new repair approach consistently outperforms existing\ntechniques both in terms of repair quality and scalability.\n"]},
{"authors": ["Yu Zhou", "Jianbin Huang", "Heli Sun", "Yizhou Sun", "Hong Chong"], "title": ["DMSS: A Robust Deep Meta Structure Based Similarity Measure in\n  Heterogeneous Information Networks"], "date": ["2017-12-25T05:07:01Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1712.09008v1"], "summary": ["  Similarity measure as a fundamental task in heterogeneous information network\nanalysis has been applied to many areas, e.g. product recommendation,\nclustering and web search. The state-of-the-art metrics depend on meta paths or\nmeta structures specified by users. In this paper, a novel similarity measure\non heterogeneous information networks, called Deep Meta Structure based\nSimilarity ($DMSS$), is proposed. The deep meta structure as a schematic\nstructure on heterogeneous information networks provides a unified framework\nintegrating all the meta paths and meta structures. It can be constructed\nautomatically. In order to formalize the semantics encapsulated in the deep\nmeta structure, we decompose it into several deep meta paths, and then combine\nall the commuting matrices of these deep meta paths according to different\nweights. It is noteworthy that the weights can be determined by the proposed\nstrategy. As a result, $DMSS$ is defined by virtue of the final commuting\nmatrix and therefore is robust to different schematic structures. Experimental\nevaluations show that the state-of-the-art metrics are really sensitive to meta\npaths or meta structures in terms of clustering and ranking. Besides, $DMSS$\noutperforms the state-of-the-art metrics in terms of ranking and clustering in\nthe case of selecting an appropriate decaying parameter.\n"]},
{"authors": ["Stefan Mengel", "Sebastian Skritek"], "title": ["On tractable query evaluation for SPARQL"], "date": ["2017-12-24T15:40:08Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1712.08939v1"], "summary": ["  Despite much work within the last decade on foundational properties of SPARQL\n- the standard query language for RDF data - rather little is known about the\nexact limits of tractability for this language. In particular, this is the case\nfor SPARQL queries that contain the OPTIONAL-operator, even though it is one of\nthe most intensively studied features of SPARQL. The aim of our work is to\nprovide a more thorough picture of tractable classes of SPARQL queries.\n  In general, SPARQL query evaluation is PSPACE-complete in combined\ncomplexity, and it remains PSPACE-hard already for queries containing only the\nOPTIONAL-operator. To amend this situation, research has focused on\n\"well-designed SPARQL queries\" and their recent generalization \"weakly\nwell-designed SPARQL queries\". For these two fragments the evaluation problem\nis coNP-complete in the absence of projection and SigmaP2-complete otherwise.\nMoreover, they have been shown to contain most SPARQL queries asked in\npractical settings.\n  In this paper, we study tractable classes of weakly well-designed queries in\nparameterized complexity considering the equivalent formulation as pattern\ntrees. We give a complete characterization of the tractable classes in the case\nwithout projection. Moreover, we show a characterization of all tractable\nclasses of simple well-designed pattern trees in the presence of projection.\n"]},
{"authors": ["Niel Chah"], "title": ["Freebase-triples: A Methodology for Processing the Freebase Data Dumps"], "date": ["2017-12-23T03:15:50Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1712.08707v1"], "summary": ["  The Freebase knowledge base was a significant Semantic Web and linked data\ntechnology during its years of operations since 2007. Following its acquisition\nby Google in 2010 and its shutdown in 2016, Freebase data is contained in a\ndata dump of billions of RDF triples. In this research, an exploration of the\nFreebase data dumps will show best practices in understanding and using the\nFreebase data and also present a general methodology for parsing the linked\ndata. The analysis is done with limited computing resources and the use of\nopen-source Unix-like tools. The results showcase the efficiency of the\ntechnique and highlight redundancies in the data, with the possibility of\nrestructuring nearly 60% of the original data. As an archival dataset that has\nnot changed since 2015, Freebase's semantic structured data has applications in\nother prominent fields, such as information retrieval (IR) and knowledge-based\nquestion answering (KBQA). Freebase can also serve as a gateway to other\nstructured datasets, such as DBpedia, Wikidata, and YAGO.\n"]},
{"authors": ["Liat Peterfreund", "Balder ten Cate", "Ronald Fagin", "Benny Kimelfeld"], "title": ["Recursive Programs for Document Spanners"], "date": ["2017-12-21T20:22:47Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1712.08198v1"], "summary": ["  A document spanner models a program for Information Extraction (IE) as a\nfunction that takes as input a text document (string over a finite alphabet)\nand produces a relation of spans (intervals in the document) over a predefined\nschema. A well studied language for expressing spanners is that of the regular\nspanners: relational algebra over regex formulas, which are obtained by adding\ncapture variables to regular expressions. Equivalently, the regular spanners\nare the ones expressible in non-recursive Datalog over regex formulas\n(extracting relations that play the role of EDBs from the input document). In\nthis paper, we investigate the expressive power of recursive Datalog over regex\nformulas. Our main result is that such programs capture precisely the document\nspanners computable in polynomial time. Additional results compare recursive\nprograms to known formalisms such as the language of core spanners (that\nextends regular spanners by allowing to test for string equality) and its\nclosure under difference. Finally, we extend our main result to a recently\nproposed framework that generalizes both the relational model and document\nspanners.\n"]},
{"authors": ["Wenying Ji", "Simaan M. AbouRizk", "Osmar R. Zaiane", "Yitong Li"], "title": ["Complexity Analysis Approach for Prefabricated Construction Products\n  Using Uncertain Data Clustering"], "date": ["2017-10-29T03:30:36Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.10555v2"], "summary": ["  This paper proposes an uncertain data clustering approach to quantitatively\nanalyze the complexity of prefabricated construction components through the\nintegration of quality performance-based measures with associated engineering\ndesign information. The proposed model is constructed in three steps, which (1)\nmeasure prefabricated construction product complexity (hereafter referred to as\nproduct complexity) by introducing a Bayesian-based nonconforming quality\nperformance indicator; (2) score each type of product complexity by developing\na Hellinger distance-based distribution similarity measurement; and (3) cluster\nproducts into homogeneous complexity groups by using the agglomerative\nhierarchical clustering technique. An illustrative example is provided to\ndemonstrate the proposed approach, and a case study of an industrial company in\nEdmonton, Canada, is conducted to validate the feasibility and applicability of\nthe proposed model. This research inventively defines and investigates product\ncomplexity from the perspective of product quality performance with design\ninformation associated. The research outcomes provide simplified,\ninterpretable, and informative insights for practitioners to better analyze and\nmanage product complexity. In addition to this practical contribution, a novel\nhierarchical clustering technique is devised. This technique is capable of\nclustering uncertain data (i.e., beta distributions) with lower computational\ncomplexity and has the potential to be generalized to cluster all types of\nuncertain data.\n"]},
{"authors": ["Nofar Carmeli", "Markus Kr\u00f6ll"], "title": ["Enumeration Complexity of Conjunctive Queries with Functional\n  Dependencies"], "date": ["2017-12-21T11:15:55Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1712.07880v1"], "summary": ["  We study the complexity of enumerating the answers of Conjunctive Queries\n(CQs) in the presence of Functional Dependencies (FDs). Our focus is on the\nability to list output tuples with a constant delay in between, following a\nlinear-time preprocessing. A known dichotomy classifies the acyclic\nself-join-free CQs into those that admit such enumeration, and those that do\nnot. However, this classification no longer holds in the common case where the\ndatabase exhibits dependencies among attributes. That is, some queries that are\nclassified as hard are in fact tractable if dependencies are accounted for. We\nestablish a generalization of the dichotomy to accommodate FDs; hence, our\nclassification determines which combination of a CQ and a set of FDs admits\nconstant-delay enumeration with a linear-time preprocessing.\n  In addition, we generalize a hardness result for cyclic CQs to accommodate a\ncommon type of FDs. Further conclusions of our development include a dichotomy\nfor enumeration with linear delay, and a dichotomy for CQs with disequalities.\nFinally, we show that all our results apply to the known class of \"cardinality\ndependencies\" that generalize FDs (e.g., by stating an upper bound on the\nnumber of genres per movies, or friends per person).\n"]},
{"authors": ["Ester Livshits", "Benny Kimelfeld", "Sudeepa Roy"], "title": ["Computing Optimal Repairs for Functional Dependencies"], "date": ["2017-12-20T20:56:52Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1712.07705v1"], "summary": ["  We investigate the complexity of computing an optimal repair of an\ninconsistent database, in the case where integrity constraints are Functional\nDependencies (FDs). We focus on two types of repairs: an optimal subset repair\n(optimal S-repair) that is obtained by a minimum number of tuple deletions, and\nan optimal update repair (optimal U-repair) that is obtained by a minimum\nnumber of value (cell) updates. For computing an optimal S-repair, we present a\npolynomial-time algorithm that succeeds on certain sets of FDs and fails on\nothers. We prove the following about the algorithm. When it succeeds, it can\nalso incorporate weighted tuples and duplicate tuples. When it fails, the\nproblem is NP-hard, and in fact, APX-complete (hence, cannot be approximated\nbetter than some constant). Thus, we establish a dichotomy in the complexity of\ncomputing an optimal S-repair. We present general analysis techniques for the\ncomplexity of computing an optimal U-repair, some based on the dichotomy for\nS-repairs. We also draw a connection to a past dichotomy in the complexity of\nfinding a \"most probable database\" that satisfies a set of FDs with a single\nattribute on the left hand side; the case of general FDs was left open, and we\nshow how our dichotomy provides the missing generalization and thereby settles\nthe open problem.\n"]},
{"authors": ["Mahmoud Abo Khamis", "Hung Q. Ngo", "Dan Olteanu", "Dan Suciu"], "title": ["Boolean Tensor Decomposition for Conjunctive Queries with Negation"], "date": ["2017-12-20T12:30:59Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1712.07445v1"], "summary": ["  We propose an algorithm for answering conjunctive queries with negation,\nwhere the negated relations are sparse. Its data complexity matches that of the\nbest known algorithms for the positive subquery of the input query and is\nexpressed in terms of the fractional hypertree width and the submodular width.\nThe query complexity depends on the structure of the negated subquery; in\ngeneral it is exponential in the number of join variables occurring in negated\nrelations yet it becomes polynomial for several classes of queries.\n  This algorithm relies on several contributions. We show how to rewrite\nqueries with negation on sparse relations into equivalent conjunctive queries\nwith not-all-equal (NAE) predicates, which are a multi-dimensional analog of\ndisequality. We then generalize the known color-coding technique to\nconjunctions of NAE predicates and explain it via a Boolean tensor\ndecomposition of conjunctions of NAE predicates. This decomposition can be\nachieved via a probabilistic construction that can be derandomized efficiently.\n"]},
{"authors": ["Rajesh Bordawekar", "Bortik Bandyopadhyay", "Oded Shmueli"], "title": ["Cognitive Database: A Step towards Endowing Relational Databases with\n  Artificial Intelligence Capabilities"], "date": ["2017-12-19T20:49:26Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1712.07199v1"], "summary": ["  We propose Cognitive Databases, an approach for transparently enabling\nArtificial Intelligence (AI) capabilities in relational databases. A novel\naspect of our design is to first view the structured data source as meaningful\nunstructured text, and then use the text to build an unsupervised neural\nnetwork model using a Natural Language Processing (NLP) technique called word\nembedding. This model captures the hidden inter-/intra-column relationships\nbetween database tokens of different types. For each database token, the model\nincludes a vector that encodes contextual semantic relationships. We seamlessly\nintegrate the word embedding model into existing SQL query infrastructure and\nuse it to enable a new class of SQL-based analytics queries called cognitive\nintelligence (CI) queries. CI queries use the model vectors to enable complex\nqueries such as semantic matching, inductive reasoning queries such as\nanalogies, predictive queries using entities not present in a database, and,\nmore generally, using knowledge from external sources. We demonstrate unique\ncapabilities of Cognitive Databases using an Apache Spark based prototype to\nexecute inductive reasoning CI queries over a multi-modal database containing\ntext and images. We believe our first-of-a-kind system exemplifies using AI\nfunctionality to endow relational databases with capabilities that were\npreviously very hard to realize in practice.\n"]},
{"authors": ["Rustam Azimov", "Semyon Grigorev"], "title": ["Context-Free Path Querying by Matrix Multiplication"], "date": ["2017-07-04T14:22:18Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1707.01007v2"], "summary": ["  Graph data models are widely used in many areas, for example, bioinformatics,\ngraph databases. In these areas, it is often required to process queries for\nlarge graphs. Some of the most common graph queries are navigational queries.\nThe result of query evaluation is a set of implicit relations between nodes of\nthe graph, i.e. paths in the graph. A natural way to specify these relations is\nby specifying paths using formal grammars over the alphabet of edge labels. An\nanswer to a context-free path query in this approach is usually a set of\ntriples (A, m, n) such that there is a path from the node m to the node n,\nwhose labeling is derived from a non-terminal A of the given context-free\ngrammar. This type of queries is evaluated using the relational query\nsemantics. Another example of path query semantics is the single-path query\nsemantics which requires presenting a single path from the node m to the node\nn, whose labeling is derived from a non-terminal A for all triples (A, m, n)\nevaluated using the relational query semantics. There is a number of algorithms\nfor query evaluation which use these semantics but all of them perform poorly\non large graphs. One of the most common technique for efficient big data\nprocessing is the use of a graphics processing unit (GPU) to perform\ncomputations, but these algorithms do not allow to use this technique\nefficiently. In this paper, we show how the context-free path query evaluation\nusing these query semantics can be reduced to the calculation of the matrix\ntransitive closure. Also, we propose an algorithm for context-free path query\nevaluation which uses relational query semantics and is based on matrix\noperations that make it possible to speed up computations by using a GPU.\n"]},
{"authors": ["Siddhartha Sahu", "Amine Mhedhbi", "Semih Salihoglu", "Jimmy Lin", "M. Tamer \u00d6zsu"], "title": ["The Ubiquity of Large Graphs and Surprising Challenges of Graph\n  Processing"], "date": ["2017-09-10T22:25:13Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.03188v2"], "summary": ["  Graph processing is becoming increasingly prevalent across many application\ndomains. In spite of this prevalence, there is little research about how graphs\nare actually used in practice. We conducted an online survey aimed at\nunderstanding: (i) the types of graphs users have; (ii) the graph computations\nusers run; (iii) the types of graph software users use; and (iv) the major\nchallenges users face when processing their graphs. We describe the\nparticipants' responses to our questions highlighting common patterns and\nchallenges. We further reviewed user feedback in the mailing lists, bug\nreports, and feature requests in the source repositories of a large suite of\nsoftware products for processing graphs. Through our review, we were able to\nanswer some new questions that were raised by participants' responses and\nidentify specific challenges that users face when using different classes of\ngraph software. The participants' responses and data we obtained revealed\nsurprising facts about graph processing in practice. In particular, real-world\ngraphs represent a very diverse range of entities and are often very large, and\nscalability and visualization are undeniably the most pressing challenges faced\nby participants. We hope these findings can guide future research.\n"]},
{"authors": ["Han-mook Yoo", "Han-joon Kim", "Jonghoon Chun"], "title": ["Estimation of Individual Micro Data from Aggregated Open Data"], "date": ["2017-12-19T06:41:36Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1712.06802v1"], "summary": ["  In this paper, we propose a method of estimating individual micro data from\naggregated open data based on semi-supervised learning and conditional\nprobability. Firstly, the proposed method collects aggregated open data and\nsupport data, which are related to the individual micro data to be estimated.\nThen, we perform the locality sensitive hashing (LSH) algorithm to find a\nsubset of the support data that is similar to the aggregated open data and then\nclassify them by using the Ensemble classification model, which is learned by\nsemi-supervised learning. Finally, we use conditional probability to estimate\nthe individual micro data by finding the most suitable record for the\nprobability distribution of the individual micro data among the classification\nresults. To evaluate the performance of the proposed method, we estimated the\nindividual building data where the fire occurred using the aggregated fire open\ndata. According to the experimental results, the micro data estimation\nperformance of the proposed method is 59.41% on average in terms of accuracy.\n"]},
{"authors": ["Dong Deng", "Albert Kim", "Samuel Madden", "Michael Stonebraker"], "title": ["SilkMoth: An Efficient Method for Finding Related Sets with Maximum\n  Matching Constraints"], "date": ["2017-04-16T09:03:14Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1704.04738v3"], "summary": ["  Determining if two sets are related - that is, if they have similar values or\nif one set contains the other - is an important problem with many applications\nin data cleaning, data integration, and information retrieval. A particularly\npopular metric that has been proposed is to measure the relatedness of two sets\nby treating the elements as vertices of a bipartite graph and calculating the\nscore of the maximum matching pairing between elements. Compared to other\nmetrics which require exact matchings between elements, this metric uses a\nsimilarity function to compare elements between the two sets, making it robust\nto small dissimilarities in elements and more useful for real-world, dirty\ndata. Unfortunately, the metric suffers from expensive computational cost,\ntaking O(n^3) time, where n is the number of elements in sets, for each\nset-to-set comparison. Thus for applications which try to search for all\npairings of related sets in a brute-force manner, the runtime becomes\nunacceptably large.\n  To address this challenge, we developed SilkMoth, a system capable of rapidly\ndiscovering related set pairs in collections of sets. Internally, SilkMoth\ncreates a signature for each set, with the property that any other set which is\nrelated must match the signature. SilkMoth then uses these signatures to prune\nthe search space, so only sets which match the signatures are left as\ncandidates. Finally, SilkMoth applies the maximum matching metric on remaining\ncandidates to verify which of these candidates are truly related sets. Thus, a\ncontribution of this paper is the characterization of the space of signatures\nwhich enable this property. We show that selecting the optimal signature in\nthis space is NP-complete, and based on insights from the characterization of\nthe space, we propose two novel filters which help to prune the candidates\nfurther before verification.\n"]},
{"authors": ["Dong Deng", "Wenbo Tao", "Ziawasch Abedjan", "Ahmed Elmagarmid", "Ihab F. Ilyas", "Samuel Madden", "Mourad Ouzzani", "Michael Stonebraker", "Nan Tang"], "title": ["Entity Consolidation: The Golden Record Problem"], "date": ["2017-09-29T14:48:56Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.10436v3"], "summary": ["  Four key processes in data integration are: data preparation (i.e.,\nextracting, transforming, and cleaning data), schema integration (i.e.,\nidentifying attribute correspondences), entity resolution (i.e., finding\nclusters of records that represent the same entity) and entity consolidation\n(i.e., merging each cluster into a 'golden record', which contains the\ncanonical value for each attribute). In real-world scenarios, Master Data\nManagement (MDM) is often employed to perform the last two steps using a\n'match-merge' module, which is driven by a collection of user-written match and\nmerge rules. However, it is well understood that such solutions often fail to\nscale to the sizes and complexity of problems currently being addressed.\n  Scalable solutions to entity resolution have been previously developed to\ngenerate a collection of clusters of records representing the same entity. In\nthis paper, we propose a scalable entity consolidation algorithm to merge these\nclusters into 'golden records'. We first automatically generate matching rules\nfrom the clusters and then group these rules into sets with common\ncharacteristics to cut down on the number which must be verified by a human.\nNext, for the human-approved rule groups, we apply them to merge the duplicate\nattribute values in the clusters. Finally, we employ existing truth discovery\nmethods, such as majority consensus (MC), to resolve remaining conflicts in the\nclusters. We applied our methods on three real-world datasets. In a dataset\nwith 31,023 clusters and 80,451 duplicate records, 72,239 matching rules were\ngenerated. By having a human con rm only 100 algorithm-generated rule groups,\nwe achieved a recall of 75% and a precision of 98% for merging duplicate\nattribute values. When we invoked our algorithm prior to running MC, we\nimproved the precision of golden record construction by 40%.\n"]},
{"authors": ["Dong Deng"], "title": ["Error-Tolerant Big Data Processing"], "date": ["2017-12-18T01:57:39Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1712.06223v1"], "summary": ["  Real-world data contains various kinds of errors. Before analyzing data, one\nusually needs to process the raw data. However, traditional data processing\nbased on exactly match often misses lots of valid information. To get\nhigh-quality analysis results and fit in the big data era, this thesis studies\nthe error-tolerant big data processing. As most of the data in real world can\nbe represented as a sequence or a set, this thesis utilizes the widely-used\nsequence-based and set-based similar functions to tolerate errors in data\nprocessing and studies the approximate entity extraction, similarity join and\nsimilarity search problems. The main contributions of this thesis include:\n  1. This thesis proposes a unified framework to support approximate entity\nextraction with both sequence-based and set-based similarity functions\nsimultaneously. The experiments show that the unified framework can improve the\nstate-of-the-art methods by 1 to 2 orders of magnitude.\n  2. This thesis designs two methods respectively for the sequence and the set\nsimilarity joins. For the sequence similarity join, this thesis proposes to\nevenly partition the sequences to segments. It is guaranteed that two sequences\nare similar only if one sequence has a subsequence identical to a segment of\nanother sequence. For the set similarity join, this thesis proposes to\npartition all the sets into segments based on the universe. This thesis further\nextends the two partition-based methods to support the large-scale data\nprocessing framework, Map-Reduce and Spark. The partition-based method won the\nstring similarity join competition held by EDBT and beat the second place by 10\ntimes.\n  3. This thesis proposes a pivotal prefix filter technique to solve the\nsequence similarity search problem. This thesis shows that the pivotal prefix\nfilter has stronger pruning power and less filtering cost compared to the\nstate-of-the-art filters.\n"]},
{"authors": ["Nandan Sudarsanam", "Nishanth Kumar", "Abhishek Sharma", "Balaraman Ravindran"], "title": ["Rate of Change Analysis for Interestingness Measures"], "date": ["2017-12-14T12:13:46Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1712.05193v1"], "summary": ["  The use of Association Rule Mining techniques in diverse contexts and domains\nhas resulted in the creation of numerous interestingness measures. This, in\nturn, has motivated researchers to come up with various classification schemes\nfor these measures. One popular approach to classify the objective measures is\nto assess the set of mathematical properties they satisfy in order to help\npractitioners select the right measure for a given problem. In this research,\nwe discuss the insufficiency of the existing properties in literature to\ncapture certain behaviors of interestingness measures. This motivates us to\npresent a novel approach to analyze and classify measures. We refer to this as\na rate of change analysis (RCA). In this analysis a measure is described by how\nit varies if there is a unit change in the frequency count\n$(f_{11},f_{10},f_{01},f_{00})$, for different pre-existing states of the\nfrequency counts. More formally, we look at the first partial derivative of the\nmeasure with respect to the various frequency count variables. We then use this\nanalysis to define two new properties, Unit-Null Asymptotic Invariance (UNAI)\nand Unit-Null Zero Rate (UNZR). UNAI looks at the asymptotic effect of adding\nfrequency patterns, while UNZR looks at the initial effect of adding frequency\npatterns when they do not pre-exist in the dataset. We present a comprehensive\nanalysis of 50 interestingness measures and classify them in accordance with\nthe two properties. We also present empirical studies, involving both synthetic\nand real-world datasets, which are used to cluster various measures according\nto the rule ranking patterns of the measures. The study concludes with the\nobservation that classification of measures using the empirical clusters share\nsignificant similarities to the classification of measures done through the\nproperties presented in this research.\n"]},
{"authors": ["Fei Bi", "Lijun Chang", "Xuemin Lin", "Wenjie Zhang"], "title": ["An Optimal and Progressive Approach to Online Search of Top-k\n  Influential Communities"], "date": ["2017-11-16T00:12:25Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1711.05857v2"], "summary": ["  Community search over large graphs is a fundamental problem in graph\nanalysis. Recent studies propose to compute top-k influential communities,\nwhere each reported community not only is a cohesive subgraph but also has a\nhigh influence value. The existing approaches to the problem of top-k\ninfluential community search can be categorized as index-based algorithms and\nonline search algorithms without indexes. The index-based algorithms, although\nbeing very efficient in conducting community searches, need to pre-compute a\nspecial-purpose index and only work for one built-in vertex weight vector. In\nthis paper, we investigate on-line search approaches and propose an\ninstance-optimal algorithm LocalSearch whose time complexity is linearly\nproportional to the size of the smallest subgraph that a correct algorithm\nneeds to access without indexes. In addition, we also propose techniques to\nmake LocalSearch progressively compute and report the communities in decreasing\ninfluence value order such that k does not need to be specified. Moreover, we\nextend our framework to the general case of top-k influential community search\nregarding other cohesiveness measures. Extensive empirical studies on real\ngraphs demonstrate that our algorithms outperform the existing online search\nalgorithms by several orders of magnitude.\n"]},
{"authors": ["Vadim Tropashko"], "title": ["Ideal Databases"], "date": ["2015-12-30T22:45:56Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1601.00524v2"], "summary": ["  From algebraic geometry perspective database relations are succinctly defined\nas Finite Varieties. After establishing basic framework, we give analytic proof\nof Heath theorem from Database Dependency theory. Next, we leverage\nAlgebra/Geometry dictionary and focus on algebraic counterparts of finite\nvarieties, polynomial ideals. It is well known that intersection and sum of\nideals are lattice operations. We generalize this fact to ideals from different\nrings, therefore establishing that algebra of ideals is Relational Lattice. The\nfinal stop is casting the framework into Linear Algebra, and traversing to\nQuantum Theory.\n"]},
{"authors": ["Adam Agocs", "Dimitrios Dardanis", "Jean-Marie Le Goff", "Dimitrios Proios"], "title": ["Interactive graph query language for multidimensional data in\n  Collaboration Spotting visual analytics framework"], "date": ["2017-12-12T10:12:43Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1712.04202v1"], "summary": ["  Human reasoning in visual analytics of data networks relies mainly on the\nquality of visual perception and the capability of interactively exploring the\ndata from different facets. Visual quality strongly depends on networks' size\nand dimensional complexity while network exploration capability on the\nintuitiveness and expressiveness of user frontends. The approach taken in this\npaper aims at addressing the above by decomposing data networks into multiple\nnetworks of smaller dimensions and building an interactive graph query language\nthat supports full navigation across the sub-networks. Within sub-networks of\nreduced dimensionality, structural abstraction and semantic techniques can then\nbe used to enhance visual perception further.\n"]},
{"authors": ["Niek Tax", "Marlon Dumas"], "title": ["Mining Non-Redundant Sets of Generalizing Patterns from Sequence\n  Databases"], "date": ["2017-12-12T08:03:50Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1712.04159v1"], "summary": ["  Sequential pattern mining techniques extract patterns corresponding to\nfrequent subsequences from a sequence database. A practical limitation of these\ntechniques is that they overload the user with too many patterns. Local Process\nModel (LPM) mining is an alternative approach coming from the field of process\nmining. While in traditional sequential pattern mining, a pattern describes one\nsubsequence, an LPM captures a set of subsequences. Also, while traditional\nsequential patterns only match subsequences that are observed in the sequence\ndatabase, an LPM may capture subsequences that are not explicitly observed, but\nthat are related to observed subsequences. In other words, LPMs generalize the\nbehavior observed in the sequence database. These properties make it possible\nfor a set of LPMs to cover the behavior of a much larger set of sequential\npatterns. Yet, existing LPM mining techniques still suffer from the pattern\nexplosion problem because they produce sets of redundant LPMs. In this paper,\nwe propose several heuristics to mine a set of non-redundant LPMs either from a\nset of redundant LPMs or from a set of sequential patterns. We empirically\ncompare the proposed heuristics between them and against existing (local)\nprocess mining techniques in terms of coverage, precision, and complexity of\nthe produced sets of LPMs.\n"]},
{"authors": ["Patrick Schultz", "Ryan Wisnesky"], "title": ["Algebraic Data Integration"], "date": ["2015-03-12T03:10:25Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1503.03571v7"], "summary": ["  In this paper we develop an algebraic approach to data integration by\ncombining techniques from functional programming, category theory, and database\ntheory. In our formalism, database schemas and instances are algebraic\n(multi-sorted equational) theories of a certain form. Schemas denote\ncategories, and instances denote their initial (term) algebras. The instances\non a schema S form a category, S-Inst, and a morphism of schemas F : S -> T\ninduces three adjoint data migration functors: Sigma_F : S-Inst -> T-Inst,\ndefined by substitution along F, which has a right adjoint Delta_F : T-Inst ->\nS-Inst, which in turn has a right adjoint Pi_F : S-Inst -> T-Inst. We present a\nquery language based on for/where/return syntax where each query denotes a\nsequence of data migration functors; a pushout-based design pattern for\nperforming data integration using our formalism; and describe the\nimplementation of our formalism in a tool we call AQL.\n"]},
{"authors": ["G\u00e1bor Sz\u00e1rnyas"], "title": ["Incremental View Maintenance for Property Graph Queries"], "date": ["2017-12-12T03:02:24Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1712.04108v1"], "summary": ["  This paper discusses the challenges of incremental view maintenance for\nproperty graph queries. We select a subset of property graph queries and\npresent an approach that uses nested relational algebra to allow incremental\nevaluation.\n"]},
{"authors": ["Dimitrios Sisiaridis", "Olivier Markowitch"], "title": ["Feature Extraction and Feature Selection: Reducing Data Complexity with\n  Apache Spark"], "date": ["2017-12-11T21:02:21Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1712.08618v1"], "summary": ["  Feature extraction and feature selection are the first tasks in\npre-processing of input logs in order to detect cyber security threats and\nattacks while utilizing machine learning. When it comes to the analysis of\nheterogeneous data derived from different sources, these tasks are found to be\ntime-consuming and difficult to be managed efficiently. In this paper, we\npresent an approach for handling feature extraction and feature selection for\nsecurity analytics of heterogeneous data derived from different network\nsensors. The approach is implemented in Apache Spark, using its python API,\nnamed pyspark.\n"]},
{"authors": ["Tim Kraska", "Alex Beutel", "Ed H. Chi", "Jeffrey Dean", "Neoklis Polyzotis"], "title": ["The Case for Learned Index Structures"], "date": ["2017-12-04T17:18:41Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1712.01208v2"], "summary": ["  Indexes are models: a B-Tree-Index can be seen as a model to map a key to the\nposition of a record within a sorted array, a Hash-Index as a model to map a\nkey to a position of a record within an unsorted array, and a BitMap-Index as a\nmodel to indicate if a data record exists or not. In this exploratory research\npaper, we start from this premise and posit that all existing index structures\ncan be replaced with other types of models, including deep-learning models,\nwhich we term learned indexes. The key idea is that a model can learn the sort\norder or structure of lookup keys and use this signal to effectively predict\nthe position or existence of records. We theoretically analyze under which\nconditions learned indexes outperform traditional index structures and describe\nthe main challenges in designing learned index structures. Our initial results\nshow, that by using neural nets we are able to outperform cache-optimized\nB-Trees by up to 70% in speed while saving an order-of-magnitude in memory over\nseveral real-world data sets. More importantly though, we believe that the idea\nof replacing core components of a data management system through learned models\nhas far reaching implications for future systems designs and that this work\njust provides a glimpse of what might be possible.\n"]},
{"authors": ["Sihem Amer-Yahia", "Behrooz Omidvar-Tehrani", "Joao Comba", "Viviane Moreira", "Fabian Colque Zegarra"], "title": ["Exploration of User Groups in VEXUS"], "date": ["2017-12-10T14:01:53Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1712.03529v1"], "summary": ["  We introduce VEXUS, an interactive visualization framework for exploring user\ndata to fulfill tasks such as finding a set of experts, forming discussion\ngroups and analyzing collective behaviors. User data is characterized by a\ncombination of demographics like age and occupation, and actions such as rating\na movie, writing a paper, following a medical treatment or buying groceries.\nThe ubiquity of user data requires tools that help explorers, be they\nspecialists or novice users, acquire new insights. VEXUS lets explorers\ninteract with user data via visual primitives and builds an exploration profile\nto recommend the next exploration steps. VEXUS combines state-of-the-art\nvisualization techniques with appropriate indexing of user data to provide fast\nand relevant exploration.\n"]},
{"authors": ["Rada Chirkova", "Jon Doyle", "Juan L. Reutter"], "title": ["Assessing Achievability of Queries and Constraints"], "date": ["2017-12-09T20:57:51Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1712.03438v1"], "summary": ["  Assessing and improving the quality of data in data-intensive systems are\nfundamental challenges that have given rise to numerous applications targeting\ntransformation and cleaning of data. However, while schema design, data\ncleaning, and data migration are nowadays reasonably well understood in\nisolation, not much attention has been given to the interplay between the tools\nthat address issues in these areas. Our focus is on the problem of determining\nwhether there exist sequences of data-transforming procedures that, when\napplied to the (untransformed) input data, would yield data satisfying the\nconditions required for performing the task in question. Our goal is to develop\na framework that would address this problem, starting with the relational\nsetting.\n  In this paper we abstract data-processing tools as black-box procedures. This\nabstraction describes procedures by a specification of which parts of the\ndatabase might be modified by the procedure, as well as by the constraints that\nspecify the required states of the database before and after applying the\nprocedure. We then proceed to study fundamental algorithmic questions arising\nin this context, such as understanding when one can guarantee that sequences of\nprocedures apply to original or transformed data, when they succeed at\nimproving the data, and when knowledge bases can represent the outcomes of\nprocedures. Finally, we turn to the problem of determining whether the\napplication of a sequence of procedures to a database results in the\nsatisfaction of properties specified by either queries or constraints. We show\nthat this problem is decidable for some broad and realistic classes of\nprocedures and properties, even when procedures are allowed to alter the schema\nof instances.\n"]},
{"authors": ["Mehdi Mohammadi", "Ala Al-Fuqaha", "Sameh Sorour", "Mohsen Guizani"], "title": ["Deep Learning for IoT Big Data and Streaming Analytics: A Survey"], "date": ["2017-12-09T06:13:43Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1712.04301v1"], "summary": ["  In the era of the Internet of Things (IoT), an enormous amount of sensing\ndevices collect and/or generate various sensory data over time for a wide range\nof fields and applications. Based on the nature of the application, these\ndevices will result in big or fast/real-time data streams. Applying analytics\nover such data streams to discover new information, predict future insights,\nand make control decisions is a crucial process that makes IoT a worthy\nparadigm for businesses and a quality-of-life improving technology. In this\npaper, we provide a thorough overview on using a class of advanced machine\nlearning techniques, namely Deep Learning (DL), to facilitate the analytics and\nlearning in the IoT domain. We start by articulating IoT data characteristics\nand identifying two major treatments for IoT data from a machine learning\nperspective, namely IoT big data analytics and IoT streaming data analytics. We\nalso discuss why DL is a promising approach to achieve the desired analytics in\nthese types of data and applications. The potential of using emerging DL\ntechniques for IoT data analytics are then discussed, and its promises and\nchallenges are introduced. We present a comprehensive background on different\nDL architectures and algorithms. We also analyze and summarize major reported\nresearch attempts that leveraged DL in the IoT domain. The smart IoT devices\nthat have incorporated DL in their intelligence background are also discussed.\nDL implementation approaches on the fog and cloud centers in support of IoT\napplications are also surveyed. Finally, we shed light on some challenges and\npotential directions for future research. At the end of each section, we\nhighlight the lessons learned based on our experiments and review of the recent\nliterature.\n"]},
{"authors": ["Xin Zhang"], "title": ["Code Generation Techniques for Raw Data Processing"], "date": ["2017-12-09T00:34:47Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1712.03320v1"], "summary": ["  The motivation of the current study was to design an algorithm that can speed\nup the processing of a query. The important feature is generating code\ndynamically for a specific query. We present the technique of code generation\nthat is applied to query processing on a raw file. The idea was to customize a\nquery program with a given query and generate a machine- and query-specific\nsource code. The generated code is compiled by GCC, Clang or any other C/C++\ncompiler, and the compiled file is dynamically linked to the main program for\nfurther processing. Code generation reduces the cost of generalizing query\nprocessing. It also avoids the overhead of the conventional interpretation\nduring achieve high performance. Database Management Systems (DBMSs) perform\nexcellent jobs in many aspects of big data, such as storage, indexing, and\nanalysis. DBMSs typically format entire data and load them into their storage\nlayer. They increase data-to-query time, which is the cost time it takes to\nconvert data into a specific schema and persist them in a disk. Ideally, DBMSs\nshould adapt to the input data and extract one/some of columns, not the entire\ndata, that is/are associated with a given query. Therefore, the query engine on\na raw file can reduce the cost of conventional general operators and avoid some\nunnecessary procedures, such as fully scanning, tokenizing and paring the whole\ndata. In the current study, we introduce our code-generation approach for\nin-situ processing of raw files, which is based on the template approach and\nthe hype approach. The approach minimizes the data-to-query time and achieves a\nhigh performance for query processing. There are some benefits from our work:\nreducing branches and instructions, unrolling loops, eliminating unnecessary\ndata type checks and optimizing the binary code with a compiler on a local\nmachine.\n"]},
{"authors": ["Ivens Portugal", "Paulo Alencar", "Donald Cowan"], "title": ["Developing a Spatial-Temporal Contextual and Semantic Trajectory\n  Clustering Framework"], "date": ["2017-12-08T16:29:48Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1712.03900v1"], "summary": ["  This paper reports on ongoing research investigating more expressive\napproaches to spatial-temporal trajectory clustering. Spatial-temporal data is\nincreasingly becoming universal as a result of widespread use of GPS and mobile\ndevices, which makes mining and predictive analyses based on trajectories a\ncritical activity in many domains. Trajectory analysis methods based on\nclustering techniques heavily often rely on a similarity definition to properly\nprovide insights. However, although trajectories are currently described in\nterms of its two dimensions (space and time), their representation is limited\nin that it is not expressive enough to capture, in a combined way, the\nstructure of space and time as well as the contextual and semantic trajectory\nproperties. Moreover, the massive amounts of available trajectory data make\ntrajectory mining and analyses very challenging. In this paper, we briefly\ndiscuss (i) an improved trajectory representation that takes into consideration\nspace-time structures, context and semantic properties of trajectories; (ii)\nnew forms of relations between the dimensions of a pair of trajectories; and\n(iii) big data approaches that can be used to develop a novel spatial-temporal\nclustering framework.\n"]},
{"authors": ["Fabien Andr\u00e9"], "title": ["Exploiting Modern Hardware for High-Dimensional Nearest Neighbor Search"], "date": ["2017-12-08T02:14:17Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1712.02912v1"], "summary": ["  Many multimedia information retrieval or machine learning problems require\nefficient high-dimensional nearest neighbor search techniques. For instance,\nmultimedia objects (images, music or videos) can be represented by\nhigh-dimensional feature vectors. Finding two similar multimedia objects then\ncomes down to finding two objects that have similar feature vectors. In the\ncurrent context of mass use of social networks, large scale multimedia\ndatabases or large scale machine learning applications are more and more\ncommon, calling for efficient nearest neighbor search approaches.\n  This thesis builds on product quantization, an efficient nearest neighbor\nsearch technique that compresses high-dimensional vectors into short codes.\nThis makes it possible to store very large databases entirely in RAM, enabling\nlow response times. We propose several contributions that exploit the\ncapabilities of modern CPUs, especially SIMD and the cache hierarchy, to\nfurther decrease response times offered by product quantization.\n"]},
{"authors": ["Brad Carlile", "Akiko Marti", "Guy Delamarter"], "title": ["Columnar Database Techniques for Creating AI Features"], "date": ["2017-12-07T22:53:41Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1712.02882v1"], "summary": ["  Recent advances with in-memory columnar database techniques have increased\nthe performance of analytical queries on very large databases and data\nwarehouses. At the same time, advances in artificial intelligence (AI)\nalgorithms have increased the ability to analyze data. We use the term AI to\nencompass both Deep Learning (DL or neural network) and Machine Learning (ML\naka Big Data analytics). Our exploration of the AI full stack has led us to a\ncross-stack columnar database innovation that efficiently creates features for\nAI analytics. The innovation is to create Augmented Dictionary Values (ADVs) to\nadd to existing columnar database dictionaries in order to increase the\nefficiency of featurization by minimizing data movement and data duplication.\nWe show how various forms of featurization (feature selection, feature\nextraction, and feature creation) can be efficiently calculated in a columnar\ndatabase. The full stack AI investigation has also led us to propose an\nintegrated columnar database and AI architecture. This architecture has\ninformation flows and feedback loops to improve the whole analytics cycle\nduring multiple iterations of extracting data from the data sources,\nfeaturization, and analysis.\n"]},
{"authors": ["Harold Boley", "Gen Zou"], "title": ["Perspectival Knowledge in PSOA RuleML: Representation, Model Theory, and\n  Translation"], "date": ["2017-12-07T21:36:21Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1712.02869v1"], "summary": ["  In Positional-Slotted Object-Applicative (PSOA) RuleML, a predicate\napplication (atom) can have an Object IDentifier (OID) and descriptors that may\nbe positional arguments (tuples) or attribute-value pairs (slots). PSOA RuleML\n1.0 specifies for each descriptor whether it is to be interpreted under the\nperspective of the predicate in whose scope it occurs. This perspectivity\ndimension refines the space between oidless, positional atoms (relationships)\nand oidful, slotted atoms (frames): While relationships use only a\npredicate-scope-sensitive (predicate-dependent) tuple and frames use only\npredicate-scope-insensitive (predicate-independent) slots, PSOA RuleML 1.0 uses\na systematics of orthogonal constructs also permitting atoms with\n(predicate-)independent tuples and atoms with (predicate-)dependent slots. This\nsupports data and knowledge representation where a slot attribute can have\ndifferent values depending on the predicate. PSOA thus extends object-oriented\nmulti-membership and multiple inheritance. Based on objectification, PSOA laws\nare given: Besides unscoping and centralization, the semantic restriction and\ntransformation of describution permits rescoping of one atom's independent\ndescriptors to another atom with the same OID but a different predicate. For\ninheritance, default descriptors are realized by rules. On top of a metamodel\nand a Grailog visualization, PSOA's atom systematics for facts, queries, and\nrules is explained. The presentation and (XML-)serialization syntaxes of PSOA\nRuleML 1.0 are introduced. Its model-theoretic semantics is formalized by\nextending the earlier interpretation functions for dependent descriptors. The\nopen-source PSOATransRun 1.3 system realizes PSOA RuleML 1.0 by a translator to\nruntime predicates, including for dependent tuples (prdtupterm) and slots\n(prdsloterm). Our tests show efficiency advantages of dependent and tupled\nmodeling.\n"]},
{"authors": ["Andrew Ilyas", "Joana M. F. da Trindade", "Raul Castro Fernandez", "Samuel Madden"], "title": ["Extracting Syntactic Patterns from Databases"], "date": ["2017-10-31T15:21:32Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.11528v2"], "summary": ["  Many database columns contain string or numerical data that conforms to a\npattern, such as phone numbers, dates, addresses, product identifiers, and\nemployee ids. These patterns are useful in a number of data processing\napplications, including understanding what a specific field represents when\nfield names are ambiguous, identifying outlier values, and finding similar\nfields across data sets. One way to express such patterns would be to learn\nregular expressions for each field in the database. Unfortunately, exist- ing\ntechniques on regular expression learning are slow, taking hundreds of seconds\nfor columns of just a few thousand values. In contrast, we develop XSystem, an\nefficient method to learn patterns over database columns in significantly less\ntime. We show that these patterns can not only be built quickly, but are\nexpressive enough to capture a number of key applications, including detecting\noutliers, measuring column similarity, and assigning semantic labels to columns\n(based on a library of regular expressions). We evaluate these applications\nwith datasets that range from chemical databases (based on a collaboration with\na pharmaceutical company), our university data warehouse, and open data from\nMassData.gov.\n"]},
{"authors": ["Renzo Angles", "Marcelo Arenas", "Pablo Barcel\u00f3", "Peter Boncz", "George H. L. Fletcher", "Claudio Gutierrez", "Tobias Lindaaker", "Marcus Paradies", "Stefan Plantikow", "Juan Sequeda", "Oskar van Rest", "Hannes Voigt"], "title": ["G-CORE: A Core for Future Graph Query Languages"], "date": ["2017-12-05T09:59:47Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1712.01550v2"], "summary": ["  We report on a community effort between industry and academia to shape the\nfuture of graph query languages. We argue that existing graph database\nmanagement systems should consider supporting a query language with two key\ncharacteristics. First, it should be composable, meaning, that graphs are the\ninput and the output of queries. Second, the graph query language should treat\npaths as first-class citizens. Our result is G-CORE, a powerful graph query\nlanguage design that fulfills these goals, and strikes a careful balance\nbetween path query expressivity and evaluation complexity.\n"]},
{"authors": ["Yaron Gonen"], "title": ["Analyzing Large-Scale, Distributed and Uncertain Data"], "date": ["2017-12-05T18:50:07Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1712.01817v1"], "summary": ["  The exponential growth of data in current times and the demand to gain\ninformation and knowledge from the data present new challenges for database\nresearchers. Known database systems and algorithms are no longer capable of\neffectively handling such large data sets. MapReduce is a novel programming\nparadigm for processing distributable problems over large-scale data using a\ncomputer cluster.\n  In this work we explore the MapReduce paradigm from three different angles.\nWe begin by examining a well-known problem in the field of data mining: mining\nclosed frequent itemsets over a large dataset. By harnessing the power of\nMapReduce, we present a novel algorithm for mining closed frequent itemsets\nthat outperforms existing algorithms.\n  Next, we explore one of the fundamental implications of \"Big Data\": The data\nis not known with complete certainty. A probabilistic database is a relational\ndatabase with the addendum that each tuple is associated with a probability of\nits existence. A natural development of MapReduce is of a distributed\nrelational database management system, where relational calculus has been\nreduced to a combination of map and reduce function. We take this development a\nstep further by proposing a query optimizer over distributed, probabilistic\ndatabase.\n  Finally, we analyze the best known implementation of MapReduce called Hadoop,\naiming to overcome one of its major drawbacks: it does not directly support the\nexplicit specification of the data repeatedly processed throughout different\njobs.Many data-mining algorithms, such as clustering and association-rules\nrequire iterative computation: the same data are processed again and again\nuntil the computation converges or a stopping condition is satisfied. We\npropose a modification to Hadoop such that it will support efficient access to\nthe same data in different jobs.\n"]},
{"authors": ["Karthik Ramachandra", "Kwanghyun Park", "K. Venkatesh Emani", "Alan Halverson", "Cesar Galindo-Legaria", "Conor Cunningham"], "title": ["Optimization of Imperative Programs in a Relational Database"], "date": ["2017-12-01T21:24:25Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1712.00498v2"], "summary": ["  For decades, RDBMSs have supported declarative SQL as well as imperative\nfunctions and procedures as ways for users to express data processing tasks.\nWhile the evaluation of declarative SQL has received a lot of attention\nresulting in highly sophisticated techniques, the evaluation of imperative\nprograms has remained naive and highly inefficient. Imperative programs offer\nseveral benefits over SQL and hence are often preferred and widely used. But\nunfortunately, their abysmal performance discourages, and even prohibits their\nuse in many situations. We address this important problem that has hitherto\nreceived little attention.\n  We present Froid, an extensible framework for optimizing imperative programs\nin relational databases. Froid's novel approach automatically transforms entire\nUser Defined Functions (UDFs) into relational algebraic expressions, and embeds\nthem into the calling SQL query. This form is now amenable to cost-based\noptimization and results in efficient, set-oriented, parallel plans as opposed\nto inefficient, iterative, serial execution of UDFs. Froid's approach\nadditionally brings the benefits of many compiler optimizations to UDFs with no\nadditional implementation effort. We describe the design of Froid and present\nour experimental evaluation that demonstrates performance improvements of up to\nmultiple orders of magnitude on real workloads.\n"]},
{"authors": ["Athanasios N. Nikolakopoulos", "Vassilis Kalantzis", "Efstratios Gallopoulos", "John D. Garofalakis"], "title": ["EigenRec: Generalizing PureSVD for Effective and Efficient Top-N\n  Recommendations"], "date": ["2015-11-19T00:34:51Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1511.06033v3"], "summary": ["  We introduce EigenRec; a versatile and efficient Latent-Factor framework for\nTop-N Recommendations that includes the well-known PureSVD algorithm as a\nspecial case. EigenRec builds a low dimensional model of an inter-item\nproximity matrix that combines a similarity component, with a scaling operator,\ndesigned to control the influence of the prior item popularity on the final\nmodel. Seeing PureSVD within our framework provides intuition about its inner\nworkings, exposes its inherent limitations, and also, paves the path towards\npainlessly improving its recommendation performance. A comprehensive set of\nexperiments on the MovieLens and the Yahoo datasets based on widely applied\nperformance metrics, indicate that EigenRec outperforms several\nstate-of-the-art algorithms, in terms of Standard and Long-Tail recommendation\naccuracy, exhibiting low susceptibility to sparsity, even in its most extreme\nmanifestations -- the Cold-Start problems. At the same time EigenRec has an\nattractive computational profile and it can apply readily in large-scale\nrecommendation settings.\n"]},
{"authors": ["Alejandro Grez", "Cristian Riveros", "Martin Ugarte", "Stijn Vansummeren"], "title": ["A Second-Order Approach to Complex Event Recognition"], "date": ["2017-12-04T13:33:37Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1712.01063v1"], "summary": ["  Complex Event Recognition (CER for short) refers to the activity of detecting\npatterns in streams of continuously arriving data. This field has been\ntraditionally approached from a practical point of view, resulting in\nheterogeneous implementations with fundamentally different capabilities. The\nmain reason behind this is that defining formal semantics for a CER language is\nnot trivial: they usually combine first-order variables for joining and\nfiltering events with regular operators like sequencing and Kleene closure.\nMoreover, their semantics usually focus only on the detection of complex\nevents, leaving the concept of output mostly unattended.\n  In this paper, we propose to unify the semantics and output of complex event\nrecognition languages by using second order objects. Specifically, we introduce\na CER language called Second Order Complex Event Logic (SO-CEL for short), that\nuses second order variables for managing and outputting sequences of events.\nThis makes the definition of the main CER operators simple, allowing us to\ndevelop the first steps in understanding its expressive power. We start by\ncomparing SO-CEL with a version that uses first-order variables called FO-CEL,\nshowing that they are equivalent in expressive power when restricted to unary\npredicates but, surprisingly, incomparable in general. Nevertheless, we show\nthat if we restrict to sets of binary predicates, then SO-CEL is strictly more\nexpressive than FO-CEL. Then, we introduce a natural computational model called\nUnary Complex Event Automata (UCEA) that provides a better understanding of\nSO-CEL. We show that, under unary predicates, SO-CEL captures the subclass of\nUCEA that satisfy the so-called *-property. Finally, we identify the operations\nthat SO-CEL is lacking to capture UCEA and introduce a natural extension of the\nlanguage that captures the complete class of UCEA under unary predicates.\n"]},
{"authors": ["Hayden Jananthan", "Ziqi Zhou", "Vijay Gadepally", "Dylan Hutchison", "Suna Kim", "Jeremy Kepner"], "title": ["Polystore Mathematics of Relational Algebra"], "date": ["2017-12-03T17:25:03Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1712.00802v1"], "summary": ["  Financial transactions, internet search, and data analysis are all placing\nincreasing demands on databases. SQL, NoSQL, and NewSQL databases have been\ndeveloped to meet these demands and each offers unique benefits. SQL, NoSQL,\nand NewSQL databases also rely on different underlying mathematical models.\nPolystores seek to provide a mechanism to allow applications to transparently\nachieve the benefits of diverse databases while insulating applications from\nthe details of these databases. Integrating the underlying mathematics of these\ndiverse databases can be an important enabler for polystores as it enables\neffective reasoning across different databases. Associative arrays provide a\ncommon approach for the mathematics of polystores by encompassing the\nmathematics found in different databases: sets (SQL), graphs (NoSQL), and\nmatrices (NewSQL). Prior work presented the SQL relational model in terms of\nassociative arrays and identified key mathematical properties that are\npreserved within SQL. This work provides the rigorous mathematical definitions,\nlemmas, and theorems underlying these properties. Specifically, SQL Relational\nAlgebra deals primarily with relations - multisets of tuples - and operations\non and between these relations. These relations can be modeled as associative\narrays by treating tuples as non-zero rows in an array. Operations in\nrelational algebra are built as compositions of standard operations on\nassociative arrays which mirror their matrix counterparts. These constructions\nprovide insight into how relational algebra can be handled via array\noperations. As an example application, the composition of two projection\noperations is shown to also be a projection, and the projection of a union is\nshown to be equal to the union of the projections.\n"]},
{"authors": ["Zhaoyang Shao", "Davood Rafiei", "Themis Palpanas"], "title": ["Efficient Error-tolerant Search on Knowledge Graphs"], "date": ["2016-09-10T22:08:51Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1609.03095v3"], "summary": ["  Edge-labeled graphs are widely used to describe relationships between\nentities in a database. Given a query subgraph that represents an example of\nwhat the user is searching for, we study the problem of efficiently searching\nfor similar subgraphs in a large data graph, where the similarity is defined in\nterms of the well-known graph edit distance. We call these queries\n\"error-tolerant exemplar queries\" since matches are allowed despite small\nvariations in the graph structure and the labels. The problem in its general\ncase is computationally intractable, but efficient solutions are reachable for\nlabeled graphs under well-behaved distribution of the labels, commonly found in\nknowledge graphs. We propose two efficient exact algorithms, based on a\nfiltering-and-verification framework, for finding subgraphs in a large data\ngraph that are isomorphic to a query graph under some edit operations. Our\nfiltering scheme, which uses the neighbourhood structure around a node and the\npresence or absence of paths, significantly reduces the number of candidates\nthat are passed to the verification stage. Moreover, we analyze the costs of\nour algorithms and the conditions under which one algorithm is expected to\noutperform the other. Our analysis identifies some of the variables that affect\nthe cost, including the number and the selectivity of query edge labels and the\ndegree of nodes in the data graph, and characterizes their relationships. We\nempirically evaluate the effectiveness of our filtering schemes and queries,\nthe efficiency of our algorithms and the reliability of our cost models on real\ndatasets.\n"]},
{"authors": ["Rong Kang", "Chen Wang", "Peng Wang", "Yuting Ding", "Jianmin Wang"], "title": ["Fine-grained Pattern Matching Over Streaming Time Series"], "date": ["2017-10-27T11:45:14Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.10088v3"], "summary": ["  Pattern matching of streaming time series with lower latency under limited\ncomputing resource comes to a critical problem, especially as the growth of\nIndustry 4.0 and Industry Internet of Things. However, against traditional\nsingle pattern matching problem, a pattern may contain multiple segments\nrepresenting different statistical properties or physical meanings for more\nprecise and expressive matching in real world. Hence, we formulate a new\nproblem, called \"fine-grained pattern matching\", which allows users to specify\nvaried granularities of matching deviation to different segments of a given\npattern, and fuzzy regions for adaptive breakpoints determination between\nconsecutive segments. In this paper, we propose a novel two-phase approach. In\nthe pruning phase, we introduce Equal-Length Block (ELB) representation\ntogether with Block-Skipping Pruning (BSP) policy, which guarantees low cost\nfeature calculation, effective pruning and no false dismissals. In the\npost-processing phase, a delta-function is proposed to enable us to conduct\nexact matching in linear complexity. Extensive experiments are conducted to\nevaluate on synthetic and real-world datasets, which illustrates that our\nalgorithm outperforms the brute-force method and MSM, a multi-step filter\nmechanism over the multi-scaled representation.\n"]},
{"authors": ["Haoci Zhang", "Thibault Sellam", "Eugene Wu"], "title": ["Mining Precision Interfaces From Query Logs"], "date": ["2017-11-30T21:03:24Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1712.00078v1"], "summary": ["  Interactive tools make data analysis both more efficient and more accessible\nto a broad population. Simple interfaces such as Google Finance as well as\ncomplex visual exploration interfaces such as Tableau are effective because\nthey are tailored to the desired user tasks. Yet, designing interactive\ninterfaces requires technical expertise and domain knowledge. Experts are\nscarce and expensive, and therefore it is currently infeasible to provide\ntailored (or precise) interfaces for every user and every task.\n  We envision a data-driven approach to generate tailored interactive\ninterfaces. We observe that interactive interfaces are designed to express sets\nof programs; thus, samples of programs-increasingly collected by data\nsystems-may help us build interactive interfaces. Based on this idea, Precision\nInterfaces is a language-agnostic system that examines an input query log,\nidentifies how the queries structurally change, and generates interactive web\ninterfaces to express these changes. The focus of this paper is on applying\nthis idea towards logs of structured queries. Our experiments show that\nPrecision Interfaces can support multiple query languages (SQL and SPARQL),\nderive Tableau's salient interaction components from OLAP queries, analyze <75k\nqueries in <12 minutes, and generate interaction designs that improve upon\nexisting interfaces and are comparable to human-crafted interfaces.\n"]},
{"authors": ["Jiawei Zhang", "Limeng Cui", "Yanjie Fu"], "title": ["LATTE: Application Oriented Network Embedding"], "date": ["2017-11-30T15:44:14Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1711.11466v1"], "summary": ["  In recent years, many research works propose to embed the networked data into\na low-dimensional feature space. With the embedding feature vectors, the\noriginal network structure can be effectively reconstructed, classic learning\nalgorithms can be applied directly, and more importantly the learned embedding\nrepresentations can also be widely used in external applications. However, due\nto the detachment of embedding process with external tasks, the learned\nembedding results by most existing embedding models can be ine ective for\napplication tasks with speci c objectives, e.g., community detection vs\ninformation di usion. In addition, the networked data has become more and more\ncomplicated nowadays, which can involve both heterogeneous structures and\ndiverse a ributes, and few existing homogeneous network embedding models can\nhandle them well. In this paper, we will study the application oriented\nheterogeneous network embedding problem. Signi cantly di erent from the\nexisting works, besides the network structure preservation, the problem should\nalso incorporate the objectives of external application in the objective\nfunction. To resolve the problem, in this paper, we propose a novel network\nembedding framework, namely the \"appLicAtion orienTed neTwork Embedding\"\n(Latte) model. In Latte, we introduce a new concept called \"attributed\nheterogeneous social network\" to model the diverse structure and attribute\ninformation available in the networks. Meanwhile, the heterogeneous network\nstructure can be applied to compute the node \"diffusive proximity\" score, which\ncapture both local and global network structures. Furthermore, Latte learns the\nnetwork representation feature vectors by extending the autoencoder model model\nto the heterogeneous network scenario, which can also effectively unite the\nobjectives of network embedding and external application tasks.\n"]},
{"authors": ["Antonia Korba"], "title": ["HSC: A Novel Method for Clustering Hierarchies of Networked Data"], "date": ["2017-11-29T19:29:16Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1711.11071v1"], "summary": ["  Hierarchical clustering is one of the most powerful solutions to the problem\nof clustering, on the grounds that it performs a multi scale organization of\nthe data. In recent years, research on hierarchical clustering methods has\nattracted considerable interest due to the demanding modern application\ndomains.\n  We present a novel divisive hierarchical clustering framework called\nHierarchical Stochastic Clustering (HSC), that acts in two stages. In the first\nstage, it finds a primary hierarchy of clustering partitions in a dataset. In\nthe second stage, feeds a clustering algorithm with each one of the clusters of\nthe very detailed partition, in order to settle the final result. The output is\na hierarchy of clusters. Our method is based on the previous research of Meyer\nand Weissel Stochastic Data Clustering and the theory of Simon and Ando on\nVariable Aggregation.\n  Our experiments show that our framework builds a meaningful hierarchy of\nclusters and benefits consistently the clustering algorithm that acts in the\nsecond stage, not only computationally but also in terms of cluster quality.\nThis result suggest that HSC framework is ideal for obtaining hierarchical\nsolutions of large volumes of data.\n"]},
{"authors": ["Koninika Pal", "Sebastian Michel"], "title": ["Learning Interesting Categorical Attributes for Refined Data Exploration"], "date": ["2017-11-29T16:14:52Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1711.10933v1"], "summary": ["  This work proposes and evaluates a novel approach to determine interesting\ncategorical attributes for lists of entities. Once identified, such categories\nare of immense value to allow constraining (filtering) a current view of a user\nto subsets of entities. We show how a classifier is trained that is able to\ntell whether or not a categorical attribute can act as a constraint, in the\nsense of human-perceived interestingness. The training data is harnessed from\nWeb tables, treating the presence or absence of a table as an indication that\nthe attribute used as a filter constraint is reasonable or not. For learning\nthe classification model, we review four well-known statistical measures\n(features) for categorical attributes---entropy, unalikeability, peculiarity,\nand coverage. We additionally propose three new statistical measures to capture\nthe distribution of data, tailored to our main objective. The learned model is\nevaluated by relevance assessments obtained through a user study, reflecting\nthe applicability of the approach as a whole and, further, demonstrates the\nsuperiority of the proposed diversity measures over existing statistical\nmeasures like information entropy.\n"]},
{"authors": ["Ning Gao", "Zhang Liu", "Dirk Grunwald"], "title": ["DTranx: A SEDA-based Distributed and Transactional Key Value Store with\n  Persistent Memory Log"], "date": ["2017-11-27T05:38:10Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1711.09543v1"], "summary": ["  Current distributed key value stores achieve scalability by trading off\nconsistency. As persistent memory technologies evolve tremendously, it is not\nnecessary to sacrifice consistency for performance. This paper proposes DTranx,\na distributed key value store based on a persistent memory aware log. DTranx\nintegrates a state transition based garbage collection mechanism in the log\ndesign to effectively and efficiently reclaim old logs. In addition, DTranx\nadopts the SEDA architecture to exploit higher concurrency in multi-core\nenvironments and employs the optimal core binding strategy to minimize context\nswitch overhead. Moreover, we customize a hybrid commit protocol that combines\noptimistic concurrency control and two-phase commit to reduce critical section\nof distributed locking and introduce a locking mechanism to avoid deadlocks and\nlivelocks.\n  In our evaluations, DTranx reaches 514.11k transactions per second with 36\nservers and 95\\% read workloads. The persistent memory aware log is 30 times\nfaster than the SSD based system. And, our state transition based garbage\ncollection mechanism is efficient and effective. It does not affect normal\ntransactions and log space usage is steadily low.\n"]},
{"authors": ["Jiawei Zhang", "Limeng Cui", "Philip S. Yu", "Yuanhua Lv"], "title": ["BL-ECD: Broad Learning based Enterprise Community Detection via\n  Hierarchical Structure Fusion"], "date": ["2017-11-26T15:56:06Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1711.09411v1"], "summary": ["  Employees in companies can be divided into di erent communities, and those\nwho frequently socialize with each other will be treated as close friends and\nare grouped in the same community. In the enterprise context, a large amount of\ninformation about the employees is available in both (1) o ine company internal\nsources and (2) online enterprise social networks (ESNs). Each of the\ninformation sources also contain multiple categories of employees'\nsocialization activities at the same time. In this paper, we propose to detect\nthe social communities of the employees in companies based on the broad\nlearning se ing with both these online and o ine information sources\nsimultaneously, and the problem is formally called the \"Broad Learning based\nEnterprise Community Detection\" (BL-Ecd) problem. To address the problem, a\nnovel broad learning based community detection framework named \"HeterogeneoUs\nMulti-sOurce ClusteRing\" (Humor) is introduced in this paper. Based on the\nvarious enterprise social intimacy measures introduced in this paper, Humor\ndetects a set of micro community structures of the employees based on each of\nthe socialization activities respectively. To obtain the (globally) consistent\ncommunity structure of employees in the company, Humor further fuses these\nmicro community structures via two broad learning phases: (1) intra-fusion of\nmicro community structures to obtain the online and o ine (locally) consistent\ncommunities respectively, and (2) inter-fusion of the online and o ine\ncommunities to achieve the (globally) consistent community structure of\nemployees. Extensive experiments conducted on real-world enterprise datasets\ndemonstrate our method can perform very well in addressing the BL-Ecd problem.\n"]},
{"authors": ["Jiawei Zhang", "Congying Xia", "Chenwei Zhang", "Limeng Cui", "Yanjie Fu", "Philip S. Yu"], "title": ["BL-MNE: Emerging Heterogeneous Social Network Embedding through Broad\n  Learning with Aligned Autoencoder"], "date": ["2017-11-26T15:44:49Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1711.09409v1"], "summary": ["  Network embedding aims at projecting the network data into a low-dimensional\nfeature space, where the nodes are represented as a unique feature vector and\nnetwork structure can be effectively preserved. In recent years, more and more\nonline application service sites can be represented as massive and complex\nnetworks, which are extremely challenging for traditional machine learning\nalgorithms to deal with. Effective embedding of the complex network data into\nlow-dimension feature representation can both save data storage space and\nenable traditional machine learning algorithms applicable to handle the network\ndata. Network embedding performance will degrade greatly if the networks are of\na sparse structure, like the emerging networks with few connections. In this\npaper, we propose to learn the embedding representation for a target emerging\nnetwork based on the broad learning setting, where the emerging network is\naligned with other external mature networks at the same time. To solve the\nproblem, a new embedding framework, namely \"Deep alIgned autoencoder based\neMbEdding\" (DIME), is introduced in this paper. DIME handles the diverse link\nand attribute in a unified analytic based on broad learning, and introduces the\nmultiple aligned attributed heterogeneous social network concept to model the\nnetwork structure. A set of meta paths are introduced in the paper, which\ndefine various kinds of connections among users via the heterogeneous link and\nattribute information. The closeness among users in the networks are defined as\nthe meta proximity scores, which will be fed into DIME to learn the embedding\nvectors of users in the emerging network. Extensive experiments have been done\non real-world aligned social networks, which have demonstrated the\neffectiveness of DIME in learning the emerging network embedding vectors.\n"]},
{"authors": ["Anand Gupta", "Hardeo Thakur", "Ritvik Shrivastava", "Pulkit Kumar", "Sreyashi Nag"], "title": ["A Big Data Analysis Framework Using Apache Spark and Deep Learning"], "date": ["2017-11-25T20:11:41Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1711.09279v1"], "summary": ["  With the spreading prevalence of Big Data, many advances have recently been\nmade in this field. Frameworks such as Apache Hadoop and Apache Spark have\ngained a lot of traction over the past decades and have become massively\npopular, especially in industries. It is becoming increasingly evident that\neffective big data analysis is key to solving artificial intelligence problems.\nThus, a multi-algorithm library was implemented in the Spark framework, called\nMLlib. While this library supports multiple machine learning algorithms, there\nis still scope to use the Spark setup efficiently for highly time-intensive and\ncomputationally expensive procedures like deep learning. In this paper, we\npropose a novel framework that combines the distributive computational\nabilities of Apache Spark and the advanced machine learning architecture of a\ndeep multi-layer perceptron (MLP), using the popular concept of Cascade\nLearning. We conduct empirical analysis of our framework on two real world\ndatasets. The results are encouraging and corroborate our proposed framework,\nin turn proving that it is an improvement over traditional big data analysis\nmethods that use either Spark or Deep learning as individual elements.\n"]},
{"authors": ["Lijun Chang"], "title": ["A Near-optimal Algorithm for Edge Connectivity-based Hierarchical Graph\n  Decomposition"], "date": ["2017-11-25T04:43:12Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1711.09189v1"], "summary": ["  Driven by many applications in graph analytics, the problem of computing\n$k$-edge connected components ($k$-ECCs) of a graph $G$ for a user-given $k$\nhas been extensively studied recently. In this paper, we investigate the\nproblem of constructing the hierarchy of edge connectivity-based graph\ndecomposition, which compactly represents the $k$-ECCs of a graph for all\npossible $k$ values. This is based on the fact that each $k$-ECC is entirely\ncontained in a $(k-1)$-ECC. In contrast to the existing approaches that conduct\nthe computation either in a bottom-up or a top-down manner, we propose a binary\nsearch-based framework which invokes a $k$-ECC computation algorithm as a black\nbox. Let $T_{kecc}(G)$ be the time complexity of computing all $k$-ECCs of $G$\nfor a specific $k$ value. We prove that the time complexity of our framework is\n${\\cal O}\\big( (\\log \\delta(G))\\times T_{kecc}(G)\\big)$, where $\\delta(G)$ is\nthe degeneracy of $G$ and equals the maximum value among the minimum vertex\ndegrees of all subgraphs of $G$. As $\\delta(G)$ is typically small for\nreal-world graphs, this time complexity is optimal up to a logarithmic factor.\n"]},
{"authors": ["Quoc-Cuong To", "Juan Soto", "Volker Markl"], "title": ["A Survey of State Management in Big Data Processing Systems"], "date": ["2017-02-06T12:41:28Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.01596v3"], "summary": ["  State management and its use in diverse applications varies widely across big\ndata processing systems. This is evident in both the research literature and\nexisting systems, such as Apache Flink, Apache Samza, Apache Spark, and Apache\nStorm. Given the pivotal role that state management plays in various use cases,\nin this survey, we present some of the most important uses of state as an\nenabler, discuss the alternative approaches used to handle and implement state,\npropose a taxonomy to capture the many facets of state management, and\nhighlight new research directions. Our aim is to provide insight into disparate\nstate management techniques, motivate others to pursue research in this area,\nand draw attention to some open problems.\n"]},
{"authors": ["Oleg Ivanov", "Sergey Bartunov"], "title": ["Adaptive Cardinality Estimation"], "date": ["2017-11-22T15:20:19Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1711.08330v1"], "summary": ["  In this paper we address cardinality estimation problem which is an important\nsubproblem in query optimization. Query optimization is a part of every\nrelational DBMS responsible for finding the best way of the execution for the\ngiven query. These ways are called plans. The execution time of different plans\nmay differ by several orders, so query optimizer has a great influence on the\nwhole DBMS performance. We consider cost-based query optimization approach as\nthe most popular one. It was observed that cost-based optimization quality\ndepends much on cardinality estimation quality. Cardinality of the plan node is\nthe number of tuples returned by it.\n  In the paper we propose a novel cardinality estimation approach with the use\nof machine learning methods. The main point of the approach is using query\nexecution statistics of the previously executed queries to improve cardinality\nestimations. We called this approach adaptive cardinality estimation to reflect\nthis point. The approach is general, flexible, and easy to implement. The\nexperimental evaluation shows that this approach significantly increases the\nquality of cardinality estimation, and therefore increases the DBMS performance\nfor some queries by several times or even by several dozens of times.\n"]},
{"authors": ["Madhulika Mohanty", "Maya Ramanath", "Mohamed Yahya", "Gerhard Weikum"], "title": ["Spec-QP: Speculative Query Planning for Joins over Knowledge Graphs"], "date": ["2017-11-20T23:47:07Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1711.07581v1"], "summary": ["  Organisations store huge amounts of data from multiple heterogeneous sources\nin the form of Knowledge Graphs (KGs). One of the ways to query these KGs is to\nuse SPARQL queries over a database engine. Since SPARQL follows exact match\nsemantics, the queries may return too few or no results. Recent works have\nproposed query relaxation where the query engine judiciously replaces a query\npredicate with similar predicates using weighted relaxation rules mined from\nthe KG. The space of possible relaxations is potentially too large to fully\nexplore and users are typically interested in only top-k results, so such query\nengines use top-k algorithms for query processing. However, they may still\nprocess all the relaxations, many of whose answers do not contribute towards\ntop-k answers. This leads to computation overheads and delayed response times.\n  We propose Spec-QP, a query planning framework that speculatively determines\nwhich relaxations will have their results in the top-k answers. Only these\nrelaxations are processed using the top-k operators. We, therefore, reduce the\ncomputation overheads and achieve faster response times without adversely\naffecting the quality of results. We tested Spec-QP over two datasets - XKG and\nTwitter, to demonstrate the efficiency of our planning framework at reducing\nruntimes with reasonable accuracy for query engines supporting relaxations.\n"]},
{"authors": ["Pan Hu", "Boris Motik", "Ian Horrocks"], "title": ["Optimised Maintenance of Datalog Materialisations"], "date": ["2017-11-10T19:10:20Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1711.03987v2"], "summary": ["  To efficiently answer queries, datalog systems often materialise all\nconsequences of a datalog program, so the materialisation must be updated\nwhenever the input facts change. Several solutions to the materialisation\nupdate problem have been proposed. The Delete/Rederive (DRed) and the\nBackward/Forward (B/F) algorithms solve this problem for general datalog, but\nboth contain steps that evaluate rules 'backwards' by matching their heads to a\nfact and evaluating the partially instantiated rule bodies as queries. We show\nthat this can be a considerable source of overhead even on very small updates.\nIn contrast, the Counting algorithm does not evaluate the rules 'backwards',\nbut it can handle only nonrecursive rules. We present two hybrid approaches\nthat combine DRed and B/F with Counting so as to reduce or even eliminate\n'backward' rule evaluation while still handling arbitrary datalog programs. We\nshow empirically that our hybrid algorithms are usually significantly faster\nthan existing approaches, sometimes by orders of magnitude.\n"]},
{"authors": ["Edans F. O. Sandes", "George Teodoro", "Alba C. M. A. Melo"], "title": ["Bitmap Filter: Speeding up Exact Set Similarity Joins with Bitwise\n  Operations"], "date": ["2017-11-20T13:06:10Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1711.07295v1"], "summary": ["  The Exact Set Similarity Join problem aims to find all similar sets between\ntwo collections of sets, with respect to a threshold and a similarity function\nsuch as overlap, Jaccard, dice or cosine. The naive approach verifies all pairs\nof sets and it is often considered impractical due the high number of\ncombinations. So, Exact Set Similarity Join algorithms are usually based on the\nFilter-Verification Framework, that applies a series of filters to reduce the\nnumber of verified pairs. This paper presents a new filtering technique called\nBitmap Filter, which is able to accelerate state-of-the-art algorithms for the\nexact Set Similarity Join problem. The Bitmap Filter uses hash functions to\ncreate bitmaps of fixed b bits, representing characteristics of the sets. Then,\nit applies bitwise operations (such as xor and population count) on the bitmaps\nin order to infer a similarity upper bound for each pair of sets. If the upper\nbound is below a given similarity threshold, the pair of sets is pruned. The\nBitmap Filter benefits from the fact that bitwise operations are efficiently\nimplemented by many modern general-purpose processors and it was easily applied\nto four state-of-the-art algorithms implemented in CPU: AllPairs, PPJoin,\nAdaptJoin and GroupJoin. Furthermore, we propose a Graphic Processor Unit (GPU)\nalgorithm based on the naive approach but using the Bitmap Filter to speedup\nthe computation. The experiments considered 9 collections containing from 100\nthousands up to 10 million sets and the joins were made using Jaccard\nthresholds from 0.50 to 0.95. The Bitmap Filter was able to improve 90% of the\nexperiments in CPU, with speedups of up to 4.50x and 1.43x on average. Using\nthe GPU algorithm, the experiments were able to speedup the original CPU\nalgorithms by up to 577x using an Nvidia Geforce GTX 980 Ti.\n"]},
{"authors": ["Gowtham Atluri", "Anuj Karpatne", "Vipin Kumar"], "title": ["Spatio-Temporal Data Mining: A Survey of Problems and Methods"], "date": ["2017-11-13T17:17:29Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1711.04710v2"], "summary": ["  Large volumes of spatio-temporal data are increasingly collected and studied\nin diverse domains including, climate science, social sciences, neuroscience,\nepidemiology, transportation, mobile health, and Earth sciences.\nSpatio-temporal data differs from relational data for which computational\napproaches are developed in the data mining community for multiple decades, in\nthat both spatial and temporal attributes are available in addition to the\nactual measurements/attributes. The presence of these attributes introduces\nadditional challenges that needs to be dealt with. Approaches for mining\nspatio-temporal data have been studied for over a decade in the data mining\ncommunity. In this article we present a broad survey of this relatively young\nfield of spatio-temporal data mining. We discuss different types of\nspatio-temporal data and the relevant data mining questions that arise in the\ncontext of analyzing each of these datasets. Based on the nature of the data\nmining problem studied, we classify literature on spatio-temporal data mining\ninto six major categories: clustering, predictive learning, change detection,\nfrequent pattern mining, anomaly detection, and relationship mining. We discuss\nthe various forms of spatio-temporal data mining problems in each of these\ncategories.\n"]},
{"authors": ["Hugo Firth", "Paolo Missier", "Jack Aiston"], "title": ["Loom: Query-aware Partitioning of Online Graphs"], "date": ["2017-11-17T16:06:04Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1711.06608v1"], "summary": ["  As with general graph processing systems, partitioning data over a cluster of\nmachines improves the scalability of graph database management systems.\nHowever, these systems will incur additional network cost during the execution\nof a query workload, due to inter-partition traversals. Workload-agnostic\npartitioning algorithms typically minimise the likelihood of any edge crossing\npartition boundaries. However, these partitioners are sub-optimal with respect\nto many workloads, especially queries, which may require more frequent\ntraversal of specific subsets of inter-partition edges. Furthermore, they\nlargely unsuited to operating incrementally on dynamic, growing graphs.\n  We present a new graph partitioning algorithm, Loom, that operates on a\nstream of graph updates and continuously allocates the new vertices and edges\nto partitions, taking into account a query workload of graph pattern\nexpressions along with their relative frequencies.\n  First we capture the most common patterns of edge traversals which occur when\nexecuting queries. We then compare sub-graphs, which present themselves\nincrementally in the graph update stream, against these common patterns.\nFinally we attempt to allocate each match to single partitions, reducing the\nnumber of inter-partition edges within frequently traversed sub-graphs and\nimproving average query performance.\n  Loom is extensively evaluated over several large test graphs with realistic\nquery workloads and various orderings of the graph updates. We demonstrate\nthat, given a workload, our prototype produces partitionings of significantly\nbetter quality than existing streaming graph partitioning algorithms Fennel and\nLDG.\n"]},
{"authors": ["Jia Zou", "R. Matthew Barnett", "Tania Lorido-Botran", "Shangyu Luo", "Carlos Monroy", "Sourav Sikdar", "Kia Teymourian", "Binhang Yuan", "Chris Jermaine"], "title": ["PlinyCompute: A Platform for High-Performance, Distributed,\n  Data-Intensive Tool Development"], "date": ["2017-11-15T14:01:06Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1711.05573v2"], "summary": ["  This paper describes PlinyCompute, a system for development of\nhigh-performance, data-intensive, distributed computing tools and libraries. In\nthe large, PlinyCompute presents the programmer with a very high-level,\ndeclarative interface, relying on automatic, relational-database style\noptimization to figure out how to stage distributed computations. However, in\nthe small, PlinyCompute presents the capable systems programmer with a\npersistent object data model and API (the \"PC object model\") and associated\nmemory management system that has been designed from the ground-up for high\nperformance, distributed, data-intensive computing. This contrasts with most\nother Big Data systems, which are constructed on top of the Java Virtual\nMachine (JVM), and hence must at least partially cede performance-critical\nconcerns such as memory management (including layout and de/allocation) and\nvirtual method/function dispatch to the JVM. This hybrid approach---declarative\nin the large, trusting the programmer's ability to utilize PC object model\nefficiently in the small---results in a system that is ideal for the\ndevelopment of reusable, data-intensive tools and libraries. Through extensive\nbenchmarking, we show that implementing complex objects manipulation and\nnon-trivial, library-style computations on top of PlinyCompute can result in a\nspeedup of 2x to more than 50x or more compared to equivalent implementations\non Spark.\n"]},
{"authors": ["Jeevana Priya Inala", "Rishabh Singh"], "title": ["WebRelate: Integrating Web Data with Spreadsheets using Examples"], "date": ["2017-11-15T20:17:07Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1711.05787v1"], "summary": ["  Data integration between web sources and relational data is a key challenge\nfaced by data scientists and spreadsheet users. There are two main challenges\nin programmatically joining web data with relational data. First, most websites\ndo not expose a direct interface to obtain tabular data, so the user needs to\nformulate a logic to get to different webpages for each input row in the\nrelational table. Second, after reaching the desired webpage, the user needs to\nwrite complex scripts to extract the relevant data, which is often conditioned\non the input data. Since many data scientists and end-users come from diverse\nbackgrounds, writing such complex regular-expression based logical scripts to\nperform data integration tasks is unfortunately often beyond their programming\nexpertise.\n  We present WebRelate, a system that allows users to join semi-structured web\ndata with relational data in spreadsheets using input-output examples.\nWebRelate decomposes the web data integration task into two sub-tasks of i) URL\nlearning and ii) input-dependent web extraction. The first sub-task generates\nthe URLs for the webpages containing the desired data for all rows in the\nrelational table. WebRelate achieves this by learning a string transformation\nprogram using a few example URLs. The second sub-task uses examples of desired\ndata to be extracted from the corresponding webpages and learns a program to\nextract the data for the other rows. We design expressive domain-specific\nlanguages for URL generation and web data extraction, and present efficient\nsynthesis algorithms for learning programs in these DSLs from few input-output\nexamples. We evaluate WebRelate on 88 real-world web data integration tasks\ntaken from online help forums and Excel product team, and show that WebRelate\ncan learn the desired programs within few seconds using only 1 example for the\nmajority of the tasks.\n"]},
{"authors": ["Sergio Miranda Freire", "Luciana Tricai Cavalini", "Douglas Teodoro", "Erik Sundvall"], "title": ["Archetypes for Representing Data about the Brazilian Public Hospital\n  Information System and Outpatient High Complexity Procedures System"], "date": ["2017-11-15T02:18:53Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1711.09731v1"], "summary": ["  The Brazilian Ministry of Health has selected the openEHR model as a standard\nfor electronic health record systems. This paper presents a set of archetypes\nto represent the main data from the Brazilian Public Hospital Information\nSystem and the High Complexity Procedures Module of the Brazilian public\nOutpatient Health Information System. The archetypes from the public openEHR\nClinical Knowledge Manager (CKM), were examined in order to select archetypes\nthat could be used to represent the data of the above mentioned systems. For\nseveral concepts, it was necessary to specialize the CKM archetypes, or design\nnew ones. A total of 22 archetypes were used: 8 new, 5 specialized and 9 reused\nfrom CKM. This set of archetypes can be used not only for information exchange,\nbut also for generating a big anonymized dataset for testing openEHR-based\nsystems.\n"]},
{"authors": ["Thomas Guyet", "Yves Moinard", "Ren\u00e9 Quiniou", "Torsten Schaub"], "title": ["Efficiency Analysis of ASP Encodings for Sequential Pattern Mining Tasks"], "date": ["2017-11-14T14:09:05Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1711.05090v1"], "summary": ["  This article presents the use of Answer Set Programming (ASP) to mine\nsequential patterns. ASP is a high-level declarative logic programming paradigm\nfor high level encoding combinatorial and optimization problem solving as well\nas knowledge representation and reasoning. Thus, ASP is a good candidate for\nimplementing pattern mining with background knowledge, which has been a data\nmining issue for a long time. We propose encodings of the classical sequential\npattern mining tasks within two representations of embeddings (fill-gaps vs\nskip-gaps) and for various kinds of patterns: frequent, constrained and\ncondensed. We compare the computational performance of these encodings with\neach other to get a good insight into the efficiency of ASP encodings. The\nresults show that the fill-gaps strategy is better on real problems due to\nlower memory consumption. Finally, compared to a constraint programming\napproach (CPSM), another declarative programming paradigm, our proposal showed\ncomparable performance.\n"]},
{"authors": ["Rema Ananthanarayanan", "Pranay Kr. Lohia", "Srikanta Bedathur"], "title": ["DataVizard: Recommending Visual Presentations for Structured Data"], "date": ["2017-11-14T06:43:30Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1711.04971v1"], "summary": ["  Selecting the appropriate visual presentation of the data such that it\npreserves the semantics of the underlying data and at the same time provides an\nintuitive summary of the data is an important, often the final step of data\nanalytics. Unfortunately, this is also a step involving significant human\neffort starting from selection of groups of columns in the structured results\nfrom analytics stages, to the selection of right visualization by experimenting\nwith various alternatives. In this paper, we describe our \\emph{DataVizard}\nsystem aimed at reducing this overhead by automatically recommending the most\nappropriate visual presentation for the structured result. Specifically, we\nconsider the following two scenarios: first, when one needs to visualize the\nresults of a structured query such as SQL; and the second, when one has\nacquired a data table with an associated short description (e.g., tables from\nthe Web). Using a corpus of real-world database queries (and their results) and\na number of statistical tables crawled from the Web, we show that DataVizard is\ncapable of recommending visual presentations with high accuracy. We also\npresent the results of a user survey that we conducted in order to assess user\nviews of the suitability of the presented charts vis-a-vis the plain text\ncaptions of the data.\n"]},
{"authors": ["Jennifer Edmond", "Georgina Nugent Folan"], "title": ["Digitising Cultural Complexity: Representing Rich Cultural Data in a Big\n  Data environment"], "date": ["2017-11-13T07:31:24Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1711.04452v1"], "summary": ["  One of the major terminological forces driving ICT integration in research\ntoday is that of \"big data.\" While the phrase sounds inclusive and integrative,\n\"big data\" approaches are highly selective, excluding input that cannot be\neffectively structured, represented, or digitised. Data of this complex sort is\nprecisely the kind that human activity produces, but the technological\nimperative to enhance signal through the reduction of noise does not\naccommodate this richness. Data and the computational approaches that\nfacilitate \"big data\" have acquired a perceived objectivity that belies their\ncurated, malleable, reactive, and performative nature. In an input environment\nwhere anything can \"be data\" once it is entered into the system as \"data,\" data\ncleaning and processing, together with the metadata and information\narchitectures that structure and facilitate our cultural archives acquire a\ncapacity to delimit what data are. This engenders a process of simplification\nthat has major implications for the potential for future innovation within\nresearch environments that depend on rich material yet are increasingly\nmediated by digital technologies. This paper presents the preliminary findings\nof the European-funded KPLEX (Knowledge Complexity) project which investigates\nthe delimiting effect digital mediation and datafication has on rich, complex\ncultural data. The paper presents a systematic review of existing implicit\ndefinitions of data, elaborating on the implications of these definitions and\nhighlighting the ways in which metadata and computational technologies can\nrestrict the interpretative potential of data. It sheds light on the gap\nbetween analogue or augmented digital practices and fully computational ones,\nand the strategies researchers have developed to deal with this gap. The paper\nproposes a reconceptualisation of data as it is functionally employed within\ndigitally-mediated research so as to incorporate and acknowledge the richness\nand complexity of our source materials.\n"]},
{"authors": ["Xiaojun Xu", "Chang Liu", "Dawn Song"], "title": ["SQLNet: Generating Structured Queries From Natural Language Without\n  Reinforcement Learning"], "date": ["2017-11-13T06:41:29Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1711.04436v1"], "summary": ["  Synthesizing SQL queries from natural language is a long-standing open\nproblem and has been attracting considerable interest recently. Toward solving\nthe problem, the de facto approach is to employ a sequence-to-sequence-style\nmodel. Such an approach will necessarily require the SQL queries to be\nserialized. Since the same SQL query may have multiple equivalent\nserializations, training a sequence-to-sequence-style model is sensitive to the\nchoice from one of them. This phenomenon is documented as the \"order-matters\"\nproblem. Existing state-of-the-art approaches rely on reinforcement learning to\nreward the decoder when it generates any of the equivalent serializations.\nHowever, we observe that the improvement from reinforcement learning is\nlimited.\n  In this paper, we propose a novel approach, i.e., SQLNet, to fundamentally\nsolve this problem by avoiding the sequence-to-sequence structure when the\norder does not matter. In particular, we employ a sketch-based approach where\nthe sketch contains a dependency graph so that one prediction can be done by\ntaking into consideration only the previous predictions that it depends on. In\naddition, we propose a sequence-to-set model as well as the column attention\nmechanism to synthesize the query based on the sketch. By combining all these\nnovel techniques, we show that SQLNet can outperform the prior art by 9% to 13%\non the WikiSQL task.\n"]},
{"authors": ["Alessandro Ronca", "Mark Kaminski", "Bernardo Cuenca Grau", "Boris Motik", "Ian Horrocks"], "title": ["Stream Reasoning in Temporal Datalog"], "date": ["2017-11-10T21:11:17Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1711.04013v1"], "summary": ["  In recent years, there has been an increasing interest in extending\ntraditional stream processing engines with logical, rule-based, reasoning\ncapabilities. This poses significant theoretical and practical challenges since\nrules can derive new information and propagate it both towards past and future\ntime points; as a result, streamed query answers can depend on data that has\nnot yet been received, as well as on data that arrived far in the past. Stream\nreasoning algorithms, however, must be able to stream out query answers as soon\nas possible, and can only keep a limited number of previous input facts in\nmemory. In this paper, we propose novel reasoning problems to deal with these\nchallenges, and study their computational properties on Datalog extended with a\ntemporal sort and the successor function (a core rule-based language for stream\nreasoning applications).\n"]},
{"authors": ["Navid Yaghmazadeh", "Xinyu Wang", "Isil Dillig"], "title": ["Automated Migration of Hierarchical Data to Relational Tables using\n  Programming-by-Example"], "date": ["2017-11-10T20:08:18Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1711.04001v1"], "summary": ["  While many applications export data in hierarchical formats like XML and\nJSON, it is often necessary to convert such hierarchical documents to a\nrelational representation. This paper presents a novel programming-by-example\napproach, and its implementation in a tool called Mitra, for automatically\nmigrating tree-structured documents to relational tables. We have evaluated the\nproposed technique using two sets of experiments. In the first experiment, we\nused Mitra to automate 98 data transformation tasks collected from\nStackOverflow. Our method can generate the desired program for 94% of these\nbenchmarks with an average synthesis time of 3.8 seconds. In the second\nexperiment, we used Mitra to generate programs that can convert real-world XML\nand JSON datasets to full-fledged relational databases. Our evaluation shows\nthat Mitra can automate the desired transformation for all datasets.\n"]},
{"authors": ["Sandeep Singh Sandha"], "title": ["StreetX: Spatio-Temporal Access Control Model for Data"], "date": ["2017-11-10T18:32:17Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1711.03955v1"], "summary": ["  Cities are a big source of spatio-temporal data that is shared across\nentities to drive potential use cases. Many of the Spatio-temporal datasets are\nconfidential and are selectively shared. To allow selective sharing, several\naccess control models exist, however user cannot express arbitrary space and\ntime constraints on data attributes using them. In this paper we focus on\nspatio-temporal access control model. We show that location and time attributes\nof data may decide its confidentiality via a motivating example and thus can\naffect user's access control policy. In this paper, we present StreetX which\nenables user to represent constraints on multiple arbitrary space regions and\ntime windows using a simple abstract language. StreetX is scalable and is\ndesigned to handle large amount of spatio-temporal data from multiple users.\nMultiple space and time constraints can affect performance of the query and may\nalso result in conflicts. StreetX automatically resolve conflicts and optimizes\nthe query evaluation with access control to improve performance. We implemented\nand tested prototype of StreetX using space constraints by defining region\nhaving 1749 polygon coordinates on 10 million data records. Our testing shows\nthat StreetX extends the current access control with spatio-temporal\ncapabilities.\n"]},
{"authors": ["Albert Atserias", "Martin Grohe", "D\u00e1niel Marx"], "title": ["Size bounds and query plans for relational joins"], "date": ["2017-11-10T15:16:52Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1711.03860v1"], "summary": ["  Relational joins are at the core of relational algebra, which in turn is the\ncore of the standard database query language SQL. As their evaluation is\nexpensive and very often dominated by the output size, it is an important task\nfor database query optimisers to compute estimates on the size of joins and to\nfind good execution plans for sequences of joins. We study these problems from\na theoretical perspective, both in the worst-case model, and in an average-case\nmodel where the database is chosen according to a known probability\ndistribution. In the former case, our first key observation is that the\nworst-case size of a query is characterised by the fractional edge cover number\nof its underlying hypergraph, a combinatorial parameter previously known to\nprovide an upper bound. We complete the picture by proving a matching lower\nbound, and by showing that there exist queries for which the join-project plan\nsuggested by the fractional edge cover approach may be substantially better\nthan any join plan that does not use intermediate projections. On the other\nhand, we show that in the average-case model, every join-project plan can be\nturned into a plan containing no projections in such a way that the expected\ntime to evaluate the plan increases only by a constant factor independent of\nthe size of the database. Not surprisingly, the key combinatorial parameter in\nthis context is the maximum density of the underlying hypergraph. We show how\nto make effective use of this parameter to eliminate the projections.\n"]},
{"authors": ["Nelson Piedra", "Janneth Chicaiza", "Jorge Lopez-Vargas", "Edmundo Tovar"], "title": ["Discovery of potential collaboration networks from open knowledge\n  sources"], "date": ["2017-11-08T20:58:16Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1711.03537v1"], "summary": ["  Scientific publishing conveys the outputs of an academic or research\nactivity, in this sense; it also reflects the efforts and issues in which\npeople engage. To identify potential collaborative networks one of the simplest\napproaches is to leverage the co-authorship relations. In this approach,\nsemantic and hierarchic relationships defined by a Knowledge Organization\nSystem are used in order to improve the system's ability to recommend potential\nnetworks beyond the lexical or syntactic analysis of the topics or concepts\nthat are of interest to academics.\n"]},
{"authors": ["Tejas Kulkarni", "Graham Cormode", "Divesh Srivastava"], "title": ["Marginal Release Under Local Differential Privacy"], "date": ["2017-11-08T14:15:53Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1711.02952v1"], "summary": ["  Many analysis and machine learning tasks require the availability of marginal\nstatistics on multidimensional datasets while providing strong privacy\nguarantees for the data subjects. Applications for these statistics range from\nfinding correlations in the data to fitting sophisticated prediction models. In\nthis paper, we provide a set of algorithms for materializing marginal\nstatistics under the strong model of local differential privacy. We prove the\nfirst tight theoretical bounds on the accuracy of marginals compiled under each\napproach, perform empirical evaluation to confirm these bounds, and evaluate\nthem for tasks such as modeling and correlation testing. Our results show that\nreleasing information based on (local) Fourier transformations of the input is\npreferable to alternatives based directly on (local) marginals.\n"]},
{"authors": ["Heli Sun", "Zhou Yang", "Jianbin Huang", "Xiaolin Jia", "Ziyu Guan", "Zhongmeng Zhao"], "title": ["Efficient Destination Prediction Based on Route Choices with Transition\n  Matrix Optimization"], "date": ["2017-11-08T06:40:28Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1711.04595v1"], "summary": ["  Destination prediction is an essential task in a variety of mobile\napplications. In this paper, we optimize the matrix operation and adapt a\nsemi-lazy framework to improve the prediction accuracy and efficiency of a\nstate-of-the-art approach. To this end, we employ efficient dynamic-programming\nby devising several data constructs including Efficient Transition Probability\nand Transition Probabilities with Detours that are capable of pinpointing the\nminimum amount of computation. We prove that our method achieves one order of\ncut in both time and space complexity. The experimental results on real-world\nand synthetic datasets have shown that our solution consistently outperforms\nits state-of-the-art counterparts in terms of both efficiency (approximately\nover 100 times faster) and accuracy (above 30% increase).\n"]},
{"authors": ["Noah Johnson", "Joseph P. Near", "Dawn Song"], "title": ["Towards Practical Differential Privacy for SQL Queries"], "date": ["2017-06-28T20:44:28Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1706.09479v3"], "summary": ["  Differential privacy promises to enable general data analytics while\nprotecting individual privacy, but existing differential privacy mechanisms do\nnot support the wide variety of features and databases used in real-world\nSQL-based analytics systems.\n  This paper presents the first practical approach for differential privacy of\nSQL queries. Using 8.1 million real-world queries, we conduct an empirical\nstudy to determine the requirements for practical differential privacy, and\ndiscuss limitations of previous approaches in light of these requirements. To\nmeet these requirements we propose elastic sensitivity, a novel method for\napproximating the local sensitivity of queries with general equijoins. We prove\nthat elastic sensitivity is an upper bound on local sensitivity and can\ntherefore be used to enforce differential privacy using any local\nsensitivity-based mechanism.\n  We build FLEX, a practical end-to-end system to enforce differential privacy\nfor SQL queries using elastic sensitivity. We demonstrate that FLEX is\ncompatible with any existing database, can enforce differential privacy for\nreal-world SQL queries, and incurs negligible (0.03%) performance overhead.\n"]},
{"authors": ["Willi Mann", "Nikolaus Augsten", "Christian S. Jensen"], "title": ["SWOOP: Top-k Similarity Joins over Set Streams"], "date": ["2017-11-07T14:20:30Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1711.02476v1"], "summary": ["  We provide efficient support for applications that aim to continuously find\npairs of similar sets in rapid streams of sets. A prototypical example setting\nis that of tweets. A tweet is a set of words, and Twitter emits about half a\nbillion tweets per day. Our solution makes it possible to efficiently maintain\nthe top-$k$ most similar tweets from a pair of rapid Twitter streams, e.g., to\ndiscover similar trends in two cities if the streams concern cities.\n  Using a sliding window model, the top-$k$ result changes as new sets in the\nstream enter the window or existing ones leave the window. Maintaining the\ntop-$k$ result under rapid streams is challenging. First, when a set arrives,\nit may form a new pair for the top-$k$ result with any set already in the\nwindow. Second, when a set leaves the window, all its pairings in the top-$k$\nare invalidated and must be replaced. It is not enough to maintain the $k$ most\nsimilar pairs, as less similar pairs may eventually be promoted to the top-$k$\nresult. A straightforward solution that pairs every new set with all sets in\nthe window and keeps all pairs for maintaining the top-$k$ result is memory\nintensive and too slow. We propose SWOOP, a highly scalable stream join\nalgorithm that solves these issues. Novel indexing techniques and sophisticated\nfilters efficiently prune useless pairs as new sets enter the window. SWOOP\nincrementally maintains a stock of similar pairs to update the top-$k$ result\nat any time, and the stock is shown to be minimal. Our experiments confirm that\nSWOOP can deal with stream rates that are orders of magnitude faster than the\nrates of existing approaches.\n"]},
{"authors": ["Jose Picado", "Arash Termehchy", "Alan Fern", "Parisa Ataei"], "title": ["Schema Independent Relational Learning"], "date": ["2015-08-16T16:57:20Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1508.03846v2"], "summary": ["  Learning novel concepts and relations from relational databases is an\nimportant problem with many applications in database systems and machine\nlearning. Relational learning algorithms learn the definition of a new relation\nin terms of existing relations in the database. Nevertheless, the same data set\nmay be represented under different schemas for various reasons, such as\nefficiency, data quality, and usability. Unfortunately, the output of current\nrelational learning algorithms tends to vary quite substantially over the\nchoice of schema, both in terms of learning accuracy and efficiency. This\nvariation complicates their off-the-shelf application. In this paper, we\nintroduce and formalize the property of schema independence of relational\nlearning algorithms, and study both the theoretical and empirical dependence of\nexisting algorithms on the common class of (de) composition schema\ntransformations. We study both sample-based learning algorithms, which learn\nfrom sets of labeled examples, and query-based algorithms, which learn by\nasking queries to an oracle. We prove that current relational learning\nalgorithms are generally not schema independent. For query-based learning\nalgorithms we show that the (de) composition transformations influence their\nquery complexity. We propose Castor, a sample-based relational learning\nalgorithm that achieves schema independence by leveraging data dependencies. We\nsupport the theoretical results with an empirical study that demonstrates the\nschema dependence/independence of several algorithms on existing benchmark and\nreal-world datasets under (de) compositions.\n"]},
{"authors": ["Shanshan Han", "Hongzhi Wang", "Jialin Wan", "Jianzhong Li"], "title": ["An Iterative Scheme for Leverage-based Approximate Aggregation"], "date": ["2017-11-06T15:34:35Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1711.01960v1"], "summary": ["  Currently data explosion poses great challenges to approximate aggregation on\nefficiency and accuracy. To address this problem, we propose a novel approach\nto calculate aggregation answers in high accuracy using only a small share of\ndata. We introduce leverages to reflect individual differences of samples from\nthe statistical perspective. Two kinds of estimators, the leverage-based\nestimator and the sketch estimator (a \"rough picture\" of the aggregation\nanswer), are in constraint relations and iteratively improved according to the\nactual conditions until their difference is below a threshold. Due to the\niteration mechanism and the leverages, our approach achieves high accuracy.\nMoreover, some features, including not requiring recording sampled data and\neasy to extend to various execution modes (such as, the online mode), make our\napproach well suited to deal with big data. Experiments show that our approach\nhas extraordinary performance, and when compared with the uniform sampling, our\napproach can achieve high-quality answers with only 1/3 of the same sample\nsize.\n"]},
{"authors": ["Sanjay Krishnan", "Michael J. Franklin", "Ken Goldberg", "Eugene Wu"], "title": ["BoostClean: Automated Error Detection and Repair for Machine Learning"], "date": ["2017-11-03T18:50:08Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1711.01299v1"], "summary": ["  Predictive models based on machine learning can be highly sensitive to data\nerror. Training data are often combined with a variety of different sources,\neach susceptible to different types of inconsistencies, and new data streams\nduring prediction time, the model may encounter previously unseen\ninconsistencies. An important class of such inconsistencies is domain value\nviolations that occur when an attribute value is outside of an allowed domain.\nWe explore automatically detecting and repairing such violations by leveraging\nthe often available clean test labels to determine whether a given detection\nand repair combination will improve model accuracy. We present BoostClean which\nautomatically selects an ensemble of error detection and repair combinations\nusing statistical boosting. BoostClean selects this ensemble from an extensible\nlibrary that is pre-populated general detection functions, including a novel\ndetector based on the Word2Vec deep learning model, which detects errors across\na diverse set of domains. Our evaluation on a collection of 12 datasets from\nKaggle, the UCI repository, real-world data analyses, and production datasets\nthat show that Boost- Clean can increase absolute prediction accuracy by up to\n9% over the best non-ensembled alternatives. Our optimizations including\nparallelism, materialization, and indexing techniques show a 22.2x end-to-end\nspeedup on a 16-core machine.\n"]},
{"authors": ["Niek Tax", "Natalia Sidorova", "Wil M. P. van der Aalst"], "title": ["Discovering More Precise Process Models from Event Logs by Filtering Out\n  Chaotic Activities"], "date": ["2017-11-03T18:13:36Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1711.01287v1"], "summary": ["  Process Discovery is concerned with the automatic generation of a process\nmodel that describes a business process from execution data of that business\nprocess. Real life event logs can contain chaotic activities. These activities\nare independent of the state of the process and can, therefore, happen at\nrather arbitrary points in time. We show that the presence of such chaotic\nactivities in an event log heavily impacts the quality of the process models\nthat can be discovered with process discovery techniques. The current modus\noperandi for filtering activities from event logs is to simply filter out\ninfrequent activities. We show that frequency-based filtering of activities\ndoes not solve the problems that are caused by chaotic activities. Moreover, we\npropose a novel technique to filter out chaotic activities from event logs. We\nevaluate this technique on a collection of seventeen real-life event logs that\noriginate from both the business process management domain and the smart home\nenvironment domain. As demonstrated, the developed activity filtering methods\nenable the discovery of process models that are more behaviorally specific\ncompared to process models that are discovered using standard frequency-based\nfiltering.\n"]},
{"authors": ["Tommaso Soru", "Diego Esteves", "Edgard Marx", "Axel-Cyrille Ngonga Ngomo"], "title": ["Mandolin: A Knowledge Discovery Framework for the Web of Data"], "date": ["2017-11-03T18:04:06Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1711.01283v1"], "summary": ["  Markov Logic Networks join probabilistic modeling with first-order logic and\nhave been shown to integrate well with the Semantic Web foundations. While\nseveral approaches have been devised to tackle the subproblems of rule mining,\ngrounding, and inference, no comprehensive workflow has been proposed so far.\nIn this paper, we fill this gap by introducing a framework called Mandolin,\nwhich implements a workflow for knowledge discovery specifically on RDF\ndatasets. Our framework imports knowledge from referenced graphs, creates\nsimilarity relationships among similar literals, and relies on state-of-the-art\ntechniques for rule mining, grounding, and inference computation. We show that\nour best configuration scales well and achieves at least comparable results\nwith respect to other statistical-relational-learning algorithms on link\nprediction.\n"]},
{"authors": ["Jim Pivarski", "Peter Elmer", "Brian Bockelman", "Zhe Zhang"], "title": ["Fast Access to Columnar, Hierarchically Nested Data via Code\n  Transformation"], "date": ["2017-08-20T23:41:13Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1708.08319v2"], "summary": ["  Big Data query systems represent data in a columnar format for fast,\nselective access, and in some cases (e.g. Apache Drill), perform calculations\ndirectly on the columnar data without row materialization, avoiding runtime\ncosts.\n  However, many analysis procedures cannot be easily or efficiently expressed\nas SQL. In High Energy Physics, the majority of data processing requires nested\nloops with complex dependencies. When faced with tasks like these, the\nconventional approach is to convert the columnar data back into an object form,\nusually with a performance price.\n  This paper describes a new technique to transform procedural code so that it\noperates on hierarchically nested, columnar data natively, without row\nmaterialization. It can be viewed as a compiler pass on the typed abstract\nsyntax tree, rewriting references to objects as columnar array lookups.\n  We will also present performance comparisons between transformed code and\nconventional object-oriented code in a High Energy Physics context.\n"]},
{"authors": ["Diego Figueira", "Luc Segoufin"], "title": ["Bottom-up automata on data trees and vertical XPath"], "date": ["2017-10-24T13:04:12Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.08748v2"], "summary": ["  A data tree is a finite tree whose every node carries a label from a finite\nalphabet and a datum from some infinite domain. We introduce a new model of\nautomata over unranked data trees with a decidable emptiness problem. It is\nessentially a bottom-up alternating automaton with one register that can store\none data value and can be used to perform equality tests with the data values\noccurring within the subtree of the current node. We show that it captures the\nexpressive power of the vertical fragment of XPath - containing the child,\ndescendant, parent and ancestor axes - obtaining thus a decision procedure for\nits satisfiability problem.\n"]},
{"authors": ["Li Wang", "Tom Z. J. Fu", "Richard T. B. Ma", "Marianne Winslett", "Zhenjie Zhang"], "title": ["Elasticutor: Rapid Elasticity for Realtime Stateful Stream Processing"], "date": ["2017-11-03T07:44:07Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1711.01046v1"], "summary": ["  Elasticity is highly desirable for stream processing systems to guarantee low\nlatency against workload dynamics, such as surges in data arrival rate and\nfluctuations in data distribution. Existing systems achieve elasticity\nfollowing a resource-centric approach that uses dynamic key partitioning across\nthe parallel instances, i.e. executors, to balance the workload and scale\noperators. However, such operator-level key repartitioning needs global\nsynchronization and prohibits rapid elasticity. To address this problem, we\npropose an executor-centric approach, whose core idea is to avoid\noperator-level key repartitioning while implementing each executor as the\nbuilding block of elasticity. Following this new approach, we design the\nElasticutor framework with two level of optimizations: i) a novel\nimplementation of executors, i.e., elastic executors, that perform elastic\nmulti-core execution via efficient intra-executor load balancing and executor\nscaling and ii) a global model-based scheduler that dynamically allocates CPU\ncores to executors based on the instantaneous workloads. We implemented a\nprototype of Elasticutor and conducted extensive experiments. Our results show\nthat Elasticutor doubles the throughput and achieves an average processing\nlatency up to 2 orders of magnitude lower than previous methods, for a dynamic\nworkload of real-world applications.\n"]},
{"authors": ["Xu Hu", "Jun Huang", "Minghui Qiu", "Cen Chen", "Wei Chu"], "title": ["PS-DBSCAN: An Efficient Parallel DBSCAN Algorithm Based on Platform Of\n  AI (PAI)"], "date": ["2017-11-03T06:36:20Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1711.01034v1"], "summary": ["  We present PS-DBSCAN, a communication efficient parallel DBSCAN algorithm\nthat combines the disjoint-set data structure and Parameter Server framework in\nPlatform of AI (PAI). Since data points within the same cluster may be\ndistributed over different workers which result in several disjoint-sets,\nmerging them incurs large communication costs. In our algorithm, we employ a\nfast global union approach to union the disjoint-sets to alleviate the\ncommunication burden. Experiments over the datasets of different scales\ndemonstrate that PS-DBSCAN outperforms the PDSDBSCAN with 2-10 times speedup on\ncommunication efficiency.\n  We have released our PS-DBSCAN in an algorithm platform called Platform of AI\n(PAI - https://pai.base.shuju.aliyun.com/) in Alibaba Cloud. We have also\ndemonstrated how to use the method in PAI.\n"]},
{"authors": ["Gabriela Montoya", "Hala Skaf-Molli", "Katja Hose"], "title": ["The Odyssey Approach for Optimizing Federated SPARQL Queries"], "date": ["2017-05-17T13:10:59Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1705.06135v3"], "summary": ["  Answering queries over a federation of SPARQL endpoints requires combining\ndata from more than one data source. Optimizing queries in such scenarios is\nparticularly challenging not only because of (i) the large variety of possible\nquery execution plans that correctly answer the query but also because (ii)\nthere is only limited access to statistics about schema and instance data of\nremote sources. To overcome these challenges, most federated query engines rely\non heuristics to reduce the space of possible query execution plans or on\ndynamic programming strategies to produce optimal plans. Nevertheless, these\nplans may still exhibit a high number of intermediate results or high execution\ntimes because of heuristics and inaccurate cost estimations. In this paper, we\npresent Odyssey, an approach that uses statistics that allow for a more\naccurate cost estimation for federated queries and therefore enables Odyssey to\nproduce better query execution plans. Our experimental results show that\nOdyssey produces query execution plans that are better in terms of data\ntransfer and execution time than state-of-the-art optimizers. Our experiments\nusing the FedBench benchmark show execution time gains of at least 25 times on\naverage.\n"]},
{"authors": ["Mahmoud Abo Khamis", "Hung Q. Ngo", "Dan Suciu"], "title": ["What do Shannon-type Inequalities, Submodular Width, and Disjunctive\n  Datalog have to do with one another?"], "date": ["2016-12-08T01:06:40Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1612.02503v4"], "summary": ["  Recent works on bounding the output size of a conjunctive query with\nfunctional dependencies and degree constraints have shown a deep connection\nbetween fundamental questions in information theory and database theory. We\nprove analogous output bounds for disjunctive datalog rules, and answer several\nopen questions regarding the tightness and looseness of these bounds along the\nway. Our bounds are intimately related to Shannon-type information\ninequalities. We devise the notion of a \"proof sequence\" of a specific class of\nShannon-type information inequalities called \"Shannon flow inequalities\". We\nthen show how such a proof sequence can be interpreted as symbolic instructions\nguiding an algorithm called \"PANDA\", which answers disjunctive datalog rules\nwithin the time that the size bound predicted. We show that PANDA can be used\nas a black-box to devise algorithms matching precisely the fractional hypertree\nwidth and the submodular width runtimes for aggregate and conjunctive queries\nwith functional dependencies and degree constraints.\n  Our results improve upon known results in three ways. First, our bounds and\nalgorithms are for the much more general class of disjunctive datalog rules, of\nwhich conjunctive queries are a special case. Second, the runtime of PANDA\nmatches precisely the submodular width bound, while the previous algorithm by\nMarx has a runtime that is polynomial in this bound. Third, our bounds and\nalgorithms work for queries with input cardinality bounds, functional\ndependencies, and degree constraints.\n  Overall, our results show a deep connection between three seemingly unrelated\nlines of research; and, our results on proof sequences for Shannon flow\ninequalities might be of independent interest.\n"]},
{"authors": ["Weiren Yu", "Xuemin Lin", "Wenjie Zhang", "Julie A. McCann"], "title": ["Dynamical SimRank Search on Time-Varying Networks"], "date": ["2017-10-31T21:50:10Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1711.00121v1"], "summary": ["  In this article, we study the efficient dynamical computation of all-pairs\nSimRanks on time-varying graphs. Li {\\em et al}.'s approach requires $O(r^4\nn^2)$ time and $O(r^2 n^2)$ memory in a graph with $n$ nodes, where $r$ is the\ntarget rank of the low-rank SVD. (1) We first consider edge update that does\nnot accompany new node insertions. We show that the SimRank update $\\Delta S$\nin response to every link update is expressible as a rank-one Sylvester matrix\nequation. This provides an incremental method requiring $O(Kn^2)$ time and\n$O(n^2)$ memory in the worst case to update all pairs of similarities for $K$\niterations. (2) To speed up the computation further, we propose a lossless\npruning strategy that captures the \"affected areas\" of $\\Delta S$ to eliminate\nunnecessary retrieval. This reduces the time of the incremental SimRank to\n$O(K(m+|AFF|))$, where $m$ is the number of edges in the old graph, and $|AFF|\n(\\le n^2)$ is the size of \"affected areas\" in $\\Delta S$, and in practice,\n$|AFF| \\ll n^2$. (3) We also consider edge updates that accompany node\ninsertions, and categorize them into three cases, according to which end of the\ninserted edge is a new node. For each case, we devise an efficient incremental\nalgorithm that can support new node insertions. (4) We next design an efficient\nbatch incremental method that handles \"similar sink edges\" simultaneously and\neliminates redundant edge updates. (5) To achieve linear memory, we devise a\nmemory-efficient strategy that dynamically updates all pairs of SimRanks column\nby column in just $O(Kn+m)$ memory, without the need to store all $(n^2)$ pairs\nof old SimRank scores. Experimental studies on various datasets demonstrate\nthat our solution substantially outperforms the existing incremental SimRank\nmethods, and is faster and more memory-efficient than its competitors on\nmillion-scale graphs.\n"]},
{"authors": ["Niek Tax", "Emin Alasgarov", "Natalia Sidorova", "Wil M. P. van der Aalst", "Reinder Haakma"], "title": ["Generating Time-Based Label Refinements to Discover More Precise Process\n  Models"], "date": ["2017-05-25T21:01:20Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1705.09359v2"], "summary": ["  Process mining is a research field focused on the analysis of event data with\nthe aim of extracting insights related to dynamic behavior. Applying process\nmining techniques on data from smart home environments has the potential to\nprovide valuable insights in (un)healthy habits and to contribute to ambient\nassisted living solutions. Finding the right event labels to enable the\napplication of process mining techniques is however far from trivial, as simply\nusing the triggering sensor as the label for sensor events results in\nuninformative models that allow for too much behavior (overgeneralizing).\nRefinements of sensor level event labels suggested by domain experts have been\nshown to enable discovery of more precise and insightful process models.\nHowever, there exists no automated approach to generate refinements of event\nlabels in the context of process mining. In this paper we propose a framework\nfor the automated generation of label refinements based on the time attribute\nof events, allowing us to distinguish behaviourally different instances of the\nsame event type based on their time attribute. We show on a case study with\nreal life smart home event data that using automatically generated refined\nlabels in process discovery, we can find more specific, and therefore more\ninsightful, process models. We observe that one label refinement could have an\neffect on the usefulness of other label refinements when used together.\nTherefore, we explore four strategies to generate useful combinations of\nmultiple label refinements and evaluate those on three real life smart home\nevent logs.\n"]},
{"authors": ["Paul Cuddihy", "Justin McHugh", "Jenny Weisenberg Williams", "Varish Mulwad", "Kareem S. Aggour"], "title": ["SemTK: An Ontology-first, Open Source Semantic Toolkit for Managing and\n  Querying Knowledge Graphs"], "date": ["2017-10-31T15:29:35Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.11531v1"], "summary": ["  The relatively recent adoption of Knowledge Graphs as an enabling technology\nin multiple high-profile artificial intelligence and cognitive applications has\nled to growing interest in the Semantic Web technology stack. Many\nsemantics-related tools, however, are focused on serving experts with a deep\nunderstanding of semantic technologies. For example, triplification of\nrelational data is available but there is no open source tool that allows a\nuser unfamiliar with OWL/RDF to import data into a semantic triple store in an\nintuitive manner. Further, many tools require users to have a working\nunderstanding of SPARQL to query data. Casual users interested in benefiting\nfrom the power of Knowledge Graphs have few tools available for exploring,\nquerying, and managing semantic data. We present SemTK, the Semantics Toolkit,\na user-friendly suite of tools that allow both expert and non-expert semantics\nusers convenient ingestion of relational data, simplified query generation, and\nmore. The exploration of ontologies and instance data is performed through\nSPARQLgraph, an intuitive web-based user interface in SemTK understandable and\nnavigable by a lay user. The open source version of SemTK is available at\nhttp://semtk.research.ge.com.\n"]},
{"authors": ["Michael Schlichtkrull", "Thomas N. Kipf", "Peter Bloem", "Rianne van den Berg", "Ivan Titov", "Max Welling"], "title": ["Modeling Relational Data with Graph Convolutional Networks"], "date": ["2017-03-17T17:09:14Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.06103v4"], "summary": ["  Knowledge graphs enable a wide variety of applications, including question\nanswering and information retrieval. Despite the great effort invested in their\ncreation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata)\nremain incomplete. We introduce Relational Graph Convolutional Networks\n(R-GCNs) and apply them to two standard knowledge base completion tasks: Link\nprediction (recovery of missing facts, i.e. subject-predicate-object triples)\nand entity classification (recovery of missing entity attributes). R-GCNs are\nrelated to a recent class of neural networks operating on graphs, and are\ndeveloped specifically to deal with the highly multi-relational data\ncharacteristic of realistic knowledge bases. We demonstrate the effectiveness\nof R-GCNs as a stand-alone model for entity classification. We further show\nthat factorization models for link prediction such as DistMult can be\nsignificantly improved by enriching them with an encoder model to accumulate\nevidence over multiple inference steps in the relational graph, demonstrating a\nlarge improvement of 29.8% on FB15k-237 over a decoder-only baseline.\n"]},
{"authors": ["Rianne van den Berg", "Thomas N. Kipf", "Max Welling"], "title": ["Graph Convolutional Matrix Completion"], "date": ["2017-06-07T17:05:19Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1706.02263v2"], "summary": ["  We consider matrix completion for recommender systems from the point of view\nof link prediction on graphs. Interaction data such as movie ratings can be\nrepresented by a bipartite user-item graph with labeled edges denoting observed\nratings. Building on recent progress in deep learning on graph-structured data,\nwe propose a graph auto-encoder framework based on differentiable message\npassing on the bipartite interaction graph. Our model shows competitive\nperformance on standard collaborative filtering benchmarks. In settings where\ncomplimentary feature information or structured data such as a social network\nis available, our framework outperforms recent state-of-the-art methods.\n"]},
{"authors": ["Shoumik Palkar", "James Thomas", "Deepak Narayanan", "Anil Shanbhag", "Rahul Palamuttam", "Holger Pirk", "Malte Schwarzkopf", "Saman Amarasinghe", "Samuel Madden", "Matei Zaharia"], "title": ["Weld: Rethinking the Interface Between Data-Intensive Applications"], "date": ["2017-09-14T05:37:20Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.06416v2"], "summary": ["  Data analytics applications combine multiple functions from different\nlibraries and frameworks. Even when each function is optimized in isolation,\nthe performance of the combined application can be an order of magnitude below\nhardware limits due to extensive data movement across these functions. To\naddress this problem, we propose Weld, a new interface between data-intensive\nlibraries that can optimize across disjoint libraries and functions. Weld\nexposes a lazily-evaluated API where diverse functions can submit their\ncomputations in a simple but general intermediate representation that captures\ntheir data-parallel structure. It then optimizes data movement across these\nfunctions and emits efficient code for diverse hardware. Weld can be integrated\ninto existing frameworks such as Spark, TensorFlow, Pandas and NumPy without\nchanging their user-facing APIs. We demonstrate that Weld can speed up\napplications using these frameworks by up to 29x.\n"]},
{"authors": ["Andrew Figueroa", "Steven Rollo", "Sean Murthy"], "title": ["A Brief Comparison of Two Enterprise-Class RDBMSs"], "date": ["2017-10-22T21:59:47Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.08023v1"], "summary": ["  This paper is an extended version of a report from a student-developed study\nto compare Microsoft SQL Server and PostgreSQL, two widely-used\nenterprise-class relational database management systems (RDBMSs). The study\nfollowed an introductory undergraduate course in relational systems and was\ndesigned to help gain practical understanding of specific DBMSs. During this\nstudy, we implemented three non-trivial schemas in each system, identified 26\ncommon database design, development, and administration activities while\nimplementing the schemas, and compared the support each system offers to carry\nout the identified activities. Where relevant, we also compared each system\nagainst the SQL standard.\n  In this report, we present a summary of the similarities and differences we\nfound between the two systems, and we provide a quantitative measure ranking\nboth systems' implementations of the 26 activities. We also briefly discuss the\n\"technical suitability\" of PostgreSQL to enterprise applications. Although this\nreport is not comprehensive and is too general to comment on the suitability of\neither system to a specific enterprise application, it can nevertheless provide\nan initial set of considerations and criteria to choose a system for most\nenterprise applications.\n"]},
{"authors": ["Xin Hu", "Yingting Yao", "Luting Ye", "Depeng Dang"], "title": ["Natural Language Aggregate Query over RDF Data"], "date": ["2017-10-22T05:35:37Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.07891v1"], "summary": ["  Natural language question-answering over RDF data has received widespread\nattention. Although there have been several studies that have dealt with a\nsmall number of aggregate queries, they have many restrictions (i.e.,\ninteractive information, controlled question or query template). Thus far,\nthere has been no natural language querying mechanism that can process general\naggregate queries over RDF data. Therefore, we propose a framework called NLAQ\n(Natural Language Aggregate Query). First, we propose a novel algorithm to\nautomatically understand a users query intention, which mainly contains\nsemantic relations and aggregations. Second, to build a better bridge between\nthe query intention and RDF data, we propose an extended paraphrase dictionary\nED to obtain more candidate mappings for semantic relations, and we introduce a\npredicate-type adjacent set PT to filter out inappropriate candidate mapping\ncombinations in semantic relations and basic graph patterns. Third, we design a\nsuitable translation plan for each aggregate category and effectively\ndistinguish whether an aggregate item is numeric or not, which will greatly\naffect the aggregate result. Finally, we conduct extensive experiments over\nreal datasets (QALD benchmark and DBpedia), and the experimental results\ndemonstrate that our solution is effective.\n"]},
{"authors": ["Sang-Woo Jun", "Andy Wright", "Sizhuo Zhang", "Shuotao Xu", " Arvind"], "title": ["BigSparse: High-performance external graph analytics"], "date": ["2017-10-21T01:09:53Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.07736v1"], "summary": ["  We present BigSparse, a fully external graph analytics system that picks up\nwhere semi-external systems like FlashGraph and X-Stream, which only store\nvertex data in memory, left off. BigSparse stores both edge and vertex data in\nan array of SSDs and avoids random updates to the vertex data, by first logging\nthe vertex updates and then sorting the log to sequentialize accesses to the\nSSDs. This newly introduced sorting overhead is reduced significantly by\ninterleaving sorting with vertex reduction operations. In our experiments on a\nserver with 32GB to 64GB of DRAM, BigSparse outperforms other in-memory and\nsemi-external graph analytics systems for algorithms such as PageRank,\nBreadthFirst Search, and Betweenness-Centrality for terabyte-size graphs with\nbillions of vertices. BigSparse is capable of highspeed analytics of much\nlarger graphs, on the same machine configuration.\n"]},
{"authors": ["Yuepeng Wang", "Isil Dillig", "Shuvendu K. Lahiri", "William R. Cook"], "title": ["Verifying Equivalence of Database-Driven Applications"], "date": ["2017-10-20T18:38:40Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.07660v1"], "summary": ["  This paper addresses the problem of verifying equivalence between a pair of\nprograms that operate over databases with different schemas. This problem is\nparticularly important in the context of web applications, which typically\nundergo database refactoring either for performance or maintainability reasons.\nWhile web applications should have the same externally observable behavior\nbefore and after schema migration, there are no existing tools for proving\nequivalence of such programs. This paper takes a first step towards solving\nthis problem by formalizing the equivalence and refinement checking problems\nfor database-driven applications. We also propose a proof methodology based on\nthe notion of bisimulation invariants over relational algebra with updates and\ndescribe a technique for synthesizing such bisimulation invariants. We have\nimplemented the proposed technique in a tool called Mediator for verifying\nequivalence between database-driven applications written in our intermediate\nlanguage and evaluate our tool on 21 benchmarks extracted from textbooks and\nreal-world web applications. Our results show that the proposed methodology can\nsuccessfully verify 20 of these benchmarks.\n"]},
{"authors": ["Jyoti Leeka", "Srikanta Bedathur", "Debajyoti Bera", "Sriram Lakshminarasimhan"], "title": ["STREAK: An Efficient Engine for Processing Top-k SPARQL Queries with\n  Spatial Filters"], "date": ["2017-10-20T04:12:39Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.07411v1"], "summary": ["  The importance of geo-spatial data in critical applications such as emergency\nresponse, transportation, agriculture etc., has prompted the adoption of recent\nGeoSPARQL standard in many RDF processing engines. In addition to large\nrepositories of geo-spatial data -- e.g., LinkedGeoData, OpenStreetMap, etc. --\nspatial data is also routinely found in automatically constructed\nknowledgebases such as Yago and WikiData. While there have been research\nefforts for efficient processing of spatial data in RDF/SPARQL, very little\neffort has gone into building end-to-end systems that can holistically handle\ncomplex SPARQL queries along with spatial filters.\n  In this paper, we present Streak, a RDF data management system that is\ndesigned to support a wide-range of queries with spatial filters including\ncomplex joins, top-k, higher-order relationships over spatially enriched\ndatabases. Streak introduces various novel features such as a careful\nidentifier encoding strategy for spatial and non-spatial entities, the use of a\nsemantics-aware Quad-tree index that allows for early-termination and a clever\nuse of adaptive query processing with zero plan-switch cost. We show that\nStreak can scale to some of the largest publicly available semantic data\nresources such as Yago3 and LinkedGeoData which contain spatial entities and\nquantifiable predicates useful for result ranking. For experimental\nevaluations, we focus on top-k distance join queries and demonstrate that\nStreak outperforms popular spatial join algorithms as well as state of the art\nend-to-end systems like Virtuoso and PostgreSQL.\n"]},
{"authors": ["Jedrzej Potoniec", "Piotr Jakubowski", "Agnieszka \u0141awrynowicz"], "title": ["Swift Linked Data Miner: Mining OWL 2 EL class expressions directly from\n  online RDF datasets"], "date": ["2017-10-19T12:25:06Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.07114v1"], "summary": ["  In this study, we present Swift Linked Data Miner, an interruptible algorithm\nthat can directly mine an online Linked Data source (e.g., a SPARQL endpoint)\nfor OWL 2 EL class expressions to extend an ontology with new SubClassOf:\naxioms. The algorithm works by downloading only a small part of the Linked Data\nsource at a time, building a smart index in the memory and swiftly iterating\nover the index to mine axioms. We propose a transformation function from mined\naxioms to RDF Data Shapes. We show, by means of a crowdsourcing experiment,\nthat most of the axioms mined by Swift Linked Data Miner are correct and can be\nadded to an ontology. We provide a ready to use Prot\\'eg\\'e plugin implementing\nthe algorithm, to support ontology engineers in their daily modeling work.\n"]},
{"authors": ["Emeric Dynomant", "Mathilde Gorieu", "Helene Perrin", "Marion Denorme", "Fabien Pichon", "Arnaud Desfeux"], "title": ["MEDOC: a Python wrapper to load MEDLINE into a local MySQL database"], "date": ["2017-10-18T06:14:53Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.06590v1"], "summary": ["  Since the MEDLINE database was released, the number of documents indexed by\nthis entity has risen every year. Several tools have been developed by the\nNational Institutes of Health (NIH) to query this corpus of scientific\npublications. However, in terms of advances in big data, text-mining and data\nscience, an option to build a local relational database containing all metadata\navailable on MEDLINE would be truly useful to optimally exploit these\nresources. MEDOC (MEdline DOwnloading Contrivance) is a Python program designed\nto download data on an FTP and to load all extracted information into a local\nMySQL database. It took MEDOC 4 days and 17 hours to load the 26 million\ndocuments available on this server onto a standard computer. This indexed\nrelational database allows the user to build complex and rapid queries. All\nfields can thus be searched for desired information, a task that is difficult\nto accomplish through the PubMed graphical interface. MEDOC is free and\npublicly available at https://github.com/MrMimic/MEDOC.\n"]},
{"authors": ["Arijit Khan", "Gustavo Segovia", "Donald Kossmann"], "title": ["On Smart Query Routing: For Distributed Graph Querying with Decoupled\n  Storage"], "date": ["2016-11-12T06:22:28Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1611.03959v2"], "summary": ["  We study online graph queries that retrieve nearby nodes of a query node from\na large network. To answer such queries with high throughput and low latency,\nwe partition the graph and process the data in parallel across a cluster of\nservers. State-of-the-art distributed graph querying systems place each graph\npartition on a separate server, where query answering over that partition takes\nplace. This design has two major disadvantages. First, the router needs to\nmaintain a fixed routing table. Hence, these systems are less flexible with\nrespect to query routing, fault tolerance, and graph updates. Second, the graph\ndata must be partitioned such that the workload across the servers is balanced,\nand the inter-machine communication is minimized. In addition, it is required\nto update the existing partitions based on workload changes over graph nodes.\nHowever, graph partitioning, online monitoring of workloads, and dynamically\nupdating the graph partitions are expensive. In this work, we mitigate both\nthese problems by decoupling graph storage from query processors, and by\ndeveloping smart routing strategies that improve the cache locality in query\nprocessors. Since a query processor is no longer assigned any fixed part of the\ngraph, it is equally capable of handling any request, thus facilitating load\nbalancing and fault tolerance. On the other hand, due to our smart routing\nstrategies, query processors can effectively leverage their cache contents,\nreducing the overall impact of how the graph is partitioned across storage\nservers. A detailed experimental evaluation with several real-world, large\ngraph datasets demonstrates that our proposed framework, gRouting - even with\nsimple hash partitioning of the data - achieves up to an order of magnitude\nbetter query throughput compared to existing graph querying systems that employ\nexpensive graph partitioning and re-partitioning strategies.\n"]},
{"authors": ["Gourab Mitra", "Shashidhar Sundareisan", "Bikash Kanti Sarkar"], "title": ["A simple data discretizer"], "date": ["2017-10-13T22:45:11Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.05091v1"], "summary": ["  Data discretization is an important step in the process of machine learning,\nsince it is easier for classifiers to deal with discrete attributes rather than\ncontinuous attributes. Over the years, several methods of performing\ndiscretization such as Boolean Reasoning, Equal Frequency Binning, Entropy have\nbeen proposed, explored, and implemented. In this article, a simple supervised\ndiscretization approach is introduced. The prime goal of MIL is to maximize\nclassification accuracy of classifier, minimizing loss of information while\ndiscretization of continuous attributes. The performance of the suggested\napproach is compared with the supervised discretization algorithm Minimum\nInformation Loss (MIL), using the state-of-the-art rule inductive algorithms-\nJ48 (Java implementation of C4.5 classifier). The presented approach is,\nindeed, the modified version of MIL. The empirical results show that the\nmodified approach performs better in several cases in comparison to the\noriginal MIL algorithm and Minimum Description Length Principle (MDLP) .\n"]},
{"authors": ["Afroza Sultana", "Chengkai Li"], "title": ["Continuous Monitoring of Pareto Frontiers on Partially Ordered\n  Attributes for Many Users"], "date": ["2017-09-25T04:29:54Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.08312v2"], "summary": ["  We study the problem of continuous object dissemination---given a large\nnumber of users and continuously arriving new objects, deliver an object to all\nusers who prefer the object. Many real world applications analyze users'\npreferences for effective object dissemination. For continuously arriving\nobjects, timely finding users who prefer a new object is challenging. In this\npaper, we consider an append-only table of objects with multiple attributes and\nusers' preferences on individual attributes are modeled as strict partial\norders. An object is preferred by a user if it belongs to the Pareto frontier\nwith respect to the user's partial orders. Users' preferences can be similar.\nExploiting shared computation across similar preferences of different users, we\ndesign algorithms to find target users of a new object. In order to find users\nof similar preferences, we study the novel problem of clustering users'\npreferences that are represented as partial orders. We also present an\napproximate solution of the problem of finding target users which is more\nefficient than the exact one while ensuring sufficient accuracy. Furthermore,\nwe extend the algorithms to operate under the semantics of sliding window. We\npresent the results from comprehensive experiments for evaluating the\nefficiency and effectiveness of the proposed techniques.\n"]},
{"authors": ["Mohamed S. Hassan", "Tatiana Kuznetsova", "Hyun Chai Jeong", "Walid G. Aref", "Mohammad Sadoghi"], "title": ["Empowering In-Memory Relational Database Engines with Native Graph\n  Processing"], "date": ["2017-09-20T03:51:41Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.06715v2"], "summary": ["  The plethora of graphs and relational data give rise to many interesting\ngraph-relational queries in various domains, e.g., finding related proteins\nsatisfying relational predicates in a biological network. The maturity of\nRDBMSs motivated academia and industry to invest efforts in leveraging RDBMSs\nfor graph processing, where efficiency is proven for vital graph queries.\nHowever, none of these efforts process graphs natively inside the RDBMS, which\nis particularly challenging due to the impedance mismatch between the\nrelational and the graph models. In this paper, we propose to treat graphs as\nfirst-class citizens inside the relational engine so that operations on graphs\nare executed natively inside the RDBMS. We realize our approach inside VoltDB,\nan open-source in-memory relational database, and name this realization\nGRFusion. The SQL and the query engine of GRFusion are empowered to\ndeclaratively define graphs and execute cross-data-model query plans formed by\ngraph and relational operators, resulting in up to four orders-of-magnitude in\nquery-time speedup w.r.t. state-of-the-art approaches.\n"]},
{"authors": ["Carlos Baquero", "Paulo Sergio Almeida", "Ali Shoker"], "title": ["Pure Operation-Based Replicated Data Types"], "date": ["2017-10-12T12:18:30Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.04469v1"], "summary": ["  Distributed systems designed to serve clients across the world often make use\nof geo-replication to attain low latency and high availability. Conflict-free\nReplicated Data Types (CRDTs) allow the design of predictable multi-master\nreplication and support eventual consistency of replicas that are allowed to\ntransiently diverge. CRDTs come in two flavors: state-based, where a state is\nchanged locally and shipped and merged into other replicas; operation-based,\nwhere operations are issued locally and reliably causal broadcast to all other\nreplicas. However, the standard definition of op-based CRDTs is very\nencompassing, allowing even sending the full-state, and thus imposing storage\nand dissemination overheads as well as blurring the distinction from\nstate-based CRDTs. We introduce pure op-based CRDTs, that can only send\noperations to other replicas, drawing a clear distinction from state-based\nones. Data types with commutative operations can be trivially implemented as\npure op-based CRDTs using standard reliable causal delivery; whereas data types\nhaving non-commutative operations are implemented using a PO-Log, a partially\nordered log of operations, and making use of an extended API, i.e., a Tagged\nCausal Stable Broadcast (TCSB), that provides extra causality information upon\ndelivery and later informs when delivered messages become causally stable,\nallowing further PO-Log compaction. The framework is illustrated by a catalog\nof pure op-based specifications for classic CRDTs, including counters,\nmulti-value registers, add-wins and remove-wins sets.\n"]},
{"authors": ["Jakub Michaliszyn", "Jan Otop", "Piotr Wieczorek"], "title": ["Querying Best Paths in Graph Databases"], "date": ["2017-10-12T09:30:34Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.04419v1"], "summary": ["  Querying graph databases has recently received much attention. We propose a\nnew approach to this problem, which balances competing goals of expressive\npower, language clarity and computational complexity. A distinctive feature of\nour approach is the ability to express properties of minimal (e.g. shortest)\nand maximal (e.g. most valuable) paths satisfying given criteria. To express\ncomplex properties in a modular way, we introduce labelling-generating\nontologies. The resulting formalism is computationally attractive -- queries\ncan be answered in non-deterministic logarithmic space in the size of the\ndatabase.\n"]},
{"authors": ["David Chapela-Campa", "Manuel Mucientes", "Manuel Lama"], "title": ["Mining Frequent Patterns in Process Models"], "date": ["2017-10-11T18:33:19Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.05693v1"], "summary": ["  Process mining has emerged as a way to analyze the behavior of an\norganization by extracting knowledge from event logs and by offering techniques\nto discover, monitor and enhance real processes. In the discovery of process\nmodels, retrieving a complex one, i.e., a hardly readable process model, can\nhinder the extraction of information. Even in well-structured process models,\nthere is information that cannot be obtained with the current techniques. In\nthis paper, we present WoMine, an algorithm to retrieve frequent behavioural\npatterns from the model. Our approach searches in process models extracting\nstructures with sequences, selections, parallels and loops, which are\nfrequently executed in the logs. This proposal has been validated with a set of\nprocess models, including some from BPI Challenges, and compared with the state\nof the art techniques. Experiments have validated that WoMine can find all\ntypes of patterns, extracting information that cannot be mined with the state\nof the art techniques.\n"]},
{"authors": ["Booma Sowkarthiga Balasubramani", "Omar Belingheri", "Eric S. Boria", "Isabel F. Cruz", "Sybil Derrible", "Michael D. Siciliano"], "title": ["GUIDES - Geospatial Urban Infrastructure Data Engineering Solutions"], "date": ["2017-10-11T15:58:21Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.04144v1"], "summary": ["  As the underground infrastructure systems of cities age, maintenance and\nrepair become an increasing concern. Cities face difficulties in planning\nmaintenance, predicting and responding to infrastructure related issues, and in\nrealizing their vision to be a smart city due to their incomplete understanding\nof the existing state of the infrastructure. Only few cities have accurate and\ncomplete digital information on their underground infrastructure (e.g.,\nelectricity, water, natural gas) systems, which poses problems to those\nplanning and performing construction projects. To address these issues, we\nintroduce GUIDES as a new data conversion and management framework for urban\nunderground infrastructure systems that enable city administrators, workers,\nand contractors along with the general public and other users to query\ndigitized and integrated data to make smarter decisions. This demo paper\npresents the GUIDES architecture and describes two of its central components:\n(i) mapping of underground infrastructure systems, and (ii) integration of\nheterogeneous geospatial data.\n"]},
{"authors": ["Robin Haunschild", "Sven E. Hug", "Martin P. Br\u00e4ndle", "Lutz Bornmann"], "title": ["The number of linked references of publications in Microsoft Academic in\n  comparison with the Web of Science"], "date": ["2017-10-11T12:15:55Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.04031v1"], "summary": ["  In the context of a comprehensive Microsoft Academic (MA) study, we explored\nin an initial step the quality of linked references data in MA in comparison\nwith Web of Science (WoS). Linked references are the backbone of bibliometrics,\nbecause they are the basis of the times cited information in citation indexes.\nWe found that the concordance of linked references between MA and WoS ranges\nfrom weak to non-existent for the full sample (publications of the University\nof Zurich with less than 50 linked references in MA). An analysis with a sample\nrestricted to less than 50 linked references in WoS showed a strong agreement\nbetween linked references in MA and WoS.\n"]},
{"authors": ["Hongwei Liang", "Ke Wang"], "title": ["Constructing Top-k Routes with Personalized Submodular Maximization of\n  POI Features"], "date": ["2017-10-10T23:00:38Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.03852v1"], "summary": ["  We consider a practical top-k route problem: given a collection of points of\ninterest (POIs) with rated features and traveling costs between POIs, a user\nwants to find k routes from a source to a destination, that maximally match her\nneeds on feature preferences and can be completed within a travel cost budget.\nOne challenge is dealing with the personalized diversity requirement where each\nuser has a different trade-off between quantity (the number of POIs with a\nspecified feature) and variety (the coverage of specified features). Another\nchallenge is the large scale of the POI network and the great many alternative\nroutes to search. We model the personalized diversity requirement by the whole\nclass of submodular functions, and present an optimal solution to the top-k\nroute problem through an index structure for retrieving relevant POIs in both\nfeature and route spaces and various strategies for pruning the search space\nusing user preferences and constraints. We also present heuristic solutions and\nevaluate all the solutions on real life POI network data.\n"]},
{"authors": ["Yuqing Zhu", "Jianxun Liu", "Mengying Guo", "Yungang Bao", "Wenlong Ma", "Zhuoyue Liu", "Kunpeng Song", "Yingchun Yang"], "title": ["BestConfig: Tapping the Performance Potential of Systems via Automatic\n  Configuration Tuning"], "date": ["2017-10-10T08:10:06Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.03439v1"], "summary": ["  An ever increasing number of configuration parameters are provided to system\nusers. But many users have used one configuration setting across different\nworkloads, leaving untapped the performance potential of systems. A good\nconfiguration setting can greatly improve the performance of a deployed system\nunder certain workloads. But with tens or hundreds of parameters, it becomes a\nhighly costly task to decide which configuration setting leads to the best\nperformance. While such task requires the strong expertise in both the system\nand the application, users commonly lack such expertise.\n  To help users tap the performance potential of systems, we present\nBestConfig, a system for automatically finding a best configuration setting\nwithin a resource limit for a deployed system under a given application\nworkload. BestConfig is designed with an extensible architecture to automate\nthe configuration tuning for general systems. To tune system configurations\nwithin a resource limit, we propose the divide-and-diverge sampling method and\nthe recursive bound-and-search algorithm. BestConfig can improve the throughput\nof Tomcat by 75%, that of Cassandra by 63%, that of MySQL by 430%, and reduce\nthe running time of Hive join job by about 50% and that of Spark join job by\nabout 80%, solely by configuration adjustment.\n"]},
{"authors": ["Eirini Molla", "Theodoros Tzouramanis", "Stefanos Gritzalis"], "title": ["SOPE: A Spatial Order Preserving Encryption Model for Multi-dimensional\n  Data"], "date": ["2017-10-09T20:55:50Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.09420v1"], "summary": ["  Due to the increasing demand for cloud services and the threat of privacy\ninvasion, the user is suggested to encrypt the data before it is outsourced to\nthe remote server. The safe storage and efficient retrieval of d-dimensional\ndata on an untrusted server has therefore crucial importance. The paper\nproposes a new encryption model which offers spatial order-preservation for\nd-dimensional data (SOPE model). The paper studies the operations for the\nconstruction of the encrypted database and suggests algorithms that exploit\nunique properties that this new model offers for the efficient execution of a\nwhole range of well-known queries over the encrypted d-dimensional data. The\nnew model utilizes well-known database indices, such as the B+-tree and the\nR-tree, as backbone structures in their traditional form, as it suggests no\nmodifications to them for loading the data and for the efficient execution of\nthe supporting query algorithms. An extensive experimental study that is also\npresented in the paper indicates the effectiveness and practicability of the\nproposed encryption model for real-life d-dimensional data applications.\n"]},
{"authors": ["Rosana Veroneze", "Fernando J. Von Zuben"], "title": ["Efficient mining of maximal biclusters in mixed-attribute datasets"], "date": ["2017-10-09T19:59:19Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.03289v1"], "summary": ["  This paper presents a novel enumerative biclustering algorithm to directly\nmine all maximal biclusters in mixed-attribute datasets (containing both\nnumerical and categorical attributes), with or without missing values. The\nproposal is an extension of RIn-Close_CVC, which was originally conceived to\nmine perfect or perturbed biclusters with constant values on columns solely\nfrom numerical datasets, and without missing values. Even endowed with\nadditional and more general features, the extended RIn-Close_CVC retains four\nkey properties: (1) efficiency, (2) completeness, (3) correctness, and (4)\nnon-redundancy. Our proposal is the first one to deal with mixed-attribute\ndatasets without requiring any pre-processing step, such as discretization and\nitemization of real-valued attributes. This is a decisive aspect, because\ndiscretization and itemization implies a priori decisions, with information\nloss and no clear control over the consequences. On the other hand, even having\nto specify a priori an individual threshold for each numerical attribute, that\nwill be used to indicate internal consistency per attribute, each threshold\nwill be applied during the construction of the biclusters, shaping the\npeculiarities of the data distribution. We also explore the strong connection\nbetween biclustering and frequent pattern mining to (1) provide filters to\nselect a compact bicluster set that exhibits high relevance and low redundancy,\nand (2) in the case of labeled datasets, automatically present the biclusters\nin a user-friendly and intuitive form, by means of quantitative class\nassociation rules. Our experimental results showed that the biclusters yield a\nparsimonious set of relevant rules, providing useful and interpretable models\nfor five mixed-attribute labeled datasets.\n"]},
{"authors": ["Zhuoyue Zhao", "Jialing Pei", "Eric Lo", "Kenny Q. Zhu", "Chris Liu"], "title": ["InferSpark: Statistical Inference at Scale"], "date": ["2017-07-07T06:02:50Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1707.02047v2"], "summary": ["  The Apache Spark stack has enabled fast large-scale data processing. Despite\na rich library of statistical models and inference algorithms, it does not give\ndomain users the ability to develop their own models. The emergence of\nprobabilistic programming languages has showed the promise of developing\nsophisticated probabilistic models in a succinct and programmatic way. These\nframeworks have the potential of automatically generating inference algorithms\nfor the user defined models and answering various statistical queries about the\nmodel. It is a perfect time to unite these two great directions to produce a\nprogrammable big data analysis framework. We thus propose, InferSpark, a\nprobabilistic programming framework on top of Apache Spark. Efficient\nstatistical inference can be easily implemented on this framework and inference\nprocess can leverage the distributed main memory processing power of Spark.\nThis framework makes statistical inference on big data possible and speed up\nthe penetration of probabilistic programming into the data engineering domain.\n"]},
{"authors": ["Kasun Bandara", "Christoph Bergmeir", "Slawek Smyl"], "title": ["Forecasting Across Time Series Databases using Long Short-Term Memory\n  Networks on Groups of Similar Series"], "date": ["2017-10-09T04:08:15Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.03222v1"], "summary": ["  With the advent of Big Data, nowadays in many applications databases\ncontaining large quantities of similar time series are available. Forecasting\ntime series in these domains with traditional univariate forecasting procedures\nleaves great potentials for producing accurate forecasts untapped. Recurrent\nneural networks, and in particular Long Short-Term Memory (LSTM) networks have\nproven recently that they are able to outperform state-of-the-art univariate\ntime series forecasting methods in this context, when trained across all\navailable time series. However, if the time series database is heterogeneous\naccuracy may degenerate, so that on the way towards fully automatic forecasting\nmethods in this space, a notion of similarity between the time series needs to\nbe built into the methods. To this end, we present a prediction model using\nLSTMs on subgroups of similar time series, which are identified by time series\nclustering techniques. The proposed methodology is able to consistently\noutperform the baseline LSTM model, and it achieves competitive results on\nbenchmarking datasets, in particular outperforming all other methods on the\nCIF2016 dataset.\n"]},
{"authors": ["Markku Hinkka", "Teemu Lehto", "Keijo Heljanko", "Alexander Jung"], "title": ["Structural Feature Selection for Event Logs"], "date": ["2017-10-08T11:38:37Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.02823v1"], "summary": ["  We consider the problem of classifying business process instances based on\nstructural features derived from event logs. The main motivation is to provide\nmachine learning based techniques with quick response times for interactive\ncomputer assisted root cause analysis. In particular, we create structural\nfeatures from process mining such as activity and transition occurrence counts,\nand ordering of activities to be evaluated as potential features for\nclassification. We show that adding such structural features increases the\namount of information thus potentially increasing classification accuracy.\nHowever, there is an inherent trade-off as using too many features leads to too\nlong run-times for machine learning classification models. One way to improve\nthe machine learning algorithms' run-time is to only select a small number of\nfeatures by a feature selection algorithm. However, the run-time required by\nthe feature selection algorithm must also be taken into account. Also, the\nclassification accuracy should not suffer too much from the feature selection.\nThe main contributions of this paper are as follows: First, we propose and\ncompare six different feature selection algorithms by means of an experimental\nsetup comparing their classification accuracy and achievable response times.\nSecond, we discuss the potential use of feature selection results for computer\nassisted root cause analysis as well as the properties of different types of\nstructural features in the context of feature selection.\n"]},
{"authors": ["Jizhou Sun", "Jianzhong Li", "Hong Gao"], "title": ["Discovery of Paradigm Dependencies"], "date": ["2017-10-08T09:53:57Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.02817v1"], "summary": ["  Missing and incorrect values often cause serious consequences. To deal with\nthese data quality problems, a class of common employed tools are dependency\nrules, such as Functional Dependencies (FDs), Conditional Functional\nDependencies (CFDs) and Edition Rules (ERs), etc. The stronger expressing\nability a dependency has, data with the better quality can be obtained. To the\nbest of our knowledge, all previous dependencies treat each attribute value as\na non-splittable whole. Actually however, in many applications, part of a value\nmay contains meaningful information, indicating that more powerful dependency\nrules to handle data quality problems are possible. In this paper, we consider\nof discovering such type of dependencies in which the left hand side is part of\na regular-expression-like paradigm, named Paradigm Dependencies (PDs). PDs tell\nthat if a string matches the paradigm, element at the specified position can\ndecides a certain other attribute's value. We propose a framework in which\nstrings with similar coding rules and different lengths are clustered together\nand aligned vertically, from which PDs can be discovered directly. The aligning\nproblem is the key component of this framework and is proved in NP-Complete. A\ngreedy algorithm is introduced in which the clustering and aligning tasks can\nbe accomplished simultaneously. Because of the greedy algorithm's high time\ncomplexity, several pruning strategies are proposed to reduce the running time.\nIn the experimental study, three real datasets as well as several synthetical\ndatasets are employed to verify our methods' effectiveness and efficiency.\n"]},
{"authors": ["Beidi Chen", "Anshumali Shrivastava", "Rebecca C. Steorts"], "title": ["Unique Entity Estimation with Application to the Syrian Conflict"], "date": ["2017-10-07T14:30:20Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.02690v1"], "summary": ["  Entity resolution identifies and removes duplicate entities in large, noisy\ndatabases and has grown in both usage and new developments as a result of\nincreased data availability. Nevertheless, entity resolution has tradeoffs\nregarding assumptions of the data generation process, error rates, and\ncomputational scalability that make it a difficult task for real applications.\nIn this paper, we focus on a related problem of unique entity estimation, which\nis the task of estimating the unique number of entities and associated standard\nerrors in a data set with duplicate entities. Unique entity estimation shares\nmany fundamental challenges of entity resolution, namely, that the\ncomputational cost of all-to-all entity comparisons is intractable for large\ndatabases. To circumvent this computational barrier, we propose an efficient\n(near-linear time) estimation algorithm based on locality sensitive hashing.\nOur estimator, under realistic assumptions, is unbiased and has provably low\nvariance compared to existing random sampling based approaches. In addition, we\nempirically show its superiority over the state-of-the-art estimators on three\nreal applications. The motivation for our work is to derive an accurate\nestimate of the documented, identifiable deaths in the ongoing Syrian conflict.\nOur methodology, when applied to the Syrian data set, provides an estimate of\n$191,874 \\pm 1772$ documented, identifiable deaths, which is very close to the\nHuman Rights Data Analysis Group (HRDAG) estimate of 191,369. Our work provides\nan example of challenges and efforts involved in solving a real, noisy\nchallenging problem where modeling assumptions may not hold.\n"]},
{"authors": ["Haoyu Zhang", "Qin Zhang"], "title": ["EmbedJoin: Efficient Edit Similarity Joins via Embeddings"], "date": ["2017-02-01T00:39:18Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.00093v3"], "summary": ["  We study the problem of edit similarity joins, where given a set of strings\nand a threshold value $K$, we want to output all pairs of strings whose edit\ndistances are at most $K$. Edit similarity join is a fundamental problem in\ndata cleaning/integration, bioinformatics, collaborative filtering and natural\nlanguage processing, and has been identified as a primitive operator for\ndatabase systems. This problem has been studied extensively in the literature.\nHowever, we have observed that all the existing algorithms fall short on long\nstrings and large distance thresholds.\n  In this paper we propose an algorithm named EmbedJoin which scales very well\nwith string length and distance threshold. Our algorithm is built on the recent\nadvance of metric embeddings for edit distance, and is very different from all\nof the previous approaches. We demonstrate via an extensive set of experiments\nthat EmbedJoin significantly outperforms the previous best algorithms on long\nstrings and large distance thresholds.\n"]},
{"authors": ["Wim Martens", "Tina Trautner"], "title": ["Enumeration Problems for Regular Path Queries"], "date": ["2017-10-06T09:04:45Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.02317v1"], "summary": ["  Evaluation of regular path queries (RPQs) is a central problem in graph\ndatabases. We investigate the corresponding enumeration problem, that is, given\na graph and an RPQ, enumerate all paths in the graph that match the RPQ. We\nconsider several versions of this problem, corresponding to different semantics\nof RPQs that have recently been considered: arbitrary paths, shortest paths,\nsimple paths, and trails. Whereas arbitrary and shortest paths can be\nenumerated in polynomial delay, the situation is much more intricate for simple\npaths and trails. For instance, already the question if a given graph contains\na simple path or trail of a certain length has cases with highly non-trivial\nsolutions and cases that are long-standing open problems. In this setting, we\nstudy RPQ evaluation from a parameterized complexity perspective. We define a\nclass of simple transitive expressions that is prominent in practice and for\nwhich we can prove two dichotomy-like results: one for simple paths and one for\ntrails paths. We observe that, even though simple path semantics and trail\nsemantics are intractable for RPQs in general, they are feasible for the vast\nmajority of the kinds of RPQs that users use in practice. At the heart of this\nstudy is a result of independent interest on the parameterized complexity of\nfinding disjoint paths in graphs: the two disjoint paths problem is W[1]-hard\nif parameterized by the length of one of the two paths.\n"]},
{"authors": ["Abolfazl Asudeh", "Azade Nazi", "Nick Koudas", "Gautam Das"], "title": ["Assisting Service Providers In Peer-to-peer Marketplaces: Maximizing\n  Gain Over Flexible Attributes"], "date": ["2017-05-08T18:09:00Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1705.03028v2"], "summary": ["  Peer to peer marketplaces such as AirBnB enable transactional exchange of\nservices directly between people. In such platforms, those providing a service\n(hosts in AirBnB) are faced with various choices. For example in AirBnB,\nalthough some amenities in a property (attributes of the property) are fixed,\nothers are relatively flexible and can be provided without significant effort.\nProviding an amenity is usually associated with a cost. Naturally different\nsets of amenities may have a different \"gains\" for a host. Consequently, given\na limited budget, deciding which amenities (attributes) to offer is\nchallenging.\n  In this paper, we formally introduce and define the problem of Gain\nMaximization over Flexible Attributes (GMFA). We first prove that the problem\nis NP-hard and show that identifying an approximate algorithm with a constant\napproximate ratio is unlikely. We then provide a practically efficient exact\nalgorithm to the GMFA problem for the general class of monotonic gain\nfunctions, which quantify the benefit of sets of attributes. As the next part\nof our contribution, we focus on the design of a practical gain function for\nGMFA. We introduce the notion of frequent-item based count (FBC), which\nutilizes the existing tuples in the database to define the notion of gain, and\npropose an efficient algorithm for computing it. We present the results of a\ncomprehensive experimental evaluation of the proposed techniques on real\ndataset from AirBnB and demonstrate the practical relevance and utility of our\nproposal.\n"]},
{"authors": ["Mangesh Bendre", "Vipul Venkataraman", "Xinyan Zhou", "Kevin Chang", "Aditya Parameswaran"], "title": ["Towards a Holistic Integration of Spreadsheets with Databases: A\n  Scalable Storage Engine for Presentational Data Management"], "date": ["2017-08-22T16:37:22Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1708.06712v2"], "summary": ["  Spreadsheet software is the tool of choice for interactive ad-hoc data\nmanagement, with adoption by billions of users. However, spreadsheets are not\nscalable, unlike database systems. On the other hand, database systems, while\nhighly scalable, do not support interactivity as a first-class primitive. We\nare developing DataSpread, to holistically integrate spreadsheets as a\nfront-end interface with databases as a back-end datastore, providing\nscalability to spreadsheets, and interactivity to databases, an integration we\nterm presentational data management (PDM). In this paper, we make a first step\ntowards this vision: developing a storage engine for PDM, studying how to\nflexibly represent spreadsheet data within a database and how to support and\nmaintain access by position. We first conduct an extensive survey of\nspreadsheet use to motivate our functional requirements for a storage engine\nfor PDM. We develop a natural set of mechanisms for flexibly representing\nspreadsheet data and demonstrate that identifying the optimal representation is\nNP-Hard; however, we develop an efficient approach to identify the optimal\nrepresentation from an important and intuitive subclass of representations. We\nextend our mechanisms with positional access mechanisms that don't suffer from\ncascading update issues, leading to constant time access and modification\nperformance. We evaluate these representations on a workload of typical\nspreadsheets and spreadsheet operations, providing up to 20% reduction in\nstorage, and up to 50% reduction in formula evaluation time.\n"]},
{"authors": ["Niranjan Kamat", "Arnab Nandi"], "title": ["InfiniViz: Interactive Visual Exploration using Progressive Bin\n  Refinement"], "date": ["2017-10-05T01:55:13Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.01854v1"], "summary": ["  Interactive visualizations can accelerate the data analysis loop through\nnear-instantaneous feedback. To achieve interactivity, techniques such as data\ncubes and sampling are typically employed. While data cubes can speedup\nquerying for moderate-sized datasets, they are ineffective at doing so at a\nlarger scales due to the size of the materialized data cubes. On the other\nhand, while sampling can help scale to large datasets, it adds sampling error\nand the associated issues into the process.\n  While increasing accuracy by looking at more data may sometimes be valuable,\nproviding result minutiae might not be necessary if they do not impart\nadditional significant information. Indeed, such details not only incur a\nhigher \\emph{computational} cost, but also tax the \\emph{cognitive} load of the\nanalyst with worthless trivia. To reduce both the computational and cognitive\nexpenses, we introduce \\emph{InfiniViz}. Through a novel result\nrefinement-based querying paradigm, \\emph{InfiniViz} provides error-free\nresults for large datasets by increasing bin resolutions progressively over\ntime. Through real and simulated workloads over real and benchmark datasets, we\nevaluate and demonstrate \\emph{InfiniViz}'s utility at reducing both cognitive\nand computational costs, while minimizing information loss.\n"]},
{"authors": ["Ashish Tapdiya", "Yuan Xue", "Daniel Fabbri"], "title": ["A Comparative Analysis of Materialized Views Selection and Concurrency\n  Control Mechanisms in NoSQL Databases"], "date": ["2017-10-04T20:24:57Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.01792v1"], "summary": ["  Increasing resource demands require relational databases to scale. While\nrelational databases are well suited for vertical scaling, specialized hardware\ncan be expensive. Conversely, emerging NewSQL and NoSQL data stores are\ndesigned to scale horizontally. NewSQL databases provide ACID transaction\nsupport; however, joins are limited to the partition keys, resulting in\nrestricted query expressiveness. On the other hand, NoSQL databases are\ndesigned to scale out linearly on commodity hardware; however, they are limited\nby slow join performance. Hence, we consider if the NoSQL join performance can\nbe improved while ensuring ACID semantics and without drastically sacrificing\nwrite performance, disk utilization and query expressiveness.\n  This paper presents the Synergy system that leverages schema and workload\ndriven mechanism to identify materialized views and a specialized concurrency\ncontrol system on top of a NoSQL database to enable scalable data management\nwith familiar relational conventions. Synergy trades slight write performance\ndegradation and increased disk utilization for faster join performance\n(compared to standard NoSQL databases) and improved query expressiveness\n(compared to NewSQL databases). Experimental results using the TPC-W benchmark\nshow that, for a database populated with 1M customers, the Synergy system\nexhibits a maximum performance improvement of 80.5% as compared to other\nevaluated systems.\n"]},
{"authors": ["Ahmed R. Mahmood", "Ahmed M. Aly", "Walid G. Aref"], "title": ["FAST: Frequency-Aware Spatio-Textual Indexing for In-Memory Continuous\n  Filter Query Processing"], "date": ["2017-09-08T04:22:41Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.02529v2"], "summary": ["  Many applications need to process massive streams of spatio-textual data in\nreal-time against continuous spatio-textual queries. For example, in\nlocation-aware ad targeting publish/subscribe systems, it is required to\ndisseminate millions of ads and promotions to millions of users based on the\nlocations and textual profiles of users. In this paper, we study indexing of\ncontinuous spatio-textual queries. There exist several related spatio-textual\nindexes that typically integrate a spatial index with a textual index. However,\nthese indexes usually have a high demand for main-memory and assume that the\nentire vocabulary of keywords is known in advance. Also, these indexes do not\nsuccessfully capture the variations in the frequencies of keywords across\ndifferent spatial regions and treat frequent and infrequent keywords in the\nsame way. Moreover, existing indexes do not adapt to the changes in workload\nover space and time. For example, some keywords may be trending at certain\ntimes in certain locations and this may change as time passes. This affects the\nindexing and searching performance of existing indexes significantly. In this\npaper, we introduce FAST, a Frequency-Aware Spatio-Textual index for continuous\nspatio-textual queries. FAST is a main-memory index that requires up to one\nthird of the memory needed by the state-of-the-art index. FAST does not assume\nprior knowledge of the entire vocabulary of indexed objects. FAST adaptively\naccounts for the difference in the frequencies of keywords within their\ncorresponding spatial regions to automatically choose the best indexing\napproach that optimizes the insertion and search times. Extensive experimental\nevaluation using real and synthetic datasets demonstrates that FAST is up to 3x\nfaster in search time and 5x faster in insertion time than the state-of-the-art\nindexes.\n"]},
{"authors": ["Naoise Holohan", "Spiros Antonatos", "Stefano Braghin", "P\u00f3l Mac Aonghusa"], "title": ["($k$,$\u03b5$)-Anonymity: $k$-Anonymity with $\u03b5$-Differential\n  Privacy"], "date": ["2017-10-04T14:25:39Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.01615v1"], "summary": ["  The explosion in volume and variety of data offers enormous potential for\nresearch and commercial use. Increased availability of personal data is of\nparticular interest in enabling highly customised services tuned to individual\nneeds. Preserving the privacy of individuals against reidentification attacks\nin this fast-moving ecosystem poses significant challenges for a one-size fits\nall approach to anonymisation.\n  In this paper we present ($k$,$\\epsilon$)-anonymisation, an approach that\ncombines the $k$-anonymisation and $\\epsilon$-differential privacy models into\na single coherent framework, providing privacy guarantees at least as strong as\nthose offered by the individual models. Linking risks of less than 5\\% are\nobserved in experimental results, even with modest values of $k$ and\n$\\epsilon$.\n  Our approach is shown to address well-known limitations of $k$-anonymity and\n$\\epsilon$-differential privacy and is validated in an extensive experimental\ncampaign using openly available datasets.\n"]},
{"authors": ["Jose Picado", "Sudhanshu Pathak", "Arash Termehchy", "Alan Fern"], "title": ["AutoMode: Relational Learning With Less Black Magic"], "date": ["2017-10-03T23:36:31Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.01420v1"], "summary": ["  Relational databases are valuable resources for learning novel and\ninteresting relations and concepts. Relational learning algorithms learn the\nDatalog definition of new relations in terms of the existing relations in the\ndatabase. In order to constraint the search through the large space of\ncandidate definitions, users must tune the algorithm by specifying a language\nbias. Unfortunately, specifying the language bias is done via trial and error\nand is guided by the expert's intuitions. Hence, it normally takes a great deal\nof time and effort to effectively use these algorithms. In particular, it is\nhard to find a user that knows computer science concepts, such as database\nschema, and has a reasonable intuition about the target relation in special\ndomains, such as biology. We propose AutoMode, a system that leverages\ninformation in the schema and content of the database to automatically induce\nthe language bias used by popular relational learning systems. We show that\nAutoMode delivers the same accuracy as using manually-written language bias by\nimposing only a slight overhead on the running time of the learning algorithm.\n"]},
{"authors": ["Noman Islam", "Zubair A. Shaikh", " Aqeel-ur-Rehman", "Muhammad Shahab Siddiqui"], "title": ["HANDY: A Hybrid Association Rules Mining Approach for Network Layer\n  Discovery of Services for Mobile Ad hoc Network"], "date": ["2017-10-03T14:49:11Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.02035v1"], "summary": ["  Mobile Ad hoc Network (MANET) is an infrastructure-less network formed\nbetween a set of mobile nodes. The discovery of services in MANET is a\nchallenging job due to the unique properties of network. In this paper, a novel\nservice discovery framework called Hybrid Association Rules Based Network Layer\nDiscovery of Services for Ad hoc Networks (HANDY) has been proposed. HANDY\nprovides three major research contributions. At first, it adopts a cross-layer\noptimized design for discovery of services that is based on simultaneous\ndiscovery of services and corresponding routes. Secondly, it provides a\nmulti-level ontology-based approach to describe the services. This resolves the\nissue of semantic interoperability among the service consumers in a scalable\nfashion. Finally, to further optimize the performance of the discovery process,\nHANDY recommends exploiting the inherent associations present among the\nservices. These associations are used in two ways. First, periodic service\nadvertisements are performed based on these associations. In addition, when a\nresponse of a service discovery request is generated, correlated services are\nalso attached with the response. The proposed service discovery scheme has been\nimplemented in JIST/SWANS simulator. The results demonstrate that the proposed\nmodifications give rise to improvement in hit ratio of the service consumers\nand latency of discovery process.\n"]},
{"authors": ["S\u00f8ren Kejser Jensen", "Torben Bach Pedersen", "Christian Thomsen"], "title": ["Time Series Management Systems: A Survey"], "date": ["2017-10-03T11:16:55Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.01077v1"], "summary": ["  The collection of time series data increases as more monitoring and\nautomation are being deployed. These deployments range in scale from an\nInternet of things (IoT) device located in a household to enormous distributed\nCyber-Physical Systems (CPSs) producing large volumes of data at high velocity.\nTo store and analyze these vast amounts of data, specialized Time Series\nManagement Systems (TSMSs) have been developed to overcome the limitations of\ngeneral purpose Database Management Systems (DBMSs) for times series\nmanagement. In this paper, we present a thorough analysis and classification of\nTSMSs developed through academic or industrial research and documented through\npublications. Our classification is organized into categories based on the\narchitectures observed during our analysis. In addition, we provide an overview\nof each system with a focus on the motivational use case that drove the\ndevelopment of the system, the functionality for storage and querying of time\nseries a system implements, the components the system is composed of, and the\ncapabilities of each system with regard to Stream Processing and Approximate\nQuery Processing (AQP). Last, we provide a summary of research directions\nproposed by other researchers in the field and present our vision for a next\ngeneration TSMS.\n"]},
{"authors": ["Evaggelia Pitoura", "Panayiotis Tsaparas", "Giorgos Flouris", "Irini Fundulaki", "Panagiotis Papadakos", "Serge Abiteboul", "Gerhard Weikum"], "title": ["On Measuring Bias in Online Information"], "date": ["2017-04-19T13:41:50Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1704.05730v2"], "summary": ["  Bias in online information has recently become a pressing issue, with search\nengines, social networks and recommendation services being accused of\nexhibiting some form of bias. In this vision paper, we make the case for a\nsystematic approach towards measuring bias. To this end, we discuss formal\nmeasures for quantifying the various types of bias, we outline the system\ncomponents necessary for realizing them, and we highlight the related research\nchallenges and open problems.\n"]},
{"authors": ["Doris Jung-Lin Lee", "John Lee", "Tarique Siddiqui", "Jaewoo Kim", "Karrie Karahalios", "Aditya Parameswaran"], "title": ["Accelerating Scientific Data Exploration via Visual Query Systems"], "date": ["2017-10-02T16:31:24Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.00763v2"], "summary": ["  The increasing availability of rich and complex data in a variety of\nscientific domains poses a pressing need for tools to enable scientists to\nrapidly make sense of and gather insights from data. One proposed solution is\nto design visual query systems (VQSs) that allow scientists to search for\ndesired patterns in their datasets. While many existing VQSs promise to\naccelerate exploratory data analysis by facilitating this search, they are\nunfortunately not widely used in practice. Through a year-long collaboration\nwith scientists in three distinct domains---astronomy, genetics, and material\nscience---we study the impact of various features within VQSs that can aid\nrapid visual data analysis, and how VQSs fit into a scientists' analysis\nworkflow. Our findings offer design guidelines for improving the usability and\nadoption of next-generation VQSs, paving the way for VQSs to be applied to a\nvariety of scientific domains.\n"]},
{"authors": ["Shufeng Gong", "Yanfeng Zhang", "Ge Yu"], "title": ["Clustering Stream Data by Exploring the Evolution of Density Mountain"], "date": ["2017-10-02T18:59:09Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.00867v1"], "summary": ["  Stream clustering is a fundamental problem in many streaming data analysis\napplications. Comparing to classical batch-mode clustering, there are two key\nchallenges in stream clustering: (i) Given that input data are changing\ncontinuously, how to incrementally update clustering results efficiently? (ii)\nGiven that clusters continuously evolve with the evolution of data, how to\ncapture the cluster evolution activities? Unfortunately, most of existing\nstream clustering algorithms can neither update the cluster result in real time\nnor track the evolution of clusters.\n  In this paper, we propose an stream clustering algorithm EDMStream by\nexploring the Evolution of Density Mountain. The density mountain is used to\nabstract the data distribution, the changes of which indicate data distribution\nevolution. We track the evolution of clusters by monitoring the changes of\ndensity mountains. We further provide efficient data structures and filtering\nschemes to ensure the update of density mountains in real time, which makes\nonline clustering possible. The experimental results on synthetic and real\ndatasets show that, comparing to the state-of-the-art stream clustering\nalgorithms, e.g., D-Stream, DenStream, DBSTREAM and MR-Stream, our algorithm\ncan response to a cluster update much faster (say 7-15x faster than the best of\nthe competitors) and at the same time achieve comparable cluster quality.\nFurthermore, EDMStream can successfully capture the cluster evolution\nactivities.\n"]},
{"authors": ["Graham Cormode", "Tejas Kulkarni", "Divesh Srivastava"], "title": ["Constrained Differential Privacy for Count Data"], "date": ["2017-10-02T12:37:48Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.00608v1"], "summary": ["  Concern about how to aggregate sensitive user data without compromising\nindividual privacy is a major barrier to greater availability of data. The\nmodel of differential privacy has emerged as an accepted model to release\nsensitive information while giving a statistical guarantee for privacy. Many\ndifferent algorithms are possible to address different target functions. We\nfocus on the core problem of count queries, and seek to design mechanisms to\nrelease data associated with a group of n individuals. Prior work has focused\non designing mechanisms by raw optimization of a loss function, without regard\nto the consequences on the results. This can leads to mechanisms with\nundesirable properties, such as never reporting some outputs (gaps), and\noverreporting others (spikes). We tame these pathological behaviors by\nintroducing a set of desirable properties that mechanisms can obey. Any\ncombination of these can be satisfied by solving a linear program (LP) which\nminimizes a cost function, with constraints enforcing the properties. We focus\non a particular cost function, and provide explicit constructions that are\noptimal for certain combinations of properties, and show a closed form for\ntheir cost. In the end, there are only a handful of distinct optimal mechanisms\nto choose between: one is the well-known (truncated) geometric mechanism; the\nsecond a novel mechanism that we introduce here, and the remainder are found as\nthe solution to particular LPs. These all avoid the bad behaviors we identify.\nWe demonstrate in a set of experiments on real and synthetic data which is\npreferable in practice, for different combinations of data distributions,\nconstraints, and privacy parameters.\n"]},
{"authors": ["Jiaye Wu", "Peng Wang", "Chen Wang", "Wei Wang", "Jianmin Wang"], "title": ["KV-match: An Efficient Subsequence Matching Approach for Large Scale\n  Time Series"], "date": ["2017-10-02T09:59:50Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.00560v1"], "summary": ["  Time series data have exploded due to the popularity of new applications,\nlike data center management and IoT. Time series data management system (TSDB),\nemerges to store and query the large volume of time series data. Subsequence\nmatching is critical in many time series mining algorithms, and extensive\napproaches have been proposed. However, the shift of distributed storage system\nand the performance gap make these approaches not compatible with TSDB. To fill\nthis gap, we propose a new index structure, KV-index, and the corresponding\nmatching algorithm, KV-match. KV-index is a file-based structure, which can be\neasily implemented on local files, HDFS or HBase tables. KV-match algorithm\nprobes the index efficiently with a few sequential scans. Moreover, two\noptimization techniques, window reduction and window reordering, are proposed\nto further accelerate the processing. To support the query of arbitrary\nlengths, we extend KV-match to KV-match$_{DP}$, which utilizes multiple varied\nlength indexes to process the query simultaneously. A two-dimensional dynamic\nprogramming algorithm is proposed to find the optimal query segmentation. We\nimplement our approach on both local files and HBase tables, and conduct\nextensive experiments on synthetic and real-world datasets. Results show that\nour index is of comparable size to the popular tree-style index while our query\nprocessing is order of magnitudes more efficient.\n"]},
{"authors": ["Amanpreet Singh", "Karthik Venkatesan", "Simranjyot Singh Gill"], "title": ["Building a Structured Query Engine"], "date": ["2017-10-02T01:54:40Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.00454v1"], "summary": ["  Finding patterns in data and being able to retrieve information from those\npatterns is an important task in Information retrieval. Complex search\nrequirements which are not fulfilled by simple string matching and require\nexploring certain patterns in data demand a better query engine that can\nsupport searching via structured queries. In this article, we built a\nstructured query engine which supports searching data through structured\nqueries on the lines of ElasticSearch. We will show how we achieved real time\nindexing and retrieving of data through a RESTful API and how complex queries\ncan be created and processed using efficient data structures we created for\nstoring the data in structured way. Finally, we will conclude with an example\nof movie recommendation system built on top of this query engine.\n"]},
{"authors": ["Lijun Chang", "Xing Feng", "Xuemin Lin", "Lu Qin", "Wenjie Zhang"], "title": ["Efficient Graph Edit Distance Computation and Verification via\n  Anchor-aware Lower Bound Estimation"], "date": ["2017-09-20T10:55:22Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.06810v2"], "summary": ["  Graph edit distance (GED) is an important similarity measure adopted in a\nsimilarity-based analysis between two graphs, and computing GED is a primitive\noperator in graph database analysis. Partially due to the NP-hardness, the\nexisting techniques for computing GED are only able to process very small\ngraphs with less than 30 vertices. Motivated by this, in this paper we\nsystematically study the problems of both GED computation, and GED verification\n(i.e., verify whether the GED between two graphs is no larger than a user-given\nthreshold). Firstly, we develop a unified framework that can be instantiated\ninto either a best-first search approach AStar+ or a depth-first search\napproach DFS+. Secondly, we design anchor-aware lower bound estimation\ntechniques to compute tighter lower bounds for intermediate search states,\nwhich significantly reduce the search spaces of both AStar+ and DFS+. We also\npropose efficient techniques to compute the lower bounds. Thirdly, based on our\nunified framework, we contrast AStar+ with DFS+ regarding their time and space\ncomplexities, and recommend that AStar+ is better than DFS+ by having a much\nsmaller search space. Extensive empirical studies validate that AStar+ performs\nbetter than DFS+, and show that our AStar+-BMa approach outperforms the\nstate-of-the-art technique by more than four orders of magnitude.\n"]},
{"authors": ["Rong Zhu", "Zhaonian Zou", "Jianzhong Li"], "title": ["Diversified Coherent Core Search on Multi-Layer Graphs"], "date": ["2017-09-27T12:23:54Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.09471v3"], "summary": ["  Mining dense subgraphs on multi-layer graphs is an interesting problem, which\nhas witnessed lots of applications in practice. To overcome the limitations of\nthe quasi-clique-based approach, we propose d-coherent core (d-CC), a new\nnotion of dense subgraph on multi-layer graphs, which has several elegant\nproperties. We formalize the diversified coherent core search (DCCS) problem,\nwhich finds k d-CCs that can cover the largest number of vertices. We propose a\ngreedy algorithm with an approximation ratio of 1 - 1/e and two search\nalgorithms with an approximation ratio of 1/4. The experiments verify that the\nsearch algorithms are faster than the greedy algorithm and produce comparably\ngood results as the greedy algorithm in practice. As opposed to the\nquasi-clique-based approach, our DCCS algorithms can fast detect larger dense\nsubgraphs that cover most of the quasi-clique-based results.\n"]},
{"authors": ["AnHai Doan", "Adel Ardalan", "Jeffrey R. Ballard", "Sanjib Das", "Yash Govind", "Pradap Konda", "Han Li", "Erik Paulson", "Paul Suganthan G. C.", "Haojun Zhang"], "title": ["Toward a System Building Agenda for Data Integration"], "date": ["2017-09-29T18:43:23Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.00027v1"], "summary": ["  In this paper we argue that the data management community should devote far\nmore effort to building data integration (DI) systems, in order to truly\nadvance the field. Toward this goal, we make three contributions. First, we\ndraw on our recent industrial experience to discuss the limitations of current\nDI systems. Second, we propose an agenda to build a new kind of DI systems to\naddress these limitations. These systems guide users through the DI workflow,\nstep by step. They provide tools to address the \"pain points\" of the steps, and\ntools are built on top of the Python data science and Big Data ecosystem\n(PyData). We discuss how to foster an ecosystem of such tools within PyData,\nthen use it to build DI systems for collaborative/cloud/crowd/lay user\nsettings. Finally, we discuss ongoing work at Wisconsin, which suggests that\nthese DI systems are highly promising and building them raises many interesting\nresearch challenges.\n"]},
{"authors": ["Conred W. Rosenbrock"], "title": ["A Practical Python API for Querying AFLOWLIB"], "date": ["2017-09-28T20:38:47Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1710.00813v1"], "summary": ["  Large databases such as aflowlib.org provide valuable data sources for\ndiscovering material trends through machine learning. Although a REST API and\nquery language are available, there is a learning curve associated with the\nAFLUX language that acts as a barrier for new users. Additionally, the data is\nstored using non-standard serialization formats. Here we present a high-level\nAPI that allows immediate access to the aflowlib data using standard python\noperators and language features. It provides an easy way to integrate aflowlib\ndata with other python materials packages such as ase and quippy, and provides\nautomatic deserialization into numpy arrays and python objects. This package is\navailable via \"pip install aflow\".\n"]},
{"authors": ["Mukund Sundararajan", "Qiqi Yan"], "title": ["A Simple and Efficient MapReduce Algorithm for Data Cube Materialization"], "date": ["2017-09-28T17:26:32Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.10072v1"], "summary": ["  Data cube materialization is a classical database operator introduced in Gray\net al.~(Data Mining and Knowledge Discovery, Vol.~1), which is critical for\nmany analysis tasks. Nandi et al.~(Transactions on Knowledge and Data\nEngineering, Vol.~6) first studied cube materialization for large scale\ndatasets using the MapReduce framework, and proposed a sophisticated\nmodification of a simple broadcast algorithm to handle a dataset with a 216GB\ncube size within 25 minutes with 2k machines in 2012. We take a different\napproach, and propose a simple MapReduce algorithm which (1) minimizes the\ntotal number of copy-add operations, (2) leverages locality of computation, and\n(3) balances work evenly across machines. As a result, the algorithm shows\nexcellent performance, and materialized a real dataset with a cube size of\n35.0G tuples and 1.75T bytes in 54 minutes, with 0.4k machines in 2014.\n"]},
{"authors": ["Christoph Berkholz", "Jens Keppeler", "Nicole Schweikardt"], "title": ["Answering UCQs under updates and in the presence of integrity\n  constraints"], "date": ["2017-09-28T16:15:37Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.10039v1"], "summary": ["  We investigate the query evaluation problem for fixed queries over fully\ndynamic databases where tuples can be inserted or deleted. The task is to\ndesign a dynamic data structure that can immediately report the new result of a\nfixed query after every database update. We consider unions of conjunctive\nqueries (UCQs) and focus on the query evaluation tasks testing (decide whether\nan input tuple belongs to the query result), enumeration (enumerate, without\nrepetition, all tuples in the query result), and counting (output the number of\ntuples in the query result).\n  We identify three increasingly restrictive classes of UCQs which we call\nt-hierarchical, q-hierarchical, and exhaustively q-hierarchical UCQs. Our main\nresults provide the following dichotomies: If the query's homomorphic core is\nt-hierarchical (q-hierarchical, exhaustively q-hierarchical), then the testing\n(enumeration, counting) problem can be solved with constant update time and\nconstant testing time (delay, counting time). Otherwise, it cannot be solved\nwith sublinear update time and sublinear testing time (delay, counting time),\nunless the OV-conjecture and/or the OMv-conjecture fails.\n  We also study the complexity of query evaluation in the dynamic setting in\nthe presence of integrity constraints, and we obtain according dichotomy\nresults for the special case of small domain constraints (i.e., constraints\nwhich state that all values in a particular column of a relation belong to a\nfixed domain of constant size).\n"]},
{"authors": ["Balazs Szalkai", "Vince Grolmusz"], "title": ["SCARF: A Biomedical Association Rule Finding Webserver"], "date": ["2017-09-28T08:47:04Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.09850v1"], "summary": ["  The analysis of enormous datasets with missing data entries is a standard\ntask in biological and medical data processing. Large-scale, multi-institution\nclinical studies are the typical examples of such datasets. These sets make\npossible the search for multi-parametric relations since from the plenty of the\ndata one is likely to find a satisfying number of subjects with the required\nparameter ensembles. Specifically, finding combinatorial biomarkers for some\ngiven condition also needs a very large dataset to analyze. For this goal,\nstatistical regression analysis is not the preferred tool of choice, since (i)\nthe {\\em a priori} knowledge of the parameter-sets to analyze is missing, and\n(ii) typically relatively few subjects have the interesting parameter-value\nensembles for the analysis. For fast and automatic multi-parametric relation\ndiscovery association-rule finding tools are used for more than two decades in\nthe data-mining community. Here we present the SCARF webserver for {\\em\ngeneralized} association rule mining. Association rules are of the form: $a$\nAND $b$ AND ... AND $x \\rightarrow y$, meaning that the presence of properties\n$a$ AND $b$ AND ... AND $x$ implies property $y$; our algorithm finds\ngeneralized association rules, since it also finds logical disjunctions (i.e.,\nORs) at the left-hand side, allowing the discovery of more complex rules in a\nmore compressed form in the database. This feature also helps reducing the\ntypically very large result-tables of such studies, since allowing ORs in the\nleft-hand side of a single rule could include dozens of classical rules. The\ncapabilities of the SCARF algorithm were demonstrated in mining the Alzheimer's\ndatabase of the Coalition Against Major Diseases (CAMD) in our recent\npublication (Archives of Gerontology and Geriatrics Vol. 73, pp. 300-307,\n2017). Here we describe the webserver implementation of the algorithm.\n"]},
{"authors": ["Kaiyu Feng", "Tao Guo", "Gao Cong", "Sourav S. Bhowmicks", "Shuai Ma"], "title": ["SURGE: Continuous Detection of Bursty Regions Over a Stream of Spatial\n  Objects"], "date": ["2017-09-26T23:58:29Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.09287v2"], "summary": ["  With the proliferation of mobile devices and location-based services,\ncontinuous generation of massive volume of streaming spatial objects (i.e.,\ngeo-tagged data) opens up new opportunities to address real-world problems by\nanalyzing them. In this paper, we present a novel continuous bursty region\ndetection problem that aims to continuously detect a bursty region of a given\nsize in a specified geographical area from a stream of spatial objects.\nSpecifically, a bursty region shows maximum spike in the number of spatial\nobjects in a given time window. The problem is useful in addressing several\nreal-world challenges such as surge pricing problem in online transportation\nand disease outbreak detection. To solve the problem, we propose an exact\nsolution and two approximate solutions, and the approximation ratio is\n$\\frac{1-\\alpha}{4}$ in terms of the burst score, where $\\alpha$ is a parameter\nto control the burst score. We further extend these solutions to support\ndetection of top-$k$ bursty regions. Extensive experiments with real-world data\nare conducted to demonstrate the efficiency and effectiveness of our solutions.\n"]},
{"authors": ["Daniel Lemire", "Nathan Kurz", "Christoph Rupp"], "title": ["Stream VByte: Faster Byte-Oriented Integer Compression"], "date": ["2017-09-25T14:45:20Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.08990v2"], "summary": ["  Arrays of integers are often compressed in search engines. Though there are\nmany ways to compress integers, we are interested in the popular byte-oriented\ninteger compression techniques (e.g., VByte or Google's Varint-GB). They are\nappealing due to their simplicity and engineering convenience. Amazon's\nvarint-G8IU is one of the fastest byte-oriented compression technique published\nso far. It makes judicious use of the powerful single-instruction-multiple-data\n(SIMD) instructions available in commodity processors. To surpass varint-G8IU,\nwe present Stream VByte, a novel byte-oriented compression technique that\nseparates the control stream from the encoded data. Like varint-G8IU, Stream\nVByte is well suited for SIMD instructions. We show that Stream VByte decoding\ncan be up to twice as fast as varint-G8IU decoding over real data sets. In this\nsense, Stream VByte establishes new speed records for byte-oriented integer\ncompression, at times exceeding the speed of the memcpy function. On a 3.4GHz\nHaswell processor, it decodes more than 4 billion differentially-coded integers\nper second from RAM to L1 cache.\n"]},
{"authors": ["Hui Li", "Sourav S Bhowmick", "Jiangtao Cui", "Jianfeng Ma"], "title": ["Time is What Prevents Everything from Happening at Once: Propagation\n  Time-conscious Influence Maximization"], "date": ["2017-05-31T08:19:49Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1705.10977v2"], "summary": ["  The influence maximization (IM) problem as defined in the seminal paper by\nKempe et al. has received widespread attention from various research\ncommunities, leading to the design of a wide variety of solutions.\nUnfortunately, this classical IM problem ignores the fact that time taken for\ninfluence propagation to reach the largest scope can be significant in\nrealworld social networks, during which the underlying network itself may have\nevolved. This phenomenon may have considerable adverse impact on the quality of\nselected seeds and as a result all existing techniques that use this classical\ndefinition as their building block generate seeds with suboptimal influence\nspread. In this paper, we revisit the classical IM problem and propose a more\nrealistic version called PROTEUS-IM (Propagation Time conscious Influence\nMaximization) to replace it by addressing the aforementioned limitation.\nSpecifically, as influence propagation may take time, we assume that the\nunderlying social network may evolve during influence propagation.\nConsequently, PROTEUSIM aims to select seeds in the current network to maximize\ninfluence spread in the future instance of the network at the end of influence\npropagation process without assuming complete topological knowledge of the\nfuture network. We propose a greedy and a Reverse Reachable (RR) set-based\nalgorithms called PROTEUS-GENIE and PROTEUS-SEER, respectively, to address this\nproblem. Our algorithms utilize the state-of-the-art Forest Fire Model for\nmodeling network evolution during influence propagation to find superior\nquality seeds. Experimental study on real and synthetic social networks shows\nthat our proposed algorithms consistently outperform state-of-the-art classical\nIM algorithms with respect to seed set quality.\n"]},
{"authors": ["Chiwan Park", "Ha-Myung Park", "Minji Yoon", "U Kang"], "title": ["PMV: Pre-partitioned Generalized Matrix-Vector Multiplication for\n  Scalable Graph Mining"], "date": ["2017-09-26T15:53:15Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.09099v1"], "summary": ["  How can we analyze enormous networks including the Web and social networks\nwhich have hundreds of billions of nodes and edges? Network analyses have been\nconducted by various graph mining methods including shortest path computation,\nPageRank, connected component computation, random walk with restart, etc. These\ngraph mining methods can be expressed as generalized matrix-vector\nmultiplication which consists of few operations inspired by typical\nmatrix-vector multiplication. Recently, several graph processing systems based\non matrix-vector multiplication or their own primitives have been proposed to\ndeal with large graphs; however, they all have failed on Web-scale graphs due\nto insufficient memory space or the lack of consideration for I/O costs. In\nthis paper, we propose PMV (Pre-partitioned generalized Matrix-Vector\nmultiplication), a scalable distributed graph mining method based on\ngeneralized matrix-vector multiplication on distributed systems. PMV\nsignificantly decreases the communication cost, which is the main bottleneck of\ndistributed systems, by partitioning the input graph in advance and judiciously\napplying execution strategies based on the density of the pre-partitioned\nsub-matrices. Experiments show that PMV succeeds in processing up to 16x larger\ngraphs than existing distributed memory-based graph mining methods, and\nrequires 9x less time than previous disk-based graph mining methods by reducing\nI/O costs significantly.\n"]},
{"authors": ["Noreddine Gherabi", "Abdelhadi Daoui", "Abderrahim Marzouk"], "title": ["An enhanced method to compute the similarity between concepts of\n  ontology"], "date": ["2017-09-26T08:18:15Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.08880v1"], "summary": ["  With the use of ontologies in several domains such as semantic web,\ninformation retrieval, artificial intelligence, the concept of similarity\nmeasuring has become a very important domain of research. Therefore, in the\ncurrent paper, we propose our method of similarity measuring which uses the\nDijkstra algorithm to define and compute the shortest path. Then, we use this\none to compute the semantic distance between two concepts defined in the same\nhierarchy of ontology. Afterward, we base on this result to compute the\nsemantic similarity. Finally, we present an experimental comparison between our\nmethod and other methods of similarity measuring.\n"]},
{"authors": ["Zequn Sun", "Wei Hu", "Chengkai Li"], "title": ["Cross-lingual Entity Alignment via Joint Attribute-Preserving Embedding"], "date": ["2017-08-16T19:30:17Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1708.05045v2"], "summary": ["  Entity alignment is the task of finding entities in two knowledge bases (KBs)\nthat represent the same real-world object. When facing KBs in different natural\nlanguages, conventional cross-lingual entity alignment methods rely on machine\ntranslation to eliminate the language barriers. These approaches often suffer\nfrom the uneven quality of translations between languages. While recent\nembedding-based techniques encode entities and relationships in KBs and do not\nneed machine translation for cross-lingual entity alignment, a significant\nnumber of attributes remain largely unexplored. In this paper, we propose a\njoint attribute-preserving embedding model for cross-lingual entity alignment.\nIt jointly embeds the structures of two KBs into a unified vector space and\nfurther refines it by leveraging attribute correlations in the KBs. Our\nexperimental results on real-world datasets show that this approach\nsignificantly outperforms the state-of-the-art embedding approaches for\ncross-lingual entity alignment and could be complemented with methods based on\nmachine translation.\n"]},
{"authors": ["Robert Brijder", "Floris Geerts", "Jan Van den Bussche", "Timmy Weerwag"], "title": ["On the expressive power of query languages for matrices"], "date": ["2017-09-25T08:05:00Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.08359v1"], "summary": ["  We investigate the expressive power of $\\mathsf{MATLANG}$, a formal language\nfor matrix manipulation based on common matrix operations and linear algebra.\nThe language can be extended with the operation $\\mathsf{inv}$ of inverting a\nmatrix. In $\\mathsf{MATLANG}+\\mathsf{inv}$ we can compute the transitive\nclosure of directed graphs, whereas we show that this is not possible without\ninversion. Indeed we show that the basic language can be simulated in the\nrelational algebra with arithmetic operations, grouping, and summation. We also\nconsider an operation $\\mathsf{eigen}$ for diagonalizing a matrix, which is\ndefined so that different eigenvectors returned for a same eigenvalue are\northogonal. We show that $\\mathsf{inv}$ can be expressed in\n$\\mathsf{MATLANG}+\\mathsf{eigen}$. We put forward the open question whether\nthere are boolean queries about matrices, or generic queries about graphs,\nexpressible in $\\mathsf{MATLANG} + \\mathsf{eigen}$ but not in\n$\\mathsf{MATLANG}+\\mathsf{inv}$. The evaluation problem for $\\mathsf{MATLANG} +\n\\mathsf{eigen}$ is shown to be complete for the complexity class $\\exists\n\\mathbf{R}$.\n"]},
{"authors": ["Lingyang Chu", "Zhefeng Wang", "Jian Pei", "Yanyan Zhang", "Yu Yang", "Enhong Chen"], "title": ["Finding Theme Communities from Database Networks: from Mining to\n  Indexing and Query Answering"], "date": ["2017-09-23T17:35:25Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.08083v1"], "summary": ["  Given a database network where each vertex is associated with a transaction\ndatabase, we are interested in finding theme communities. Here, a theme\ncommunity is a cohesive subgraph such that a common pattern is frequent in all\ntransaction databases associated with the vertices in the subgraph. Finding all\ntheme communities from a database network enjoys many novel applications.\nHowever, it is challenging since even counting the number of all theme\ncommunities in a database network is #P-hard. Inspired by the observation that\na theme community shrinks when the length of the pattern increases, we\ninvestigate several properties of theme communities and develop TCFI, a\nscalable algorithm that uses these properties to effectively prune the patterns\nthat cannot form any theme community. We also design TC-Tree, a scalable\nalgorithm that decomposes and indexes theme communities efficiently. Retrieving\n1 million theme communities from a TC-Tree takes only 1 second. Extensive\nexperiments and a case study demonstrate the effectiveness and scalability of\nTCFI and TC-Tree in discovering and querying meaningful theme communities from\nlarge database networks.\n"]},
{"authors": ["Noreddine Gherabi", "Redouane Nejjahi", "Abderrahim Marzouk"], "title": ["Towards Classification of Web ontologies using the Horizontal and\n  Vertical Segmentation"], "date": ["2017-09-23T09:20:34Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.08028v1"], "summary": ["  The new era of the Web is known as the semantic Web or the Web of data. The\nsemantic Web depends on ontologies that are seen as one of its pillars. The\nbigger these ontologies, the greater their exploitation. However, when these\nontologies become too big other problems may appear, such as the complexity to\ncharge big files in memory, the time it needs to download such files and\nespecially the time it needs to make reasoning on them. We discuss in this\npaper approaches for segmenting such big Web ontologies as well as its\nusefulness. The segmentation method extracts from an existing ontology a\nsegment that represents a layer or a generation in the existing ontology; i.e.\na horizontally extraction. The extracted segment should be itself an ontology.\n"]},
{"authors": ["Antonio Cavalcante Araujo Neto", "Joerg Sander", "Ricardo J. G. B. Campello", "Mario A. Nascimento"], "title": ["Efficient Computation of Multiple Density-Based Clustering Hierarchies"], "date": ["2017-09-13T21:24:42Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.04545v2"], "summary": ["  HDBSCAN*, a state-of-the-art density-based hierarchical clustering method,\nproduces a hierarchical organization of clusters in a dataset w.r.t. a\nparameter mpts. While the performance of HDBSCAN* is robust w.r.t. mpts in the\nsense that a small change in mpts typically leads to only a small or no change\nin the clustering structure, choosing a \"good\" mpts value can be challenging:\ndepending on the data distribution, a high or low value for mpts may be more\nappropriate, and certain data clusters may reveal themselves at different\nvalues of mpts. To explore results for a range of mpts values, however, one has\nto run HDBSCAN* for each value in the range independently, which is\ncomputationally inefficient. In this paper, we propose an efficient approach to\ncompute all HDBSCAN* hierarchies for a range of mpts values by replacing the\ngraph used by HDBSCAN* with a much smaller graph that is guaranteed to contain\nthe required information. An extensive experimental evaluation shows that with\nour approach one can obtain over one hundred hierarchies for the computational\ncost equivalent to running HDBSCAN* about 2 times.\n"]},
{"authors": ["Janis Kalofolias", "Mario Boley", "Jilles Vreeken"], "title": ["Efficiently Discovering Locally Exceptional yet Globally Representative\n  Subgroups"], "date": ["2017-09-22T20:49:41Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.07941v1"], "summary": ["  Subgroup discovery is a local pattern mining technique to find interpretable\ndescriptions of sub-populations that stand out on a given target variable. That\nis, these sub-populations are exceptional with regard to the global\ndistribution. In this paper we argue that in many applications, such as\nscientific discovery, subgroups are only useful if they are additionally\nrepresentative of the global distribution with regard to a control variable.\nThat is, when the distribution of this control variable is the same, or almost\nthe same, as over the whole data.\n  We formalise this objective function and give an efficient algorithm to\ncompute its tight optimistic estimator for the case of a numeric target and a\nbinary control variable. This enables us to use the branch-and-bound framework\nto efficiently discover the top-$k$ subgroups that are both exceptional as well\nas representative. Experimental evaluation on a wide range of datasets shows\nthat with this algorithm we discover meaningful representative patterns and are\nup to orders of magnitude faster in terms of node evaluations as well as time.\n"]},
{"authors": ["J\u00f3zsef Marton", "G\u00e1bor Sz\u00e1rnyas", "D\u00e1niel Varr\u00f3"], "title": ["Formalising opencypher Graph Queries in Relational Algebra"], "date": ["2017-05-08T12:19:37Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1705.02844v2"], "summary": ["  Graph database systems are increasingly adapted for storing and processing\nheterogeneous network-like datasets. However, due to the novelty of such\nsystems, no standard data model or query language has yet emerged.\nConsequently, migrating datasets or applications even between related\ntechnologies often requires a large amount of manual work or ad-hoc solutions,\nthus subjecting the users to the possibility of vendor lock-in. To avoid this\nthreat, vendors are working on supporting existing standard languages (e.g.\nSQL) or creating standardised languages.\n  In this paper, we present a formal specification for openCypher, a high-level\ndeclarative graph query language with an ongoing standardisation effort. We\nintroduce relational graph algebra, which extends relational operators by\nadapting graph-specific operators and define a mapping from core openCypher\nconstructs to this algebra. We propose an algorithm that allows systematic\ncompilation of openCypher queries.\n"]},
{"authors": ["Sudhakar Singh", "Rakhi Garg", "P. K. Mishra"], "title": ["A Comparative Study of Association Rule Mining Algorithms on Grid and\n  Cloud Platform"], "date": ["2017-09-22T05:06:18Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.07594v1"], "summary": ["  Association rule mining is a time consuming process due to involving both\ndata intensive and computation intensive nature. In order to mine large volume\nof data and to enhance the scalability and performance of existing sequential\nassociation rule mining algorithms, parallel and distributed algorithms are\ndeveloped. These traditional parallel and distributed algorithms are based on\nhomogeneous platform and are not lucrative for heterogeneous platform such as\ngrid and cloud. This requires design of new algorithms which address the issues\nof good data set partition and distribution, load balancing strategy,\noptimization of communication and synchronization technique among processors in\nsuch heterogeneous system. Grid and cloud are the emerging platform for\ndistributed data processing and various association rule mining algorithms have\nbeen proposed on such platforms. This survey article integrates the brief\narchitectural aspect of distributed system, various recent approaches of grid\nbased and cloud based association rule mining algorithms with comparative\nperception. We differentiate between approaches of association rule mining\nalgorithms developed on these architectures on the basis of data locality,\nprogramming paradigm, fault tolerance, communication cost, partition and\ndistribution of data sets. Although it is not complete in order to cover all\nalgorithms, yet it can be very useful for the new researchers working in the\ndirection of distributed association rule mining algorithms.\n"]},
{"authors": ["Radwa Elshawi", "Sherif Sakr"], "title": ["Big Data Systems Meet Machine Learning Challenges: Towards Big Data\n  Science as a Service"], "date": ["2017-09-21T18:50:32Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.07493v1"], "summary": ["  Recently, we have been witnessing huge advancements in the scale of data we\nroutinely generate and collect in pretty much everything we do, as well as our\nability to exploit modern technologies to process, analyze and understand this\ndata. The intersection of these trends is what is called, nowadays, as Big Data\nScience. Cloud computing represents a practical and cost-effective solution for\nsupporting Big Data storage, processing and for sophisticated analytics\napplications. We analyze in details the building blocks of the software stack\nfor supporting big data science as a commodity service for data scientists. We\nprovide various insights about the latest ongoing developments and open\nchallenges in this domain.\n"]},
{"authors": ["Yu Liu", "Bolong Zheng", "Xiaodong He", "Zhewei Wei", "Xiaokui Xiao", "Kai Zheng", "Jiaheng Lu"], "title": ["ProbeSim: Scalable Single-Source and Top-k SimRank Computations on\n  Dynamic Graphs"], "date": ["2017-09-20T16:32:29Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.06955v1"], "summary": ["  Single-source and top-$k$ SimRank queries are two important types of\nsimilarity search in graphs with numerous applications in web mining, social\nnetwork analysis, spam detection, etc. A plethora of techniques have been\nproposed for these two types of queries, but very few can efficiently support\nsimilarity search over large dynamic graphs, due to either significant\npreprocessing time or large space overheads.\n  This paper presents ProbeSim, an index-free algorithm for single-source and\ntop-$k$ SimRank queries that provides a non-trivial theoretical guarantee in\nthe absolute error of query results. ProbeSim estimates SimRank similarities\nwithout precomputing any indexing structures, and thus can naturally support\nreal-time SimRank queries on dynamic graphs. Besides the theoretical guarantee,\nProbeSim also offers satisfying practical efficiency and effectiveness due to\nseveral non-trivial optimizations. We conduct extensive experiments on a number\nof benchmark datasets, which demonstrate that our solutions significantly\noutperform the existing methods in terms of efficiency and effectiveness.\nNotably, our experiments include the first empirical study that evaluates the\neffectiveness of SimRank algorithms on graphs with billion edges, using the\nidea of pooling.\n"]},
{"authors": ["Simon Razniewski", "Vevake Balaraman", "Werner Nutt"], "title": ["Doctoral Advisor or Medical Condition: Towards Entity-specific Rankings\n  of Knowledge Base Properties [Extended Version]"], "date": ["2017-09-20T14:43:08Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.06907v1"], "summary": ["  In knowledge bases such as Wikidata, it is possible to assert a large set of\nproperties for entities, ranging from generic ones such as name and place of\nbirth to highly profession-specific or background-specific ones such as\ndoctoral advisor or medical condition. Determining a preference or ranking in\nthis large set is a challenge in tasks such as prioritisation of edits or\nnatural-language generation. Most previous approaches to ranking knowledge base\nproperties are purely data-driven, that is, as we show, mistake frequency for\ninterestingness.\n  In this work, we have developed a human-annotated dataset of 350 preference\njudgments among pairs of knowledge base properties for fixed entities. From\nthis set, we isolate a subset of pairs for which humans show a high level of\nagreement (87.5% on average). We show, however, that baseline and\nstate-of-the-art techniques achieve only 61.3% precision in predicting human\npreferences for this subset.\n  We then analyze what contributes to one property being rated as more\nimportant than another one, and identify that at least three factors play a\nrole, namely (i) general frequency, (ii) applicability to similar entities and\n(iii) semantic similarity between property and entity. We experimentally\nanalyze the contribution of each factor and show that a combination of\ntechniques addressing all the three factors achieves 74% precision on the task.\n  The dataset is available at\nwww.kaggle.com/srazniewski/wikidatapropertyranking.\n"]},
{"authors": ["Huiju Wang", "Zhengkui Wang", "Kian-Lee Tan", "Chee-Yong Chan", "Qi Fan", "Xiao Yue"], "title": ["VCExplorer: A Interactive Graph Exploration Framework Based on Hub\n  Vertices with Graph Consolidation"], "date": ["2017-09-20T07:23:03Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.06745v1"], "summary": ["  Graphs have been widely used to model different information networks, such as\nthe Web, biological networks and social networks (e.g. Twitter). Due to the\nsize and complexity of these graphs, how to explore and utilize these graphs\nhas become a very challenging problem. In this paper, we propose, VCExplorer, a\nnew interactive graph exploration framework that integrates the strengths of\ngraph visualization and graph summarization. Unlike existing graph\nvisualization tools where vertices of a graph may be clustered into a smaller\ncollection of super/virtual vertices, VCExplorer displays a small number of\nactual source graph vertices (called hubs) and summaries of the information\nbetween these vertices. We refer to such a graph as a HA-graph (Hub-based\nAggregation Graph). This allows users to appreciate the relationship between\nthe hubs, rather than super/virtual vertices. Users can navigate through the\nHA- graph by \"drilling down\" into the summaries between hubs to display more\nhubs. We illustrate how the graph aggregation techniques can be integrated into\nthe exploring framework as the consolidated information to users. In addition,\nwe propose efficient graph aggregation algorithms over multiple subgraphs via\ncomputation sharing. Extensive experimental evaluations have been conducted\nusing both real and synthetic datasets and the results indicate the\neffectiveness and efficiency of VCExplorer for exploration.\n"]},
{"authors": ["Mohamed S. Hassan", "Bruno Ribeiro", "Walid G. Aref"], "title": ["SBG-Sketch: A Self-Balanced Sketch for Labeled-Graph Stream\n  Summarization"], "date": ["2017-09-20T04:37:02Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.06723v1"], "summary": ["  Applications in various domains rely on processing graph streams, e.g.,\ncommunication logs of a cloud-troubleshooting system, road-network traffic\nupdates, and interactions on a social network. A labeled-graph stream refers to\na sequence of streamed edges that form a labeled graph. Label-aware\napplications need to filter the graph stream before performing a graph\noperation. Due to the large volume and high velocity of these streams, it is\noften more practical to incrementally build a lossy-compressed version of the\ngraph, and use this lossy version to approximately evaluate graph queries.\nChallenges arise when the queries are unknown in advance but are associated\nwith filtering predicates based on edge labels. Surprisingly common, and\nespecially challenging, are labeled-graph streams that have highly skewed label\ndistributions that might also vary over time. This paper introduces\nSelf-Balanced Graph Sketch (SBG-Sketch, for short), a graphical sketch for\nsummarizing and querying labeled-graph streams that can cope with all these\nchallenges. SBG-Sketch maintains synopsis for both the edge attributes (e.g.,\nedge weight) as well as the topology of the streamed graph. SBG-Sketch allows\nefficient processing of graph-traversal queries, e.g., reachability queries.\nExperimental results over a variety of real graph streams show SBG-Sketch to\nreduce the estimation errors of state-of-the-art methods by up to 99%.\n"]},
{"authors": ["Kijung Shin"], "title": ["WRS: Waiting Room Sampling for Accurate Triangle Counting in Real Graph\n  Streams"], "date": ["2017-09-10T17:47:04Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.03147v2"], "summary": ["  If we cannot store all edges in a graph stream, which edges should we store\nto estimate the triangle count accurately?\n  Counting triangles (i.e., cycles of length three) is a fundamental graph\nproblem with many applications in social network analysis, web mining, anomaly\ndetection, etc. Recently, much effort has been made to accurately estimate\nglobal and local triangle counts in streaming settings with limited space.\nAlthough existing methods use sampling techniques without considering temporal\ndependencies in edges, we observe temporal locality in real dynamic graphs.\nThat is, future edges are more likely to form triangles with recent edges than\nwith older edges.\n  In this work, we propose a single-pass streaming algorithm called\nWaiting-Room Sampling (WRS) for global and local triangle counting. WRS\nexploits the temporal locality by always storing the most recent edges, which\nfuture edges are more likely to form triangles with, in the waiting room, while\nit uses reservoir sampling for the remaining edges. Our theoretical and\nempirical analyses show that WRS is: (a) Fast and 'any time': runs in linear\ntime, always maintaining and updating estimates while new edges arrive, (b)\nEffective: yields up to 47% smaller estimation error than its best competitors,\nand (c) Theoretically sound: gives unbiased estimates with small variances\nunder the temporal locality.\n"]},
{"authors": ["Iovka Boneva", "Jose Emilio Labra Gayo", "Eric G. Prud'hommeau"], "title": ["Semantics and Validation of Shapes Schemas for RDF"], "date": ["2014-04-04T14:39:48Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1404.1270v3"], "summary": ["  We present a formal semantics and proof of soundness for shapes schemas, an\nexpressive schema language for RDF graphs that is the foundation of Shape\nExpressions Language 2.0. It can be used to describe the vocabulary and the\nstructure of an RDF graph, and to constrain the admissible properties and\nvalues for nodes in that graph. The language defines a typing mechanism called\nshapes against which nodes of the graph can be checked. It includes an\nalgebraic grouping operator, a choice operator and cardinality constraints for\nthe number of allowed occurrences of a property. Shapes can be combined using\nBoolean operators, and can use possibly recursive references to other shapes.\n  We describe the syntax of the language and define its semantics. The\nsemantics is proven to be well-defined for schemas that satisfy a reasonable\nsyntactic restriction, namely stratified use of negation and recursion. We\npresent two algorithms for the validation of an RDF graph against a shapes\nschema. The first algorithm is a direct implementation of the semantics,\nwhereas the second is a non-trivial improvement. We also briefly give\nimplementation guidelines.\n"]},
{"authors": ["Kai Herrmann", "Hannes Voigt", "Andreas Behrend", "Jonas Rausch", "Wolfgang Lehner"], "title": ["Living in Parallel Realities -- Co-Existing Schema Versions with a\n  Bidirectional Database Evolution Language"], "date": ["2016-08-19T10:44:42Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1608.05564v2"], "summary": ["  We introduce end-to-end support of co-existing schema versions within one\ndatabase. While it is state of the art to run multiple versions of a\ncontinuously developed application concurrently, it is hard to do the same for\ndatabases. In order to keep multiple co-existing schema versions alive; which\nare all accessing the same data set; developers usually employ handwritten\ndelta code (e.g. views and triggers in SQL). This delta code is hard to write\nand hard to maintain: if a database administrator decides to adapt the physical\ntable schema, all handwritten delta code needs to be adapted as well, which is\nexpensive and error-prone in practice. In this paper, we present InVerDa:\ndevelopers use the simple bidirectional database evolution language BiDEL,\nwhich carries enough information to generate all delta code automatically.\nWithout additional effort, new schema versions become immediately accessible\nand data changes in any version are visible in all schema versions at the same\ntime. InVerDa also allows for easily changing the physical table design without\naffecting the availability of co-existing schema versions. This greatly\nincreases robustness (orders of magnitude less lines of code) and allows for\nsignificant performance optimization. A main contribution is the formal\nevaluation that each schema version acts like a common full-fledged database\nschema independently of the chosen physical table design.\n"]},
{"authors": ["Kexin Rong", "Peter Bailis"], "title": ["ASAP: Prioritizing Attention via Time Series Smoothing"], "date": ["2017-03-02T23:09:48Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.00983v2"], "summary": ["  Time series visualization of streaming telemetry (i.e., charting of key\nmetrics such as server load over time) is increasingly prevalent in modern data\nplatforms and applications. However, many existing systems simply plot the raw\ndata streams as they arrive, often obscuring large-scale trends due to\nsmall-scale noise. We propose an alternative: to better prioritize end users'\nattention, smooth time series visualizations as much as possible to remove\nnoise, while retaining large-scale structure to highlight significant\ndeviations. We develop a new analytics operator called ASAP that automatically\nsmooths streaming time series by adaptively optimizing the trade-off between\nnoise reduction (i.e., variance) and trend retention (i.e., kurtosis). We\nintroduce metrics to quantitatively assess the quality of smoothed plots and\nprovide an efficient search strategy for optimizing these metrics that combines\ntechniques from stream processing, user interface design, and signal processing\nvia autocorrelation-based pruning, pixel-aware preaggregation, and on-demand\nrefresh. We demonstrate that ASAP can improve users' accuracy in identifying\nlong-term deviations in time series by up to 38.4% while reducing response\ntimes by up to 44.3%. Moreover, ASAP delivers these results several orders of\nmagnitude faster than alternative search strategies.\n"]},
{"authors": ["Fernando Mart\u00ednez-Plumed", "Lidia Contreras-Ochando", "C\u00e8sar Ferri", "Peter Flach", "Jos\u00e9 Hern\u00e1ndez-Orallo", "Meelis Kull", "Nicolas Lachiche", "Mar\u00eda Jos\u00e9 Ram\u00edrez-Quintana"], "title": ["CASP-DM: Context Aware Standard Process for Data Mining"], "date": ["2017-09-19T07:59:41Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.09003v1"], "summary": ["  We propose an extension of the Cross Industry Standard Process for Data\nMining (CRISPDM) which addresses specific challenges of machine learning and\ndata mining for context and model reuse handling. This new general\ncontext-aware process model is mapped with CRISP-DM reference model proposing\nsome new or enhanced outputs.\n"]},
{"authors": ["Avishek Bose", "Arslan Munir", "Neda Shabani"], "title": ["A Comparative Quantitative Analysis of Contemporary Big Data Clustering\n  Algorithms for Market Segmentation in Hospitality Industry"], "date": ["2017-09-18T23:47:44Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.06202v1"], "summary": ["  The hospitality industry is one of the data-rich industries that receives\nhuge Volumes of data streaming at high Velocity with considerably Variety,\nVeracity, and Variability. These properties make the data analysis in the\nhospitality industry a big data problem. Meeting the customers' expectations is\na key factor in the hospitality industry to grasp the customers' loyalty. To\nachieve this goal, marketing professionals in this industry actively look for\nways to utilize their data in the best possible manner and advance their data\nanalytic solutions, such as identifying a unique market segmentation clustering\nand developing a recommendation system. In this paper, we present a\ncomprehensive literature review of existing big data clustering algorithms and\ntheir advantages and disadvantages for various use cases. We implement the\nexisting big data clustering algorithms and provide a quantitative comparison\nof the performance of different clustering algorithms for different scenarios.\nWe also present our insights and recommendations regarding the suitability of\ndifferent big data clustering algorithms for different use cases. These\nrecommendations will be helpful for hoteliers in selecting the appropriate\nmarket segmentation clustering algorithm for different clustering datasets to\nimprove the customer experience and maximize the hotel revenue.\n"]},
{"authors": ["Piotr S. Maci\u0105g"], "title": ["Discovering Sequential Patterns in Event-Based Spatio-Temporal Data by\n  Means of Microclustering - Extended Report"], "date": ["2017-08-29T10:00:37Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1708.08674v2"], "summary": ["  In the paper, we consider the problem of discovering sequential patterns from\nevent-based spatio-temporal data. The problem is defined as follows: for a set\nof event types $F$ and for a dataset of events instances $D$ (where each\ninstance in $D$ denotes an occurrence of a particular event type in considered\nspatio-temporal space), discover all sequential patterns defining the following\nrelation between any event types participating in a particular pattern. The\nfollowing relation $\\rightarrow$ between any two event types, denotes the fact\nthat instances of the first event type attract in their spatial proximity and\nin considered temporal window afterwards occurrences of instances of the second\nevent type. In the article, the notion of sequential pattern in event-based\nspatio-temporal data has been defined and the already proposed approach to\ndiscovering sequential pattern has been reformulated. We show, it is possible\nto efficiently and effectively discover sequential patterns in event-based\nspatio-temporal data.\n"]},
{"authors": ["Julia Stoyanovich", "Matthew Gilbride", "Vera Zaychik Moffitt"], "title": ["Zooming in on NYC taxi data with Portal"], "date": ["2017-09-18T21:55:47Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.06176v1"], "summary": ["  In this paper we develop a methodology for analyzing transportation data at\ndifferent levels of temporal and geographic granularity, and apply our\nmethodology to the TLC Trip Record Dataset, made publicly available by the NYC\nTaxi & Limousine Commission. This data is naturally represented by a set of\ntrajectories, annotated with time and with additional information such as\npassenger count and cost. We analyze TLC data to identify hotspots, which point\nto lack of convenient public transportation options, and popular routes, which\nmotivate ride-sharing solutions or addition of a bus route.\n  Our methodology is based on using a system called Portal, which implements\nefficient representations and principled analysis methods for evolving graphs.\nPortal is implemented on top of Apache Spark, a popular distributed data\nprocessing system, is inter-operable with other Spark libraries like SparkSQL,\nand supports sophisticated kinds of analysis of evolving graphs efficiently.\nPortal is currently under development in the Data, Responsibly Lab at Drexel.\nWe plan to release Portal in the open source in Fall 2017.\n"]},
{"authors": ["Jose Ferreira", "Fernando Almeida", "Jose Monteiro"], "title": ["Building an Effective Data Warehousing for Financial Sector"], "date": ["2017-09-18T11:50:28Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.05874v1"], "summary": ["  This article presents the implementation process of a Data Warehouse and a\nmultidimensional analysis of business data for a holding company in the\nfinancial sector. The goal is to create a business intelligence system that, in\na simple, quick but also versatile way, allows the access to updated,\naggregated, real and/or projected information, regarding bank account balances.\nThe established system extracts and processes the operational database\ninformation which supports cash management information by using Integration\nServices and Analysis Services tools from Microsoft SQL Server. The end-user\ninterface is a pivot table, properly arranged to explore the information\navailable by the produced cube. The results have shown that the adoption of\nonline analytical processing cubes offers better performance and provides a\nmore automated and robust process to analyze current and provisional aggregated\nfinancial data balances compared to the current process based on static reports\nbuilt from transactional databases.\n"]},
{"authors": ["Jaqueline Brito", "Korhan Demirkaya", "Boursier Etienne", "Yannis Katsis", "Chunbin Lin", "Yannis Papakonstantinou"], "title": ["Efficient Approximate Query Answering over Sensor Data with\n  Deterministic Error Guarantees"], "date": ["2017-07-05T14:22:05Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1707.01414v2"], "summary": ["  With the recent proliferation of sensor data, there is an increasing need for\nthe efficient evaluation of analytical queries over multiple sensor datasets.\nThe magnitude of such datasets makes exact query answering infeasible, leading\nresearchers into the development of approximate query answering approaches.\nHowever, existing approximate query answering algorithms are not suited for the\nefficient processing of queries over sensor data, as they exhibit at least one\nof the following shortcomings: (a) They do not provide deterministic error\nguarantees, resorting to weaker probabilistic error guarantees that are in many\ncases not acceptable, (b) they allow queries only over a single dataset, thus\nnot supporting the multitude of queries over multiple datasets that appear in\npractice, such as correlation or cross-correlation and (c) they support\nrelational data in general and thus miss speedup opportunities created by the\nspecial nature of sensor data, which are not random but follow a typically\nsmooth underlying phenomenon.\n  To address these problems, we propose PlatoDB; a system that exploits the\nnature of sensor data to compress them and provide efficient processing of\nqueries over multiple sensor datasets, while providing deterministic error\nguarantees. PlatoDB achieves the above through a novel architecture that (a) at\ndata import time pre-processes each dataset, creating for it an intermediate\nhierarchical data structure that provides a hierarchy of summarizations of the\ndataset together with appropriate error measures and (b) at query processing\ntime leverages the pre-computed data structures to compute an approximate\nanswer and deterministic error guarantees for ad hoc queries even when these\ncombine multiple datasets.\n"]},
{"authors": ["Christiane Engels", "Andreas Behrend", "Stefan Brass"], "title": ["A Rule-Based Approach to Analyzing Database Schema Objects with Datalog"], "date": ["2017-09-15T19:23:30Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.05376v1"], "summary": ["  Database schema elements such as tables, views, triggers and functions are\ntypically defined with many interrelationships. In order to support database\nusers in understanding a given schema, a rule-based approach for analyzing the\nrespective dependencies is proposed using Datalog expressions. We show that\nmany interesting properties of schema elements can be systematically determined\nthis way. The expressiveness of the proposed analysis is exemplarily shown with\nthe problem of computing induced functional dependencies for derived relations.\nThe propagation of functional dependencies plays an important role in data\nintegration and query optimization but represents an undecidable problem in\ngeneral. And yet, our rule-based analysis covers all relational operators as\nwell as linear recursive expressions in a systematic way showing the depth of\nanalysis possible by our proposal. The analysis of functional dependencies is\nwell-integrated in a uniform approach to analyzing dependencies between schema\nelements in general.\n"]},
{"authors": ["Alejandro Grez", "Cristian Riveros", "Mart\u00edn Ugarte"], "title": ["Foundations of Complex Event Processing"], "date": ["2017-09-15T19:01:15Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.05369v1"], "summary": ["  Complex Event Processing (CEP) has emerged as the unifying field for\ntechnologies that require processing and correlating heterogeneous distributed\ndata sources in real-time. CEP finds applications in diverse domains, which has\nresulted in a large number of proposals for expressing and processing complex\nevents. However, existing CEP frameworks are based on ad-hoc solutions that do\nnot rely on solid theoretical ground, making them hard to understand, extend or\ngeneralize. Moreover, they are usually presented as application programming\ninterfaces documented by examples, and using each of them requires learning a\ndifferent set of skills.\n  In this paper we embark on the task of giving a rigorous framework to CEP. As\na starting point, we propose a formal language for specifying complex events,\ncalled CEPL, that contains the common features used in the literature and has a\nsimple and denotational semantics. We also formalize the so-called selection\nstrategies, which are the cornerstone of CEP and had only been presented as\nby-design extensions to existing frameworks. With a well-defined semantics at\nhand, we study how to efficiently evaluate CEPL for processing complex events.\nWe provide optimization results based on rewriting formulas to a normal form\nthat simplifies the evaluation of filters. Furthermore, we introduce a formal\ncomputational model for CEP based on transducers and symbolic automata, called\nmatch automata, that captures the regular core of CEPL, i.e. formulas with\nunary predicates. By using rewriting techniques and automata-based\ntranslations, we show that formulas in the regular core of CEPL can be\nevaluated using constant time per event followed by constant-delay enumeration\nof the output (under data complexity). By gathering these results together, we\npropose a framework for efficiently evaluating CEPL, establishing solid\nfoundations for future CEP systems.\n"]},
{"authors": ["Demian Hespe", "Martin Weidner", "Jonathan Dees", "Peter Sanders"], "title": ["Fast OLAP Query Execution in Main Memory on Large Data in a Cluster"], "date": ["2017-09-15T13:08:07Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.05183v1"], "summary": ["  Main memory column-stores have proven to be efficient for processing\nanalytical queries. Still, there has been much less work in the context of\nclusters. Using only a single machine poses several restrictions: Processing\npower and data volume are bounded to the number of cores and main memory\nfitting on one tightly coupled system. To enable the processing of larger data\nsets, switching to a cluster becomes necessary. In this work, we explore\ntechniques for efficient execution of analytical SQL queries on large amounts\nof data in a parallel database cluster while making maximal use of the\navailable hardware. This includes precompiled query plans for efficient CPU\nutilization, full parallelization on single nodes and across the cluster, and\nefficient inter-node communication. We implement all features in a prototype\nfor running a subset of TPC-H benchmark queries. We evaluate our implementation\nusing a 128 node cluster running TPC-H queries with 30 000 gigabyte of\nuncompressed data.\n"]},
{"authors": ["Yuliang Li", "Alin Deutsch", "Victor Vianu"], "title": ["VERIFAS: A Practical Verifier for Artifact Systems"], "date": ["2017-05-29T00:45:18Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1705.10007v3"], "summary": ["  Data-driven workflows, of which IBM's Business Artifacts are a prime\nexponent, have been successfully deployed in practice, adopted in industrial\nstandards, and have spawned a rich body of research in academia, focused\nprimarily on static analysis. The present research bridges the gap between the\ntheory and practice of artifact verification with VERIFAS, the first\nimplementation of practical significance of an artifact verifier with full\nsupport for unbounded data. VERIFAS verifies within seconds linear-time\ntemporal properties over real-world and synthetic workflows of complexity in\nthe range recommended by software engineering practice. Compared to our\nprevious implementation based on the widely-used Spin model checker, VERIFAS\nnot only supports a model with richer data manipulations but also outperforms\nit by over an order of magnitude. VERIFAS' good performance is due to a novel\nsymbolic representation approach and a family of specialized optimizations.\n"]},
{"authors": ["Ciprian-Octavian Truic\u0103", "J\u00e9r\u00f4me Darmont"], "title": ["T${}^2$K${}^2$: The Twitter Top-K Keywords Benchmark"], "date": ["2017-09-14T13:01:51Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.04747v1"], "summary": ["  Information retrieval from textual data focuses on the construction of\nvocabularies that contain weighted term tuples. Such vocabularies can then be\nexploited by various text analysis algorithms to extract new knowledge, e.g.,\ntop-k keywords, top-k documents, etc. Top-k keywords are casually used for\nvarious purposes, are often computed on-the-fly, and thus must be efficiently\ncomputed. To compare competing weighting schemes and database implementations,\nbenchmarking is customary. To the best of our knowledge, no benchmark currently\naddresses these problems. Hence, in this paper, we present a top-k keywords\nbenchmark, T${}^2$K${}^2$, which features a real tweet dataset and queries with\nvarious complexities and selectivities. T${}^2$K${}^2$ helps evaluate weighting\nschemes and database implementations in terms of computing performance. To\nillustrate T${}^2$K${}^2$'s relevance and genericity, we successfully performed\ntests on the TF-IDF and Okapi BM25 weighting schemes, on one hand, and on\ndifferent relational (Oracle, PostgreSQL) and document-oriented (MongoDB)\ndatabase implementations, on the other hand.\n"]},
{"authors": ["Philipp M. Grulich"], "title": ["Scalable real-time processing with Spark Streaming: implementation and\n  design of a Car Information System"], "date": ["2017-09-14T11:13:17Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.05197v1"], "summary": ["  Streaming data processing is a hot topic in big data these days, because it\nmade it possible to process a huge amount of events within a low latency. One\nof the most common used open-source stream processing platforms is Spark\nStreaming, which is demonstrated and discussed based on a real-world use-case\nin this paper. The use-case is about a Car Information System, which is an\nexample for a classic stream processing system. First the System is de- signed\nand engineered, whereby the application architecture is created carefully,\nbecause it should be adaptable for similar use-cases. At the end of this paper\nthe CIS and Spark Streaming is evaluated by the use of the Goal Question Metric\nmodel. The evaluation proves that Spark Streaming is capable to create stream\nprocessing in a scalable and fault tolerant manner. But it also shows that\nSpark is a very fast moving project, which could cause problems during the\ndevelopment and maintenance of a software project.\n"]},
{"authors": ["Yuan-Yen Tai"], "title": ["An efficient clustering algorithm from the measure of local Gaussian\n  distribution"], "date": ["2017-09-13T15:39:03Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.08470v1"], "summary": ["  In this paper, I will introduce a fast and novel clustering algorithm based\non Gaussian distribution and it can guarantee the separation of each cluster\ncentroid as a given parameter, $d_s$. The worst run time complexity of this\nalgorithm is approximately $\\sim$O$(T\\times N \\times \\log(N))$ where $T$ is the\niteration steps and $N$ is the number of features.\n"]},
{"authors": ["Michel de Rougemont", "Guillaume Vimont"], "title": ["Approximate Integration of streaming data"], "date": ["2017-09-13T12:37:29Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.04290v1"], "summary": ["  We approximate analytic queries on streaming data with a weighted reservoir\nsampling. For a stream of tuples of a Datawarehouse we show how to approximate\nsome OLAP queries. For a stream of graph edges from a Social Network, we\napproximate the communities as the large connected components of the edges in\nthe reservoir. We show that for a model of random graphs which follow a power\nlaw degree distribution, the community detection algorithm is a good\napproximation. Given two streams of graph edges from two Sources, we define the\n{\\em Community Correlation} as the fraction of the nodes in communities in both\nstreams. Although we do not store the edges of the streams, we can approximate\nthe Community Correlation and define the {\\em Integration of two streams}. We\nillustrate this approach with Twitter streams, associated with TV programs.\n"]},
{"authors": ["Ankur Sharma", "Felix Martin Schuhknecht", "Jens Dittrich"], "title": ["Accelerating Analytical Processing in MVCC using Fine-Granular\n  High-Frequency Virtual Snapshotting"], "date": ["2017-09-13T12:30:30Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.04284v1"], "summary": ["  Efficient transactional management is a delicate task. As systems face\ntransactions of inherently different types, ranging from point updates to long\nrunning analytical computations, it is hard to satisfy their individual\nrequirements with a single processing component. Unfortunately, most systems\nnowadays rely on such a single component that implements its parallelism using\nmulti-version concurrency control (MVCC). While MVCC parallelizes short-running\nOLTP transactions very well, it struggles in the presence of mixed workloads\ncontaining long-running scan-centric OLAP queries, as scans have to work their\nway through large amounts of versioned data. To overcome this problem, we\npropose a system, which reintroduces the concept of heterogeneous transaction\nprocessing: OLAP transactions are outsourced to run on separate (virtual)\nsnapshots while OLTP transactions run on the most recent representation of the\ndatabase. Inside both components, MVCC ensures a high degree of concurrency.\nThe biggest challenge of such a heterogeneous approach is to generate the\nsnapshots at a high frequency. Previous approaches heavily suffered from the\ntremendous cost of snapshot creation. In our system, we overcome the\nrestrictions of the OS by introducing a custom system call vm_snapshot, that is\nhand-tailored to our precise needs: it allows fine-granular snapshot creation\nat very high frequencies, rendering the snapshot creation phase orders of\nmagnitudes faster than state-of-the-art approaches. Our experimental evaluation\non a heterogeneous workload based on TPC-H transactions and handcrafted OLTP\ntransactions shows that our system enables significantly higher analytical\ntransaction throughputs on mixed workloads than homogeneous approaches. In this\nsense, we introduce a system that accelerates Analytical processing by\nintroducing custom Kernel functionalities: AnKerDB.\n"]},
{"authors": ["Spyros Sioutas", "Kostas Tsichlas", "Andreas Kosmatopoulos", "Apostolos N. Papadopoulos", "Dimitrios Tsoumakos", "Katerina Doka"], "title": ["Skyline Queries in O(1) time?"], "date": ["2017-09-12T16:51:28Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.03949v1"], "summary": ["  The skyline of a set $P$ of points ($SKY(P)$) consists of the \"best\" points\nwith respect to minimization or maximization of the attribute values. A point\n$p$ dominates another point $q$ if $p$ is as good as $q$ in all dimensions and\nit is strictly better than $q$ in at least one dimension. In this work, we\nfocus on the static $2$-d space and provide expected performance guarantees for\n$3$-sided Range Skyline Queries on the Grid, where $N$ is the cardinality of\n$P$, $B$ the size of a disk block, and $R$ the capacity of main memory. We\npresent the MLR-tree, which offers optimal expected cost for finding planar\nskyline points in a $3$-sided query rectangle, $q=[a,b]\\times(-\\infty,d]$, in\nboth RAM and I/O model on the grid $[1,M]\\times [1,M]$, by single scanning only\nthe points contained in $SKY(P)$. In particular, it supports skyline queries in\na $3$-sided range in $O(t\\cdot t_{PAM}(N))$ time ($O((t/B)\\cdot t_{PAM}(N))$\nI/Os), where $t$ is the answer size and $t_{PAM}(N)$ the time required for\nanswering predecessor queries for $d$ in a PAM (Predecessor Access Method)\nstructure, which is a special component of MLR-tree and stores efficiently\nroot-to-leaf paths or sub-paths. By choosing PAM structures with $O(1)$\nexpected time for predecessor queries under discrete $\\mu$-random distributions\nof the $x$ and $y$ coordinates, MLR-tree supports skyline queries in optimal\n$O(t)$ expected time ($O(t/B)$ expected number of I/Os) with high probability.\nThe space cost becomes superlinear and can be reduced to linear for many\nspecial practical cases. If we choose a PAM structure with $O(1)$ amortized\ntime for batched predecessor queries (under no assumption on distributions of\nthe $x$ and $y$ coordinates), MLR-tree supports batched skyline queries in\noptimal $O(t)$ amortized time, however the space becomes exponential. In\ndynamic case, the update time complexity is affected by a $O(log^{2}N)$ factor.\n"]},
{"authors": ["Herbert Jordan", "Bernhard Scholz", "Pavle Suboti\u0107"], "title": ["Optimal On The Fly Index Selection in Polynomial Time"], "date": ["2017-09-12T04:10:51Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.03685v1"], "summary": ["  The index selection problem (ISP) is an important problem for accelerating\nthe execution of relational queries, and it has received a lot of attention as\na combinatorial knapsack problem in the past. Various solutions to this very\nhard problem have been provided. In contrast to existing literature, we change\nthe underlying assumptions of the problem definition: we adapt the problem for\nsystems that store relations in memory, and use complex specification\nlanguages, e.g., Datalog. In our framework, we decompose complex queries into\nprimitive searches that select tuples in a relation for which an equality\npredicate holds. A primitive search can be accelerated by an index exhibiting a\nworst-case run-time complexity of log-linear time in the size of the output\nresult of the primitive search. However, the overheads associated with\nmaintaining indexes are very costly in terms of memory and computing time.\n  In this work, we present an optimal polynomial-time algorithm that finds the\nminimal set of indexes of a relation for a given set of primitive searches. An\nindex may cover more than one primitive search due to the algebraic properties\nof the search predicate, which is a conjunction of equalities over the\nattributes of a relation. The index search space exhibits a exponential\ncomplexity in the number of attributes in a relation, and, hence brute-force\nalgorithms searching for solutions in the index domain are infeasible. As a\nscaffolding for designing a polynomial-time algorithm, we build a partial order\non search operations and use a constructive version of Dilworth's theorem. We\nshow a strong relationship between chains of primitive searches (forming a\npartial order) and indexes. We demonstrate the effectiveness and efficiency of\nour algorithm for an in-memory Datalog compiler that is able to process\nrelations with billions of entries in memory.\n"]},
{"authors": ["Ahmed Samet", "Thomas Guyet", "Benjamin Negrevergne", "Tien-Tuan Dao", "Tuan Nha Hoang", "Marie-Christine Ho Ba Tho"], "title": ["Expert Opinion Extraction from a Biomedical Database"], "date": ["2017-09-11T07:23:00Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.03270v1"], "summary": ["  In this paper, we tackle the problem of extracting frequent opinions from\nuncertain databases. We introduce the foundation of an opinion mining approach\nwith the definition of pattern and support measure. The support measure is\nderived from the commitment definition. A new algorithm called OpMiner that\nextracts the set of frequent opinions modelled as a mass functions is detailed.\nFinally, we apply our approach on a real-world biomedical database that stores\nopinions of experts to evaluate the reliability level of biomedical data.\nPerformance analysis showed a better quality patterns for our proposed model in\ncomparison with literature-based methods.\n"]},
{"authors": ["Thomas Guyet", "Ren\u00e9 Quiniou", "V\u00e9ronique Masson"], "title": ["Mining relevant interval rules"], "date": ["2017-09-11T07:18:58Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.03267v1"], "summary": ["  This article extends the method of Garriga et al. for mining relevant rules\nto numerical attributes by extracting interval-based pattern rules. We propose\nan algorithm that extracts such rules from numerical datasets using the\ninterval-pattern approach from Kaytoue et al. This algorithm has been\nimplemented and evaluated on real datasets.\n"]},
{"authors": ["Sainyam Galhotra", "Yuriy Brun", "Alexandra Meliou"], "title": ["Fairness Testing: Testing Software for Discrimination"], "date": ["2017-09-11T02:45:22Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.03221v1"], "summary": ["  This paper defines software fairness and discrimination and develops a\ntesting-based method for measuring if and how much software discriminates,\nfocusing on causality in discriminatory behavior. Evidence of software\ndiscrimination has been found in modern software systems that recommend\ncriminal sentences, grant access to financial products, and determine who is\nallowed to participate in promotions. Our approach, Themis, generates efficient\ntest suites to measure discrimination. Given a schema describing valid system\ninputs, Themis generates discrimination tests automatically and does not\nrequire an oracle. We evaluate Themis on 20 software systems, 12 of which come\nfrom prior work with explicit focus on avoiding discrimination. We find that\n(1) Themis is effective at discovering software discrimination, (2)\nstate-of-the-art techniques for removing discrimination from algorithms fail in\nmany situations, at times discriminating against as much as 98% of an input\nsubdomain, (3) Themis optimizations are effective at producing efficient test\nsuites for measuring discrimination, and (4) Themis is more efficient on\nsystems that exhibit more discrimination. We thus demonstrate that fairness\ntesting is a critical aspect of the software development cycle in domains with\npossible discrimination and provide initial tools for measuring software\ndiscrimination.\n"]},
{"authors": ["Chao-Lin Liu", "Hongsu Wang"], "title": ["Matrix and Graph Operations for Relationship Inference: An Illustration\n  with the Kinship Inference in the China Biographical Database"], "date": ["2017-09-09T16:01:08Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.02968v1"], "summary": ["  Biographical databases contain diverse information about individuals. Person\nnames, birth information, career, friends, family and special achievements are\nsome possible items in the record for an individual. The relationships between\nindividuals, such as kinship and friendship, provide invaluable insights about\nhidden communities which are not directly recorded in databases. We show that\nsome simple matrix and graph-based operations are effective for inferring\nrelationships among individuals, and illustrate the main ideas with the China\nBiographical Database (CBDB).\n"]},
{"authors": ["Nachshon Cohen", "Michal Friedman", "James R. Larus"], "title": ["Efficient Logging in Non-Volatile Memory by Exploiting Coherency\n  Protocols"], "date": ["2017-09-08T09:35:29Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.02610v1"], "summary": ["  Non-volatile memory (NVM) technologies such as PCM, ReRAM and STT-RAM allow\nprocessors to directly write values to persistent storage at speeds that are\nsignificantly faster than previous durable media such as hard drives or SSDs.\nMany applications of NVM are constructed on a logging subsystem, which enables\noperations to appear to execute atomically and facilitates recovery from\nfailures. Writes to NVM, however, pass through a processor's memory system,\nwhich can delay and reorder them and can impair the correctness and cost of\nlogging algorithms.\n  Reordering arises because of out-of-order execution in a CPU and the\ninter-processor cache coherence protocol. By carefully considering the\nproperties of these reorderings, this paper develops a logging protocol that\nrequires only one round trip to non-volatile memory while avoiding expensive\ncomputations. We show how to extend the logging protocol to building a\npersistent set (hash map) that also requires only a single round trip to\nnon-volatile memory for insertion, updating, or deletion.\n"]},
{"authors": ["Shubhadip Mitra", "Priya Saraf", "Arnab Bhattacharya"], "title": ["TIPS: Mining Top-K Locations to Minimize User-Inconvenience for\n  Trajectory-Aware Services"], "date": ["2017-09-07T16:40:20Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.02343v2"], "summary": ["  Facility location problems aim to identify the best locations to set up new\nservices. Majority of the existing works typically assume that the users of the\nservice are static. However, there exists a wide array of services such as fuel\nstations, ATMs, food joints, etc., that are widely accessed by mobile users,\nbesides the static ones. Such trajectory-aware services should, therefore,\nfactor in the trajectories of its users rather than simply their static\nlocations. In this work, we introduce the problem of optimal placement of\nfacility locations for such trajectory-aware services that minimize the user\ninconvenience. The inconvenience of a user is defined as the extra distance\ntraveled by her from her regular path to avail a service. We call this as the\n\\tips problem (Trajectory-aware Inconvenience-minimizing Placement of Services)\nand consider two variants of it. While the goal of the first variant (called\nMAX-TIPS) is to minimize the maximum inconvenience faced by any user, that of\nthe second variant (called AVG-TIPS) is to minimize the average inconvenience.\nAs both these problems are NP-hard, we propose multiple efficient heuristics to\nsolve them. Empirical evaluation on real urban scale road networks validate the\nefficiency and effectiveness of the proposed heuristics. Time:\n"]},
{"authors": ["Harry Kalodner", "Steven Goldfeder", "Alishah Chator", "Malte M\u00f6ser", "Arvind Narayanan"], "title": ["BlockSci: Design and applications of a blockchain analysis platform"], "date": ["2017-09-08T00:11:38Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.02489v1"], "summary": ["  Analysis of blockchain data is useful for both scientific research and\ncommercial applications. We present BlockSci, an open-source software platform\nfor blockchain analysis. BlockSci is versatile in its support for different\nblockchains and analysis tasks. It incorporates an in-memory, analytical\n(rather than transactional) database, making it several hundred times faster\nthan existing tools. We describe BlockSci's design and present four analyses\nthat illustrate its capabilities.\n  This is a working paper that accompanies the first public release of\nBlockSci, available at https://github.com/citp/BlockSci. We seek input from the\ncommunity to further develop the software and explore other potential\napplications.\n"]},
{"authors": ["Ali Pesaranghader", "Herna Viktor", "Eric Paquet"], "title": ["Reservoir of Diverse Adaptive Learners and Stacking Fast Hoeffding Drift\n  Detection Methods for Evolving Data Streams"], "date": ["2017-09-07T21:19:24Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.02457v1"], "summary": ["  The last decade has seen a surge of interest in adaptive learning algorithms\nfor data stream classification, with applications ranging from predicting ozone\nlevel peaks, learning stock market indicators, to detecting computer security\nviolations. In addition, a number of methods have been developed to detect\nconcept drifts in these streams. Consider a scenario where we have a number of\nclassifiers with diverse learning styles and different drift detectors.\nIntuitively, the current 'best' (classifier, detector) pair is application\ndependent and may change as a result of the stream evolution. Our research\nbuilds on this observation. We introduce the $\\mbox{Tornado}$ framework that\nimplements a reservoir of diverse classifiers, together with a variety of drift\ndetection algorithms. In our framework, all (classifier, detector) pairs\nproceed, in parallel, to construct models against the evolving data streams. At\nany point in time, we select the pair which currently yields the best\nperformance. We further incorporate two novel stacking-based drift detection\nmethods, namely the $\\mbox{FHDDMS}$ and $\\mbox{FHDDMS}_{add}$ approaches. The\nexperimental evaluation confirms that the current 'best' (classifier, detector)\npair is not only heavily dependent on the characteristics of the stream, but\nalso that this selection evolves as the stream flows. Further, our\n$\\mbox{FHDDMS}$ variants detect concept drifts accurately in a timely fashion\nwhile outperforming the state-of-the-art.\n"]},
{"authors": ["Yiqiu Wang", "Anshumali Shrivastava", "Junghee Ryu"], "title": ["FLASH: Randomized Algorithms Accelerated over CPU-GPU for Ultra-High\n  Dimensional Similarity Search"], "date": ["2017-09-04T23:09:19Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.01190v1"], "summary": ["  We present FLASH ({\\bf F}ast {\\bf L}SH {\\bf A}lgorithm for {\\bf S}imilarity\nsearch accelerated with {\\bf H}PC (High-Performance Computing)), a similarity\nsearch system for ultra-high dimensional datasets on a single machine, which\ndoes not require similarity computation. Our system is an auspicious\nillustration of the power of randomized algorithms carefully tailored for\nhigh-performance computing platforms. We leverage LSH style randomized indexing\nprocedure and combine it with several principled techniques, such as reservoir\nsampling, recent advances in one-pass minwise hashing, and count based\nestimations. The combination, while retaining sound theoretical guarantees,\nreduces the computational as well as parallelization overhead of our proposal.\nWe provide CPU and hybrid CPU-GPU implementations of FLASH for replicability of\nour results https://github.com/RUSH-LAB/Flash.\n  We evaluate FLASH on several real high dimensional datasets coming from\ndifferent domains including text, malicious URL, click-through prediction,\nsocial networks, etc. Our experiments shed new light on the difficulties\nassociated with datasets having several millions of dimensions. Current\nstate-of-the-art implementations either fail on the presented scale or are\norders of magnitude slower than our system. FLASH is capable of computing an\napproximate k-NN graph, from scratch, over full webspam dataset (1.3 billion\nnonzeros) in less than 10 seconds. Computing full k-NN graph in less than 10\nseconds on webspam dataset, using brute-force ($n^2D$), will require at least\n20 TFLOPS. We hope that FLASH gets adopted in practice.\n"]},
{"authors": ["Subhashree S", "P Sreenivasa Kumar"], "title": ["Enriching Linked Datasets with New Object Properties"], "date": ["2016-06-24T06:00:42Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1606.07572v3"], "summary": ["  Although several RDF knowledge bases are available through the LOD\ninitiative, the ontology schema of such linked datasets is not very rich. In\nparticular, they lack object properties. The problem of finding new object\nproperties (and their instances) between any two given classes has not been\ninvestigated in detail in the context of Linked Data. In this paper, we present\nDART (Detecting Arbitrary Relations for enriching T-Boxes of Linked Data) - an\nunsupervised solution to enrich the LOD cloud with new object properties\nbetween two given classes. DART exploits contextual similarity to identify text\npatterns from the web corpus that can potentially represent relations between\nindividuals. These text patterns are then clustered by means of paraphrase\ndetection to capture the object properties between the two given LOD classes.\nDART also performs fully automated mapping of the discovered relations to the\nproperties in the linked dataset. This serves many purposes such as\nidentification of completely new relations, elimination of irrelevant\nrelations, and generation of prospective property axioms. We have empirically\nevaluated our approach on several pairs of classes and found that the system\ncan indeed be used for enriching the linked datasets with new object properties\nand their instances. We compared DART with newOntExt system which is an\noffshoot of the NELL (Never-Ending Language Learning) effort. Our experiments\nreveal that DART gives better results than newOntExt with respect to both the\ncorrectness, as well as the number of relations.\n"]},
{"authors": ["Sebastian Bre\u00df", "Bastian K\u00f6cher", "Henning Funke", "Tilmann Rabl", "Volker Markl"], "title": ["Generating Custom Code for Efficient Query Execution on Heterogeneous\n  Processors"], "date": ["2017-09-03T11:16:31Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.00700v1"], "summary": ["  Processor manufacturers build increasingly specialized processors to mitigate\nthe effects of the power wall to deliver improved performance. Currently,\ndatabase engines are manually optimized for each processor: A costly and error\nprone process.\n  In this paper, we propose concepts to enable the database engine to perform\nper-processor optimization automatically. Our core idea is to create variants\nof generated code and to learn a fast variant for each processor. We create\nvariants by modifying parallelization strategies, specializing data structures,\nand applying different code transformations.\n  Our experimental results show that the performance of variants may diverge up\nto two orders of magnitude. Therefore, we need to generate custom code for each\nprocessor to achieve peak performance. We show that our approach finds a fast\ncustom variant for multi-core CPUs, GPUs, and MICs.\n"]},
{"authors": ["Xi He", "Ashwin Machanavajjhala", "Cheryl Flynn", "Divesh Srivastava"], "title": ["Composing Differential Privacy and Secure Computation: A case study on\n  scaling private record linkage"], "date": ["2017-02-02T03:57:52Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.00535v4"], "summary": ["  Private record linkage (PRL) is the problem of identifying pairs of records\nthat are similar as per an input matching rule from databases held by two\nparties that do not trust one another. We identify three key desiderata that a\nPRL solution must ensure: 1) perfect precision and high recall of matching\npairs, 2) a proof of end-to-end privacy, and 3) communication and computational\ncosts that scale subquadratically in the number of input records. We show that\nall of the existing solutions for PRL - including secure 2-party computation\n(S2PC), and their variants that use non-private or differentially private (DP)\nblocking to ensure subquadratic cost - violate at least one of the three\ndesiderata. In particular, S2PC techniques guarantee end-to-end privacy but\nhave either low recall or quadratic cost. In contrast, no end-to-end privacy\nguarantee has been formalized for solutions that achieve subquadratic cost.\nThis is true even for solutions that compose DP and S2PC: DP does not permit\nthe release of any exact information about the databases, while S2PC algorithms\nfor PRL allow the release of matching records.\n  In light of this deficiency, we propose a novel privacy model, called output\nconstrained differential privacy, that shares the strong privacy protection of\nDP, but allows for the truthful release of the output of a certain function\napplied to the data. We apply this to PRL, and show that protocols satisfying\nthis privacy model permit the disclosure of the true matching records, but\ntheir execution is insensitive to the presence or absence of a single\nnon-matching record. We find that prior work that combine DP and S2PC\ntechniques even fail to satisfy this end-to-end privacy model. Hence, we\ndevelop novel protocols that provably achieve this end-to-end privacy\nguarantee, together with the other two desiderata of PRL.\n"]},
{"authors": ["Markus Nentwig", "Anika Gro\u00df", "Maximilian M\u00f6ller", "Erhard Rahm"], "title": ["Distributed Holistic Clustering on Linked Data"], "date": ["2017-08-30T14:36:19Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1708.09299v1"], "summary": ["  Link discovery is an active field of research to support data integration in\nthe Web of Data. Due to the huge size and number of available data sources,\nefficient and effective link discovery is a very challenging task. Common\npairwise link discovery approaches do not scale to many sources with very large\nentity sets. We here propose a distributed holistic approach to link many data\nsources based on a clustering of entities that represent the same real-world\nobject. Our clustering approach provides a compact and fused representation of\nentities, and can identify errors in existing links as well as many new links.\nWe support a distributed execution of the clustering approach to achieve faster\nexecution times and scalability for large real-world data sets. We provide a\nnovel gold standard for multi-source clustering, and evaluate our methods with\nrespect to effectiveness and efficiency for large data sets from the geographic\nand music domains.\n"]},
{"authors": ["Somayeh Sobati Moghadam", "J\u00e9r\u00f4me Darmont", "G\u00e9rald Gavin"], "title": ["Enforcing Privacy in Cloud Databases"], "date": ["2017-08-30T08:47:20Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1708.09171v1"], "summary": ["  Outsourcing databases, i.e., resorting to Database-as-a-Service (DBaaS), is\nnowadays a popular choice due to the elasticity, availability, scalability and\npay-as-you-go features of cloud computing. However, most data are sensitive to\nsome extent, and data privacy remains one of the top concerns to DBaaS users,\nfor obvious legal and competitive reasons.In this paper, we survey the\nmechanisms that aim at making databases secure in a cloud environment, and\ndiscuss current pitfalls and related research challenges.\n"]},
{"authors": ["Ester Livshits", "Benny Kimelfeld"], "title": ["The Complexity of Computing a Cardinality Repair for Functional\n  Dependencies"], "date": ["2017-08-30T07:03:58Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1708.09140v1"], "summary": ["  For a relation that violates a set of functional dependencies, we consider\nthe task of finding a maximum number of pairwise-consistent tuples, or what is\nknown as a \"cardinality repair.\" We present a polynomial-time algorithm that,\nfor certain fixed relation schemas (with functional dependencies), computes a\ncardinality repair. Moreover, we prove that on any of the schemas not covered\nby the algorithm, finding a cardinality repair is, in fact, an NP-hard problem.\nIn particular, we establish a dichotomy in the complexity of computing a\ncardinality repair, and we present an efficient algorithm to determine whether\na given schema belongs to the positive side or the negative side of the\ndichotomy.\n"]},
{"authors": ["Wolfgang Fischl", "Georg Gottlob", "Reinhard Pichler"], "title": ["General and Fractional Hypertree Decompositions: Hard and Easy Cases"], "date": ["2016-11-03T16:54:37Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1611.01090v7"], "summary": ["  Hypertree decompositions, as well as the more powerful generalized hypertree\ndecompositions (GHDs), and the yet more general fractional hypertree\ndecompositions (FHD) are hypergraph decomposition methods successfully used for\nanswering conjunctive queries and for the solution of constraint satisfaction\nproblems. Every hypergraph H has a width relative to each of these\ndecomposition methods: its hypertree width hw(H), its generalized hypertree\nwidth ghw(H), and its fractional hypertree width fhw(H), respectively.\n  It is known that hw(H) <= k can be checked in polynomial time for fixed k,\nwhile checking ghw(H) <= k is NP-complete for any k greater than or equal to 3.\nThe complexity of checking fhw(H) <= k for a fixed k has been open for more\nthan a decade.\n  We settle this open problem by showing that checking fhw(H) <= k is\nNP-complete, even for k=2. The same construction allows us to prove also the\nNP-completeness of checking ghw(H) <= k for k=2. After proving these hardness\nresults, we identify meaningful restrictions, for which checking for bounded\nghw or fhw becomes tractable.\n"]},
{"authors": ["Prajakta Kalmegh", "Shivnath Babu", "Sudeepa Roy"], "title": ["Analyzing Query Performance and Attributing Blame for Contentions in a\n  Cluster Computing Framework"], "date": ["2017-08-28T17:44:44Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1708.08435v1"], "summary": ["  Analyzing contention for resources in a cluster computing environment\naccurately is critical in order to understand the performance interferences\nfaced by a query due to concurrent query executions, and to better manage the\nworkload in the cluster. Today no tools exist to help an admin perform a deep\nanalysis of resource contentions taking into account the complex interactions\namong different queries, their stages, and tasks in a shared cluster. In this\npaper, we present ProtoXplore - a Proto or first system to eXplore the\ninteractions between concurrent queries in a shared cluster. We construct a\nmulti-level directed acyclic graph called ProtoGraph to formally capture\ndifferent types of explanations that link the performance of concurrent\nqueries. In particular, (a) we designate the components of a query's lost\n(wait) time as Immediate Explanations towards its observed performance, (b)\nrepresent the rate of contention per machine as Deep Explanations, and (c)\nassign responsibility to concurrent queries through Blame Explanations. We\ndevelop new metrics to accurately quantify the impact and distribute the blame\namong concurrent queries. We perform an extensive experimental evaluation using\nProtoXplore to analyze the query interactions of TPCDS queries on Apache Spark\nusing microbenchmarks illustrating the effectiveness of our approach, and\nillustrate how the output from ProtoXplore can be used by alternate scheduling\nand task placement strategies to help improve the performance of affected\nqueries in recurring executions.\n"]},
{"authors": ["Tianyue Zheng", "Weihong Deng", "Jiani Hu"], "title": ["Cross-Age LFW: A Database for Studying Cross-Age Face Recognition in\n  Unconstrained Environments"], "date": ["2017-08-28T06:07:27Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1708.08197v1"], "summary": ["  Labeled Faces in the Wild (LFW) database has been widely utilized as the\nbenchmark of unconstrained face verification and due to big data driven machine\nlearning methods, the performance on the database approaches nearly 100%.\nHowever, we argue that this accuracy may be too optimistic because of some\nlimiting factors. Besides different poses, illuminations, occlusions and\nexpressions, cross-age face is another challenge in face recognition. Different\nages of the same person result in large intra-class variations and aging\nprocess is unavoidable in real world face verification. However, LFW does not\npay much attention on it. Thereby we construct a Cross-Age LFW (CALFW) which\ndeliberately searches and selects 3,000 positive face pairs with age gaps to\nadd aging process intra-class variance. Negative pairs with same gender and\nrace are also selected to reduce the influence of attribute difference between\npositive/negative pairs and achieve face verification instead of attributes\nclassification. We evaluate several metric learning and deep learning methods\non the new database. Compared to the accuracy on LFW, the accuracy drops about\n10%-17% on CALFW.\n"]},
{"authors": ["Xiaofei Wang", "Qianhong Wu", "Yuqing Zhang"], "title": ["T-DB: Toward Fully Functional Transparent Encrypted Databases in DBaaS\n  Framework"], "date": ["2017-08-28T05:18:11Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1708.08191v1"], "summary": ["  Individuals and organizations tend to migrate their data to clouds,\nespecially in a DataBase as a Service (DBaaS) pattern. The major obstacle is\nthe conflict between secrecy and utilization of the relational database to be\noutsourced. We address this obstacle with a Transparent DataBase (T-DB) system\nstrictly following the unmodified DBaaS framework. A database owner outsources\nan encrypted database to a cloud platform, needing only to store the secret\nkeys for encryption and an empty table header for the database; the database\nusers can make almost all types of queries on the encrypted database as usual;\nand the cloud can process ciphertext queries as if the database were not\nencrypted. Experimentations in realistic cloud environments demonstrate that\nT-DB has perfect query answer precision and outstanding performance.\n"]},
{"authors": ["Sebastian Neef"], "title": ["Implementation and Evaluation of a Framework to calculate Impact\n  Measures for Wikipedia Authors"], "date": ["2017-08-26T21:33:02Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.01142v1"], "summary": ["  Wikipedia, an open collaborative website, can be edited by anyone, even\nanonymously, thus becoming victim to ill-intentioned changes. Therefore,\nranking Wikipedia authors by calculating impact measures based on the edit\nhistory can help to identify reputational users or harmful activity such as\nvandalism \\cite{Adler:2008:MAC:1822258.1822279}. However, processing millions\nof edits on one system can take a long time. The author implements an open\nsource framework to calculate such rankings in a distributed way (MapReduce)\nand evaluates its performance on various sized datasets. A reimplementation of\nthe contribution measures by \\citeauthor{Adler:2008:MAC:1822258.1822279}\ndemonstrates its extensibility and usability, as well as problems of handling\nhuge datasets and their possible resolutions. The results put different\nperformance optimizations into perspective and show that horizontal scaling can\ndecrease the total processing time.\n"]},
{"authors": ["Vincent Bindschaedler", "Reza Shokri", "Carl A. Gunter"], "title": ["Plausible Deniability for Privacy-Preserving Data Synthesis"], "date": ["2017-08-26T14:13:28Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1708.07975v1"], "summary": ["  Releasing full data records is one of the most challenging problems in data\nprivacy. On the one hand, many of the popular techniques such as data\nde-identification are problematic because of their dependence on the background\nknowledge of adversaries. On the other hand, rigorous methods such as the\nexponential mechanism for differential privacy are often computationally\nimpractical to use for releasing high dimensional data or cannot preserve high\nutility of original data due to their extensive data perturbation.\n  This paper presents a criterion called plausible deniability that provides a\nformal privacy guarantee, notably for releasing sensitive datasets: an output\nrecord can be released only if a certain amount of input records are\nindistinguishable, up to a privacy parameter. This notion does not depend on\nthe background knowledge of an adversary. Also, it can efficiently be checked\nby privacy tests. We present mechanisms to generate synthetic datasets with\nsimilar statistical properties to the input data and the same format. We study\nthis technique both theoretically and experimentally. A key theoretical result\nshows that, with proper randomization, the plausible deniability mechanism\ngenerates differentially private synthetic data. We demonstrate the efficiency\nof this generative technique on a large dataset; it is shown to preserve the\nutility of original data with respect to various statistical analysis and\nmachine learning measures.\n"]},
{"authors": ["Eli Dart", "Michael F. Wehner", " Prabhat"], "title": ["An Assessment of Data Transfer Performance for Large-Scale Climate Data\n  Analysis and Recommendations for the Data Infrastructure for CMIP6"], "date": ["2017-08-26T03:31:29Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1709.09575v1"], "summary": ["  We document the data transfer workflow, data transfer performance, and other\naspects of staging approximately 56 terabytes of climate model output data from\nthe distributed Coupled Model Intercomparison Project (CMIP5) archive to the\nNational Energy Research Supercomputing Center (NERSC) at the Lawrence Berkeley\nNational Laboratory required for tracking and characterizing extratropical\nstorms, a phenomena of importance in the mid-latitudes. We present this\nanalysis to illustrate the current challenges in assembling multi-model data\nsets at major computing facilities for large-scale studies of CMIP5 data.\nBecause of the larger archive size of the upcoming CMIP6 phase of model\nintercomparison, we expect such data transfers to become of increasing\nimportance, and perhaps of routine necessity. We find that data transfer rates\nusing the ESGF are often slower than what is typically available to US\nresidences and that there is significant room for improvement in the data\ntransfer capabilities of the ESGF portal and data centers both in terms of\nworkflow mechanics and in data transfer performance. We believe performance\nimprovements of at least an order of magnitude are within technical reach using\ncurrent best practices, as illustrated by the performance we achieved in\ntransferring the complete raw data set between two high performance computing\nfacilities. To achieve these performance improvements, we recommend: that\ncurrent best practices (such as the Science DMZ model) be applied to the data\nservers and networks at ESGF data centers; that sufficient financial and human\nresources be devoted at the ESGF data centers for systems and network\nengineering tasks to support high performance data movement; and that\nperformance metrics for data transfer between ESGF data centers and major\ncomputing facilities used for climate data analysis be established, regularly\ntested, and published.\n"]},
{"authors": ["Christopher R. Aberger", "Andrew Lamb", "Kunle Olukotun", "Christopher R\u00e9"], "title": ["LevelHeaded: Making Worst-Case Optimal Joins Work in the Common Case"], "date": ["2017-08-25T18:49:30Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1708.07859v1"], "summary": ["  Pipelines combining SQL-style business intelligence (BI) queries and linear\nalgebra (LA) are becoming increasingly common in industry. As a result, there\nis a growing need to unify these workloads in a single framework.\nUnfortunately, existing solutions either sacrifice the inherent benefits of\nexclusively using a relational database (e.g. logical and physical\nindependence) or incur orders of magnitude performance gaps compared to\nspecialized engines (or both). In this work we study applying a new type of\nquery processing architecture to standard BI and LA benchmarks. To do this we\npresent a new in-memory query processing engine called LevelHeaded. LevelHeaded\nuses worst-case optimal joins as its core execution mechanism for both BI and\nLA queries. With LevelHeaded, we show how crucial optimizations for BI and LA\nqueries can be captured in a worst-case optimal query architecture. Using these\noptimizations, LevelHeaded outperforms other relational database engines\n(LogicBlox, MonetDB, and HyPer) by orders of magnitude on standard LA\nbenchmarks, while performing on average within 31% of the best-of-breed BI\n(HyPer) and LA (Intel MKL) solutions on their own benchmarks. Our results show\nthat such a single query processing architecture is capable of delivering\ncompetitive performance on both BI and LA queries.\n"]},
{"authors": ["Shuo Han", "Lei Zou", "Jeffrey Xu Yu", "Dongyan Zhao"], "title": ["Keyword Search on RDF Graphs - A Query Graph Assembly Approach"], "date": ["2017-04-01T18:17:28Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1704.00205v2"], "summary": ["  Keyword search provides ordinary users an easy-to-use interface for querying\nRDF data. Given the input keywords, in this paper, we study how to assemble a\nquery graph that is to represent user's query intention accurately and\nefficiently. Based on the input keywords, we first obtain the elementary query\ngraph building blocks, such as entity/class vertices and predicate edges. Then,\nwe formally define the query graph assembly (QGA) problem. Unfortunately, we\nprove theoretically that QGA is a NP-complete problem. In order to solve that,\nwe design some heuristic lower bounds and propose a bipartite graph\nmatching-based best-first search algorithm. The algorithm's time complexity is\n$O(k^{2l} \\cdot l^{3l})$, where $l$ is the number of the keywords and $k$ is a\ntunable parameter, i.e., the maximum number of candidate entity/class vertices\nand predicate edges allowed to match each keyword. Although QGA is intractable,\nboth $l$ and $k$ are small in practice. Furthermore, the algorithm's time\ncomplexity does not depend on the RDF graph size, which guarantees the good\nscalability of our system in large RDF graphs. Experiments on DBpedia and\nFreebase confirm the superiority of our system on both effectiveness and\nefficiency.\n"]},
{"authors": ["Tommaso Soru", "Edgard Marx", "Diego Moussallem", "Gustavo Publio", "Andr\u00e9 Valdestilhas", "Diego Esteves", "Ciro Baron Neto"], "title": ["SPARQL as a Foreign Language"], "date": ["2017-08-25T06:41:55Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1708.07624v1"], "summary": ["  In the last years, the Linked Data Cloud has achieved a size of more than 100\nbillion facts pertaining to a multitude of domains. However, accessing this\ninformation has been significantly challenging for lay users. Approaches to\nproblems such as Question Answering on Linked Data and Link Discovery have\nnotably played a role in increasing information access. These approaches are\noften based on handcrafted and/or statistical models derived from data\nobservation. Recently, Deep Learning architectures based on Neural Networks\ncalled seq2seq have shown to achieve state-of-the-art results at translating\nsequences into sequences. In this direction, we propose Neural SPARQL Machines,\nend-to-end deep architectures to translate any natural language expression into\nsentences encoding SPARQL queries. Our preliminary results, restricted on\nselected DBpedia classes, show that Neural SPARQL Machines are a promising\napproach for Question Answering on Linked Data, as they can deal with known\nproblems such as vocabulary mismatch and perform graph pattern composition.\n"]},
{"authors": ["Vijaya Krishna Yalavarthi", "Xiangyu Ke", "Arijit Khan"], "title": ["Select Your Questions Wisely: For Entity Resolution With Crowd Errors"], "date": ["2017-01-28T13:14:00Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.08288v2"], "summary": ["  Crowdsourcing is becoming increasingly important in entity resolution tasks\ndue to their inherent complexity such as clustering of images and natural\nlanguage processing. Humans can provide more insightful information for these\ndifficult problems compared to machine-based automatic techniques.\nNevertheless, human workers can make mistakes due to lack of domain expertise\nor seriousness, ambiguity, or even due to malicious intents. The\nstate-of-the-art literature usually deals with human errors via majority voting\nor by assigning a universal error rate over crowd workers. However, such\napproaches are incomplete, and often inconsistent, because the expertise of\ncrowd workers are diverse with possible biases, thereby making it largely\ninappropriate to assume a universal error rate for all workers over all\ncrowdsourcing tasks.\n  To this end, we mitigate the above challenges by considering an uncertain\ngraph model, where the edge probability between two records A and B denotes the\nratio of crowd workers who voted Yes on the question if A and B are same\nentity. In order to reflect independence across different crowdsourcing tasks,\nwe apply the well-established notion of possible worlds, and develop\nparameter-free algorithms both for next crowdsourcing, as well as for entity\nresolution problems. In particular, using our framework, the problem of entity\nresolution becomes equivalent to finding the maximum-likelihood clustering;\nwhereas for the next crowdsourcing, we identify the record pair that maximally\nincreases the reliability of the maximum-likelihood clustering. Based on\ndetailed empirical analysis over real-world datasets, we find that our proposed\nsolution, PERC (probabilistic entity resolution with imperfect crowd) improves\nthe quality by 15% and reduces the overall cost by 50% for the\ncrowdsourcing-based entity resolution problem.\n"]},
{"authors": ["Th\u00f4ng T. Nguy\u00ean", "Siu Cheung Hui"], "title": ["Privacy-Preserving Mechanisms for Parametric Survival Analysis with\n  Weibull Distribution"], "date": ["2017-07-02T03:29:00Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1708.04517v2"], "summary": ["  Survival analysis studies the statistical properties of the time until an\nevent of interest occurs. It has been commonly used to study the effectiveness\nof medical treatments or the lifespan of a population. However, survival\nanalysis can potentially leak confidential information of individuals in the\ndataset. The state-of-the-art techniques apply ad-hoc privacy-preserving\nmechanisms on publishing results to protect the privacy. These techniques\nusually publish sanitized and randomized answers which promise to protect the\nprivacy of individuals in the dataset but without providing any formal\nmechanism on privacy protection. In this paper, we propose private mechanisms\nfor parametric survival analysis with Weibull distribution. We prove that our\nproposed mechanisms achieve differential privacy, a robust and rigorous\ndefinition of privacy-preservation. Our mechanisms exploit the property of\nlocal sensitivity to carefully design a utility function which enables us to\npublish parameters of Weibull distribution with high precision. Our\nexperimental studies show that our mechanisms can publish useful answers and\noutperform other differentially private techniques on real datasets.\n"]},
{"authors": ["Th\u00f4ng T. Nguy\u00ean", "Siu Cheung Hui"], "title": ["Differentially Private Regression for Discrete-Time Survival Analysis"], "date": ["2017-08-24T14:25:44Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1708.07436v2"], "summary": ["  In survival analysis, regression models are used to understand the effects of\nexplanatory variables (e.g., age, sex, weight, etc.) to the survival\nprobability. However, for sensitive survival data such as medical data, there\nare serious concerns about the privacy of individuals in the data set when\nmedical data is used to fit the regression models. The closest work addressing\nsuch privacy concerns is the work on Cox regression which linearly projects the\noriginal data to a lower dimensional space. However, the weakness of this\napproach is that there is no formal privacy guarantee for such projection. In\nthis work, we aim to propose solutions for the regression problem in survival\nanalysis with the protection of differential privacy which is a golden standard\nof privacy protection in data privacy research. To this end, we extend the\nOutput Perturbation and Objective Perturbation approaches which are originally\nproposed to protect differential privacy for the Empirical Risk Minimization\n(ERM) problems. In addition, we also propose a novel sampling approach based on\nthe Markov Chain Monte Carlo (MCMC) method to practically guarantee\ndifferential privacy with better accuracy. We show that our proposed approaches\nachieve good accuracy as compared to the non-private results while guaranteeing\ndifferential privacy for individuals in the private data set.\n"]},
{"authors": ["Tian Li", "Jie Zhong", "Ji Liu", "Wentao Wu", "Ce Zhang"], "title": ["Ease.ml: Towards Multi-tenant Resource Sharing for Machine Learning\n  Workloads"], "date": ["2017-08-24T08:21:28Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1708.07308v1"], "summary": ["  We present ease.ml, a declarative machine learning service platform we built\nto support more than ten research groups outside the computer science\ndepartments at ETH Zurich for their machine learning needs. With ease.ml, a\nuser defines the high-level schema of a machine learning application and\nsubmits the task via a Web interface. The system automatically deals with the\nrest, such as model selection and data movement. In this paper, we describe the\nease.ml architecture and focus on a novel technical problem introduced by\nease.ml regarding resource allocation. We ask, as a \"service provider\" that\nmanages a shared cluster of machines among all our users running machine\nlearning workloads, what is the resource allocation strategy that maximizes the\nglobal satisfaction of all our users?\n  Resource allocation is a critical yet subtle issue in this multi-tenant\nscenario, as we have to balance between efficiency and fairness. We first\nformalize the problem that we call multi-tenant model selection, aiming for\nminimizing the total regret of all users running automatic model selection\ntasks. We then develop a novel algorithm that combines multi-armed bandits with\nBayesian optimization and prove a regret bound under the multi-tenant setting.\nFinally, we report our evaluation of ease.ml on synthetic data and on one\nservice we are providing to our users, namely, image classification with deep\nneural networks. Our experimental evaluation results show that our proposed\nsolution can be up to 9.8x faster in achieving the same global quality for all\nusers as the two popular heuristics used by our users before ease.ml.\n"]},
{"authors": ["Zhida Chen", "Gao Cong", "Zhenjie Zhang", "Tom Z. J. Fu", "Lisi Chen"], "title": ["Distributed Publish/Subscribe Query Processing on the Spatio-Textual\n  Data Stream"], "date": ["2016-12-08T08:51:23Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1612.02564v4"], "summary": ["  Huge amount of data with both space and text information, e.g., geo-tagged\ntweets, is flooding on the Internet. Such spatio-textual data stream contains\nvaluable information for millions of users with various interests on different\nkeywords and locations. Publish/subscribe systems enable efficient and\neffective information distribution by allowing users to register continuous\nqueries with both spatial and textual constraints. However, the explosive\ngrowth of data scale and user base has posed challenges to the existing\ncentralized publish/subscribe systems for spatio-textual data streams.\n  In this paper, we propose our distributed publish/subscribe system, called\nPS2Stream, which digests a massive spatio-textual data stream and directs the\nstream to target users with registered interests. Compared with existing\nsystems, PS2Stream achieves a better workload distribution in terms of both\nminimizing the total amount of workload and balancing the load of workers. To\nachieve this, we propose a new workload distribution algorithm considering both\nspace and text properties of the data. Additionally, PS2Stream supports dynamic\nload adjustments to adapt to the change of the workload, which makes PS2Stream\nadaptive. Extensive empirical evaluation, on commercial cloud computing\nplatform with real data, validates the superiority of our system design and\nadvantages of our techniques on system performance improvement.\n"]},
{"authors": ["Byung H. Park", "Saurabh Hukerikar", "Ryan Adamson", "Christian Engelmann"], "title": ["Big Data Meets HPC Log Analytics: Scalable Approach to Understanding\n  Systems at Extreme Scale"], "date": ["2017-08-23T04:41:47Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1708.06884v1"], "summary": ["  Today's high-performance computing (HPC) systems are heavily instrumented,\ngenerating logs containing information about abnormal events, such as critical\nconditions, faults, errors and failures, system resource utilization, and about\nthe resource usage of user applications. These logs, once fully analyzed and\ncorrelated, can produce detailed information about the system health, root\ncauses of failures, and analyze an application's interactions with the system,\nproviding valuable insights to domain scientists and system administrators.\nHowever, processing HPC logs requires a deep understanding of hardware and\nsoftware components at multiple layers of the system stack. Moreover, most log\ndata is unstructured and voluminous, making it more difficult for system users\nand administrators to manually inspect the data. With rapid increases in the\nscale and complexity of HPC systems, log data processing is becoming a big data\nchallenge. This paper introduces a HPC log data analytics framework that is\nbased on a distributed NoSQL database technology, which provides scalability\nand high availability, and the Apache Spark framework for rapid in-memory\nprocessing of the log data. The analytics framework enables the extraction of a\nrange of information about the system so that system administrators and end\nusers alike can obtain necessary insights for their specific needs. We describe\nour experience with using this framework to glean insights from the log data\nabout system behavior from the Titan supercomputer at the Oak Ridge National\nLaboratory.\n"]},
{"authors": ["Somayeh Moghadam", "J\u00e9r\u00f4me Darmont", "G\u00e9rald Gavin"], "title": ["S4: A New Secure Scheme for Enforcing Privacy in Cloud Data Warehouses"], "date": ["2017-08-22T12:08:37Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1708.06574v1"], "summary": ["  Outsourcing data into the cloud becomes popular thanks to the pay-as-you-go\nparadigm. However, such practice raises privacy concerns. The conventional way\nto achieve data privacy is to encrypt sensitive data before outsourcing. When\ndata are encrypted, a trade-off must be achieved between security and efficient\nquery processing. Existing solutions that adopt multiple encryption schemes\ninduce a heavy overhead in terms of data storage and query performance, and are\nnot suited for cloud data warehouses. In this paper, we propose an efficient\nadditive encryption scheme (S4) based on Shamir's secret sharing for securing\ndata warehouses in the cloud. S4 addresses the shortcomings of existing\napproaches by reducing overhead while still enforcing good data privacy.\nExperimental results show the efficiency of S4 in terms of computation and\nstorage overhead with respect to existing solutions.\n"]},
{"authors": ["Stefan Fehrenbach", "James Cheney"], "title": ["Language-integrated provenance"], "date": ["2016-07-14T12:20:11Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1607.04104v4"], "summary": ["  Provenance, or information about the origin or derivation of data, is\nimportant for assessing the trustworthiness of data and identifying and\ncorrecting mistakes. Most prior implementations of data provenance have\ninvolved heavyweight modifications to database systems and little attention has\nbeen paid to how the provenance data can be used outside such a system. We\npresent extensions to the Links programming language that build on its support\nfor language-integrated query to support provenance queries by rewriting and\nnormalizing monadic comprehensions and extending the type system to distinguish\nprovenance metadata from normal data. The main contribution of this article is\nto show that the two most common forms of provenance can be implemented\nefficiently and used safely as a programming language feature with no changes\nto the database system.\n"]},
{"authors": ["Xiangnan Ren", "Olivier Cur\u00e9", "Hubert Naacke", "Li Ke"], "title": ["Strider-lsa: Massive RDF Stream Reasoning in the Cloud"], "date": ["2017-08-22T07:45:30Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1708.06521v1"], "summary": ["  Reasoning over semantically annotated data is an emerging trend in stream\nprocessing aiming to produce sound and complete answers to a set of continuous\nqueries. It usually comes at the cost of finding a trade-off between data\nthroughput and the cost of expressive inferences. Strider-lsa proposes such a\ntrade-off and combines a scalable RDF stream processing engine with an\nefficient reasoning system. The main reasoning tasks are based on a query\nrewriting approach for SPARQL that benefits from an intelligent encoding of\nRDFS+ (RDFS + owl:sameAs) ontology elements. Strider-lsa runs in production at\na major international water management company to detect anomalies from sensor\nstreams. The system is evaluated along different dimensions and over multiple\ndatasets to emphasize its performance.\n"]},
{"authors": ["Komal Batool", "Muaz A. Niazi"], "title": ["Tamper-Evident Complex Genomic Networks"], "date": ["2017-08-20T03:26:40Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1708.05926v1"], "summary": ["  Networks are important storage data structures now used to store personal\ninformation of individuals around the globe. With the advent of personal genome\nsequencing, networks are going to be used to store personal genomic sequencing\nof people. In contrast to social media networks, the importance of\nrelationships in this genomic network is extremely significant. Losing\nconnections between individuals thus implies losing relationship information\n(E.g. father or son etc.). There currently exists a considerably serious\nproblem in the current approach to storing network data. Simply stated, network\ndata is not tamper-evident. In other words, if some links or nodes were\nchanged/removed/added by a malicious attacker, it would be impossible for the\nadministrator to detect such changes. While, in the current age of social media\nnetworks, change in node characteristics and links can be bad in terms of\nrelationships, in the case of networks for storing personal genomes, the\nresults could be truly devastating. Here we present a scheme for building\ntamper-evident networks using a combination of Cryptographic and Ego-based\nNetwork analytic methods. Using actual published data-sets, we also demonstrate\nthe utility and validity of the scheme besides demonstrating its working in\nvarious possible scenarios of usage. Results from the extensive experiments\ndemonstrate the validity of the proposed approach.\n"]},
{"authors": ["Tien Tuan Anh Dinh", "Rui Liu", "Meihui Zhang", "Gang Chen", "Beng Chin Ooi", "Ji Wang"], "title": ["Untangling Blockchain: A Data Processing View of Blockchain Systems"], "date": ["2017-08-17T08:41:23Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1708.05665v1"], "summary": ["  Blockchain technologies are gaining massive momentum in the last few years.\nBlockchains are distributed ledgers that enable parties who do not fully trust\neach other to maintain a set of global states. The parties agree on the\nexistence, values and histories of the states. As the technology landscape is\nexpanding rapidly, it is both important and challenging to have a firm grasp of\nwhat the core technologies have to offer, especially with respect to their data\nprocessing capabilities. In this paper, we first survey the state of the art,\nfocusing on private blockchains (in which parties are authenticated). We\nanalyze both in-production and research systems in four dimensions: distributed\nledger, cryptography, consensus protocol and smart contract. We then present\nBLOCKBENCH, a benchmarking framework for understanding performance of private\nblockchains against data processing workloads. We conduct a comprehensive\nevaluation of three major blockchain systems based on BLOCKBENCH, namely\nEthereum, Parity and Hyperledger Fabric. The results demonstrate several\ntrade-offs in the design space, as well as big performance gaps between\nblockchain and database systems. Drawing from design principles of database\nsystems, we discuss several research directions for bringing blockchain\nperformance closer to the realm of databases.\n"]},
{"authors": ["Getachew B. Demisse", "Tsegaye Tadesse", "Yared Bayissa"], "title": ["Data Mining Attribute Selection Approach for Drought Modeling: A Case\n  Study for Greater Horn of Africa"], "date": ["2017-08-15T17:01:14Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1708.05072v1"], "summary": ["  The objectives of this paper were to 1) develop an empirical method for\nselecting relevant attributes for modelling drought, and 2) select the most\nrelevant attribute for drought modelling and predictions in the Greater Horn of\nAfrica (GHA). Twenty four attributes from different domain areas were used for\nthis experimental analysis. Two attribute selection algorithms were used for\nthe current study: Principal Component Analysis (PCA) and correlation-based\nattribute selection (CAS). Using the PCA and CAS algorithms, the 24 attributes\nwere ranked by their merit value. Accordingly, 15 attributes were selected for\nmodelling drought in GHA. The average merit values for the selected attributes\nranged from 0.5 to 0.9. Future research may evaluate the developed methodology\nusing relevant classification techniques and quantify the actual information\ngain from the developed approach.\n"]},
{"authors": ["Martin Kleppmann", "Alastair R. Beresford"], "title": ["A Conflict-Free Replicated JSON Datatype"], "date": ["2016-08-13T09:48:35Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1608.03960v3"], "summary": ["  Many applications model their data in a general-purpose storage format such\nas JSON. This data structure is modified by the application as a result of user\ninput. Such modifications are well understood if performed sequentially on a\nsingle copy of the data, but if the data is replicated and modified\nconcurrently on multiple devices, it is unclear what the semantics should be.\nIn this paper we present an algorithm and formal semantics for a JSON data\nstructure that automatically resolves concurrent modifications such that no\nupdates are lost, and such that all replicas converge towards the same state (a\nconflict-free replicated datatype or CRDT). It supports arbitrarily nested list\nand map types, which can be modified by insertion, deletion and assignment. The\nalgorithm performs all merging client-side and does not depend on ordering\nguarantees from the network, making it suitable for deployment on mobile\ndevices with poor network connectivity, in peer-to-peer networks, and in\nmessaging systems with end-to-end encryption.\n"]},
{"authors": ["Leopoldo Bertossi", "Mostafa Milani"], "title": ["Ontological Multidimensional Data Models and Contextual Data Qality"], "date": ["2017-04-01T03:50:53Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1704.00115v2"], "summary": ["  Data quality assessment and data cleaning are context-dependent activities.\nMotivated by this observation, we propose the Ontological Multidimensional Data\nModel (OMD model), which can be used to model and represent contexts as\nlogic-based ontologies. The data under assessment is mapped into the context,\nfor additional analysis, processing, and quality data extraction. The resulting\ncontexts allow for the representation of dimensions, and multidimensional data\nquality assessment becomes possible. At the core of a multidimensional context\nwe include a generalized multidimensional data model and a Datalog+/- ontology\nwith provably good properties in terms of query answering. These main\ncomponents are used to represent dimension hierarchies, dimensional\nconstraints, dimensional rules, and define predicates for quality data\nspecification. Query answering relies upon and triggers navigation through\ndimension hierarchies, and becomes the basic tool for the extraction of quality\ndata. The OMD model is interesting per se, beyond applications to data quality.\nIt allows for a logic-based, and computationally tractable representation of\nmultidimensional data, extending previous multidimensional data models with\nadditional expressive power and functionalities.\n"]},
{"authors": ["Cihan K\u00fc\u00e7\u00fckke\u00e7eci", "Adnan Yaz\u0131c\u0131"], "title": ["Big Data Model Simulation on a Graph Database for Surveillance in\n  Wireless Multimedia Sensor Networks"], "date": ["2017-08-13T09:05:58Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1708.03878v1"], "summary": ["  Sensors are present in various forms all around the world such as mobile\nphones, surveillance cameras, smart televisions, intelligent refrigerators and\nblood pressure monitors. Usually, most of the sensors are a part of some other\nsystem with similar sensors that compose a network. One of such networks is\ncomposed of millions of sensors connect to the Internet which is called\nInternet of things (IoT). With the advances in wireless communication\ntechnologies, multimedia sensors and their networks are expected to be major\ncomponents in IoT. Many studies have already been done on wireless multimedia\nsensor networks in diverse domains like fire detection, city surveillance,\nearly warning systems, etc. All those applications position sensor nodes and\ncollect their data for a long time period with real-time data flow, which is\nconsidered as big data. Big data may be structured or unstructured and needs to\nbe stored for further processing and analyzing. Analyzing multimedia big data\nis a challenging task requiring a high-level modeling to efficiently extract\nvaluable information/knowledge from data. In this study, we propose a big\ndatabase model based on graph database model for handling data generated by\nwireless multimedia sensor networks. We introduce a simulator to generate\nsynthetic data and store and query big data using graph model as a big\ndatabase. For this purpose, we evaluate the well-known graph-based NoSQL\ndatabases, Neo4j and OrientDB, and a relational database, MySQL.We have run a\nnumber of query experiments on our implemented simulator to show that which\ndatabase system(s) for surveillance in wireless multimedia sensor networks is\nefficient and scalable.\n"]},
{"authors": ["Pedro Almagro-Blanco", "Fernando Sancho-Caparrini"], "title": ["Generalized Graph Pattern Matching"], "date": ["2017-08-12T00:00:01Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1708.03734v1"], "summary": ["  Most of the machine learning algorithms are limited to learn from flat data:\na recordset with prefixed structure. When learning from a record, these types\nof algorithms don't take into account other objects even though they are\ndirectly connected to it and can provide valuable information for the learning\ntask. In this paper we present the concept of Generalized Graph Query, a query\ntool over graphs or multi-relational data structures. They are built using the\nsame graph structure as generalized graphs and allow to express powerful\nrelational and non-relational restrictions on this type of data. Also, this\npaper shows mechanisms to build this kind of queries dynamically and how they\ncan be used to perform bottom-up discovery processes through machine laerning\ntechniques.\n  -----\n  La mayor\\'ia de los algoritmos que aprenden a partir de datos est\\'an\nlimitados ya que s\\'olo son capaces de aprender a partir de datos estructurados\nen forma de tabla en la que cada fila representa un registro y cada columna una\npropiedad asociada. Estos algoritmos, no tienen en cuenta los atributos de las\nestructuras con las que un registro dado puede estar relacionado, a pesar de\nque \\'estos pueden aportar informaci\\'on \\'util a la hora de llevar a cabo la\ntarea de aprendizaje. En este art\\'iculo presentamos el concepto de Generalized\nGraph Query, una herramienta de consulta de patrones en grafos generalizados.\nDicha herramienta ha sido construida utilizando la estructura de Grafo\nGeneralizado y permite expresar restricciones relacionales y no relacionales\nsobre este tipo de estructuras. Adem\\'as, en este art\\'iculo se presentan\nmecanismos para la construcci\\'on autom\\'atica de este tipo de consultas y se\nmuestra c\\'omo \\'estas pueden ser utilizadas en procesos de descubrimiento tipo\nbottom-up a trav\\'es de t\\'ecnicas relacionadas con el Aprendizaje\nAutom\\'atico.\n"]},
{"authors": ["Xun Zhao", "Yanhong Wu", "Weiwei Cui", "Xinnan Du", "Yuan Chen", "Yong Wang", "Dik Lun Lee", "Huamin Qu"], "title": ["SkyLens: Visual Analysis of Skyline on Multi-dimensional Data"], "date": ["2017-08-11T08:04:58Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1708.03462v1"], "summary": ["  Skyline queries have wide-ranging applications in fields that involve\nmulti-criteria decision making, including tourism, retail industry, and human\nresources. By automatically removing incompetent candidates, skyline queries\nallow users to focus on a subset of superior data items (i.e., the skyline),\nthus reducing the decision-making overhead. However, users are still required\nto interpret and compare these superior items manually before making a\nsuccessful choice. This task is challenging because of two issues. First,\npeople usually have fuzzy, unstable, and inconsistent preferences when\npresented with multiple candidates. Second, skyline queries do not reveal the\nreasons for the superiority of certain skyline points in a multi-dimensional\nspace. To address these issues, we propose SkyLens, a visual analytic system\naiming at revealing the superiority of skyline points from different\nperspectives and at different scales to aid users in their decision making. Two\nscenarios demonstrate the usefulness of SkyLens on two datasets with a dozen of\nattributes. A qualitative study is also conducted to show that users can\nefficiently accomplish skyline understanding and comparison tasks with SkyLens.\n"]},
{"authors": ["Jason Morton"], "title": ["Contextuality from missing and versioned data"], "date": ["2017-08-10T15:37:53Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1708.03264v1"], "summary": ["  Traditionally categorical data analysis (e.g. generalized linear models)\nworks with simple, flat datasets akin to a single table in a database with no\nnotion of missing data or conflicting versions. In contrast, modern data\nanalysis must deal with distributed databases with many partial local tables\nthat need not always agree. The computational agents tabulating these tables\nare spatially separated, with binding speed-of-light constraints and data\narriving too rapidly for these distributed views ever to be fully informed and\nglobally consistent. Contextuality is a mathematical property which describes a\nkind of inconsistency arising in quantum mechanics (e.g. in Bell's theorem). In\nthis paper we show how contextuality can arise in common data collection\nscenarios, including missing data and versioning (as in low-latency distributed\ndatabases employing snapshot isolation). In the companion paper, we develop\nstatistical models adapted to this regime.\n"]},
{"authors": ["Lauren Milechin", "Vijay Gadepally", "Siddharth Samsi", "Jeremy Kepner", "Alexander Chen", "Dylan Hutchison"], "title": ["D4M 3.0: Extended Database and Language Capabilities"], "date": ["2017-08-09T17:59:42Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1708.02934v1"], "summary": ["  The D4M tool was developed to address many of today's data needs. This tool\nis used by hundreds of researchers to perform complex analytics on unstructured\ndata. Over the past few years, the D4M toolbox has evolved to support\nconnectivity with a variety of new database engines, including SciDB.\nD4M-Graphulo provides the ability to do graph analytics in the Apache Accumulo\ndatabase. Finally, an implementation using the Julia programming language is\nalso now available. In this article, we describe some of our latest additions\nto the D4M toolbox and our upcoming D4M 3.0 release. We show through\nbenchmarking and scaling results that we can achieve fast SciDB ingest using\nthe D4M-SciDB connector, that using Graphulo can enable graph algorithms on\nscales that can be memory limited, and that the Julia implementation of D4M\nachieves comparable performance or exceeds that of the existing MATLAB(R)\nimplementation.\n"]},
{"authors": ["Daniel Kang", "John Emmons", "Firas Abuzaid", "Peter Bailis", "Matei Zaharia"], "title": ["NoScope: Optimizing Neural Network Queries over Video at Scale"], "date": ["2017-03-07T18:54:28Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.02529v3"], "summary": ["  Recent advances in computer vision-in the form of deep neural networks-have\nmade it possible to query increasing volumes of video data with high accuracy.\nHowever, neural network inference is computationally expensive at scale:\napplying a state-of-the-art object detector in real time (i.e., 30+ frames per\nsecond) to a single video requires a $4000 GPU. In response, we present\nNoScope, a system for querying videos that can reduce the cost of neural\nnetwork video analysis by up to three orders of magnitude via\ninference-optimized model search. Given a target video, object to detect, and\nreference neural network, NoScope automatically searches for and trains a\nsequence, or cascade, of models that preserves the accuracy of the reference\nnetwork but is specialized to the target video and are therefore far less\ncomputationally expensive. NoScope cascades two types of models: specialized\nmodels that forego the full generality of the reference model but faithfully\nmimic its behavior for the target video and object; and difference detectors\nthat highlight temporal differences across frames. We show that the optimal\ncascade architecture differs across videos and objects, so NoScope uses an\nefficient cost-based optimizer to search across models and cascades. With this\napproach, NoScope achieves two to three order of magnitude speed-ups\n(265-15,500x real-time) on binary classification tasks over fixed-angle webcam\nand surveillance video while maintaining accuracy within 1-5% of\nstate-of-the-art neural networks.\n"]},
{"authors": ["Sudeepa Roy", "Babak Salimi"], "title": ["A Framework for Inferring Causality from Multi-Relational Observational\n  Data using Conditional Independence"], "date": ["2017-08-08T15:56:18Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1708.02536v1"], "summary": ["  The study of causality or causal inference - how much a given treatment\ncausally affects a given outcome in a population - goes way beyond correlation\nor association analysis of variables, and is critical in making sound data\ndriven decisions and policies in a multitude of applications. The gold standard\nin causal inference is performing \"controlled experiments\", which often is not\npossible due to logistical or ethical reasons. As an alternative, inferring\ncausality on \"observational data\" based on the \"Neyman-Rubin potential outcome\nmodel\" has been extensively used in statistics, economics, and social sciences\nover several decades. In this paper, we present a formal framework for sound\ncausal analysis on observational datasets that are given as multiple relations\nand where the population under study is obtained by joining these base\nrelations. We study a crucial condition for inferring causality from\nobservational data, called the \"strong ignorability assumption\" (the treatment\nand outcome variables should be independent in the joined relation given the\nobserved covariates), using known conditional independences that hold in the\nbase relations. We also discuss how the structure of the conditional\nindependences in base relations given as graphical models help infer new\nconditional independences in the joined relation. The proposed framework\ncombines concepts from databases, statistics, and graphical models, and aims to\ninitiate new research directions spanning these fields to facilitate powerful\ndata-driven decisions in today's big data world.\n"]},
{"authors": ["Arun Kejariwal", "Sanjeev Kulkarni", "Karthik Ramasamy"], "title": ["Real Time Analytics: Algorithms and Systems"], "date": ["2017-08-07T15:59:34Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1708.02621v1"], "summary": ["  Velocity is one of the 4 Vs commonly used to characterize Big Data. In this\nregard, Forrester remarked the following in Q3 2014: \"The high velocity,\nwhite-water flow of data from innumerable real-time data sources such as market\ndata, Internet of Things, mobile, sensors, click-stream, and even transactions\nremain largely unnavigated by most firms. The opportunity to leverage streaming\nanalytics has never been greater.\" Example use cases of streaming analytics\ninclude, but not limited to: (a) visualization of business metrics in real-time\n(b) facilitating highly personalized experiences (c) expediting response during\nemergencies. Streaming analytics is extensively used in a wide variety of\ndomains such as healthcare, e-commerce, financial services, telecommunications,\nenergy and utilities, manufacturing, government and transportation.\n  In this tutorial, we shall present an in-depth overview of streaming\nanalytics - applications, algorithms and platforms - landscape. We shall walk\nthrough how the field has evolved over the last decade and then discuss the\ncurrent challenges - the impact of the other three Vs, viz., Volume, Variety\nand Veracity, on Big Data streaming analytics. The tutorial is intended for\nboth researchers and practitioners in the industry. We shall also present\nstate-of-the-affairs of streaming analytics at Twitter.\n"]},
{"authors": ["Caihua Shan", "Nikos Mamoulis", "Guoliang Li", "Reynold Cheng", "Zhipeng Huang", "Yudian Zheng"], "title": ["T-Crowd: Effective Crowdsourcing for Tabular Data"], "date": ["2017-08-07T14:03:17Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1708.02125v1"], "summary": ["  Crowdsourcing employs human workers to solve computer-hard problems, such as\ndata cleaning, entity resolution, and sentiment analysis. When crowdsourcing\ntabular data, e.g., the attribute values of an entity set, a worker's answers\non the different attributes (e.g., the nationality and age of a celebrity star)\nare often treated independently. This assumption is not always true and can\nlead to suboptimal crowdsourcing performance. In this paper, we present the\nT-Crowd system, which takes into consideration the intricate relationships\namong tasks, in order to converge faster to their true values. Particularly,\nT-Crowd integrates each worker's answers on different attributes to effectively\nlearn his/her trustworthiness and the true data values. The attribute\nrelationship information is also used to guide task allocation to workers.\nFinally, T-Crowd seamlessly supports categorical and continuous attributes,\nwhich are the two main datatypes found in typical databases. Our extensive\nexperiments on real and synthetic datasets show that T-Crowd outperforms\nstate-of-the-art methods in terms of truth inference and reducing the cost of\ncrowdsourcing.\n"]},
{"authors": ["Xiu Susie Fang", "Quan Z. Sheng", "Xianzhi Wang", "Wei Emma Zhang", "Anne H. H. Ngu"], "title": ["From Appearance to Essence: Comparing Truth Discovery Methods without\n  Using Ground Truth"], "date": ["2017-08-07T08:12:22Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1708.02029v1"], "summary": ["  Truth discovery has been widely studied in recent years as a fundamental\nmeans for resolving the conflicts in multi-source data. Although many truth\ndiscovery methods have been proposed based on different considerations and\nintuitions, investigations show that no single method consistently outperforms\nthe others. To select the right truth discovery method for a specific\napplication scenario, it becomes essential to evaluate and compare the\nperformance of different methods. A drawback of current research efforts is\nthat they commonly assume the availability of certain ground truth for the\nevaluation of methods. However, the ground truth may be very limited or even\nout-of-reach in practice, rendering the evaluation biased by the small ground\ntruth or even unfeasible. In this paper, we present CompTruthHyp, a general\napproach for comparing the performance of truth discovery methods without using\nground truth. In particular, our approach calculates the probability of\nobservations in a dataset based on the output of different methods. The\nprobability is then ranked to reflect the performance of these methods. We\nreview and compare twelve existing truth discovery methods and consider both\nsingle-valued and multi-valued objects. Empirical studies on both real-world\nand synthetic datasets demonstrate the effectiveness of our approach for\ncomparing truth discovery methods.\n"]},
{"authors": ["Xiu Susie Fang", "Quan Z. Sheng", "Xianzhi Wang", "Anne H. H. Ngu"], "title": ["SmartMTD: A Graph-Based Approach for Effective Multi-Truth Discovery"], "date": ["2017-08-07T07:36:33Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1708.02018v1"], "summary": ["  The Big Data era features a huge amount of data that are contributed by\nnumerous sources and used by many critical data-driven applications. Due to the\nvarying reliability of sources, it is common to see conflicts among the\nmulti-source data, making it difficult to determine which data sources to\ntrust. Recently, truth discovery has emerged as a means of addressing this\nchallenging issue by determining data veracity jointly with estimating the\nreliability of data sources. A fundamental issue with current truth discovery\nmethods is that they generally assume only one true value for each object,\nwhile in reality, objects may have multiple true values. In this paper, we\npropose a graph-based approach, called SmartMTD, to unravel the truth discovery\nproblem beyond the single-truth assumption, or the multi-truth discovery\nproblem. SmartMTD models and quantifies two types of source relations to\nestimate source reliability precisely and to detect malicious agreement among\nsources for effective multi-truth discovery. In particular, two graphs are\nconstructed based on the modeled source relations. They are further used to\nderive the two aspects of source reliability (i.e., positive precision and\nnegative precision) via random walk computation. Empirical studies on two large\nreal-world datasets demonstrate the effectiveness of our approach.\n"]},
{"authors": ["Albert Kim", "Liqi Xu", "Tarique Siddiqui", "Silu Huang", "Samuel Madden", "Aditya Parameswaran"], "title": ["Optimally Leveraging Density and Locality to Support LIMIT Queries"], "date": ["2016-11-15T05:12:10Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1611.04705v3"], "summary": ["  Existing database systems are not optimized for queries with a LIMIT\nclause---operating instead in an all-or-nothing manner. In this paper, we\npropose a fast LIMIT query evaluation engine, called NeedleTail, aimed at\nletting analysts browse a small sample of the query results on large datasets\nas quickly as possible, independent of the overall size of the result set.\nNeedleTail introduces density maps, a lightweight in-memory indexing structure,\nand a set of efficient algorithms (with desirable theoretical guarantees) to\nquickly locate promising blocks, trading off locality and density. In settings\nwhere the samples are used to compute aggregates, we extend techniques from\nsurvey sampling to mitigate the bias in our samples. Our experimental results\ndemonstrate that NeedleTail returns results 4x faster on HDDs and 9x faster on\nSSDs on average, while occupying up to 23x less memory than existing\ntechniques.\n"]},
{"authors": ["Rafael S. Gon\u00e7alves", "Martin J. O'Connor", "Marcos Mart\u00ednez-Romero", "John Graybeal", "Mark A. Musen"], "title": ["Metadata in the BioSample Online Repository are Impaired by Numerous\n  Anomalies"], "date": ["2017-08-03T19:27:06Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1708.01286v1"], "summary": ["  The metadata about scientific experiments are crucial for finding,\nreproducing, and reusing the data that the metadata describe. We present a\nstudy of the quality of the metadata stored in BioSample--a repository of\nmetadata about samples used in biomedical experiments managed by the U.S.\nNational Center for Biomedical Technology Information (NCBI). We tested whether\n6.6 million BioSample metadata records are populated with values that fulfill\nthe stated requirements for such values. Our study revealed multiple anomalies\nin the analyzed metadata. The BioSample metadata field names and their values\nare not standardized or controlled--15% of the metadata fields use field names\nnot specified in the BioSample data dictionary. Only 9 out of 452\nBioSample-specified fields ordinarily require ontology terms as values, and the\nquality of these controlled fields is better than that of uncontrolled ones, as\neven simple binary or numeric fields are often populated with inadequate values\nof different data types (e.g., only 27% of Boolean values are valid). Overall,\nthe metadata in BioSample reveal that there is a lack of principled mechanisms\nto enforce and validate metadata requirements. The aberrancies in the metadata\nare likely to impede search and secondary use of the associated datasets.\n"]},
{"authors": ["Angela Bonifati", "Wim Martens", "Thomas Timm"], "title": ["An Analytical Study of Large SPARQL Query Logs"], "date": ["2017-08-01T14:36:55Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1708.00363v1"], "summary": ["  With the adoption of RDF as the data model for Linked Data and the Semantic\nWeb, query specification from end- users has become more and more common in\nSPARQL end- points. In this paper, we conduct an in-depth analytical study of\nthe queries formulated by end-users and harvested from large and up-to-date\nquery logs from a wide variety of RDF data sources. As opposed to previous\nstudies, ours is the first assessment on a voluminous query corpus, span- ning\nover several years and covering many representative SPARQL endpoints. Apart\nfrom the syntactical structure of the queries, that exhibits already\ninteresting results on this generalized corpus, we drill deeper in the\nstructural char- acteristics related to the graph- and hypergraph represen-\ntation of queries. We outline the most common shapes of queries when visually\ndisplayed as pseudographs, and char- acterize their (hyper-)tree width.\nMoreover, we analyze the evolution of queries over time, by introducing the\nnovel con- cept of a streak, i.e., a sequence of queries that appear as\nsubsequent modifications of a seed query. Our study offers several fresh\ninsights on the already rich query features of real SPARQL queries formulated\nby real users, and brings us to draw a number of conclusions and pinpoint\nfuture di- rections for SPARQL query evaluation, query optimization, tuning,\nand benchmarking.\n"]},
{"authors": ["Jorge Lloret-Gazo"], "title": ["A Survey on Visual Query Systems in the Web Era (extended version)"], "date": ["2017-08-01T07:37:16Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1708.00192v1"], "summary": ["  As more and more collections of data are becoming available on the web to\neveryone, non expert users demand easy ways to retrieve data from these\ncollections. One solution is the so called Visual Query Systems (VQS) where\nqueries are represented visually and users do not have to understand query\nlanguages such as SQL or XQuery. In 1996, a paper by Catarci reviewed the\nVisual Query Systems available until that year. In this paper, we review VQSs\nfrom 1997 until now and try to determine whether they have been the solution\nfor non expert users. The short answer is no because very few systems have in\nfact been used in real environments or as commercial tools. We have also\ngathered basic features of VQSs such as the visual representation adopted to\npresent the reality of interest or the visual representation adopted to express\nqueries.\n"]},
{"authors": ["Leopoldo Bertossi", "Babak Salimi"], "title": ["Causes for Query Answers from Databases: Datalog Abduction,\n  View-Updates, and Integrity Constraints"], "date": ["2016-11-06T00:35:09Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1611.01711v3"], "summary": ["  Causality has been recently introduced in databases, to model, characterize,\nand possibly compute causes for query answers. Connections between QA-causality\nand consistency-based diagnosis and database repairs (wrt. integrity constraint\nviolations) have already been established. In this work we establish precise\nconnections between QA-causality and both abductive diagnosis and the\nview-update problem in databases, allowing us to obtain new algorithmic and\ncomplexity results for QA-causality. We also obtain new results on the\ncomplexity of view-conditioned causality, and investigate the notion of\nQA-causality in the presence of integrity constraints, obtaining complexity\nresults from a connection with view-conditioned causality. The abduction\nconnection under integrity constraints allows us to obtain algorithmic tools\nfor QA-causality.\n"]},
{"authors": ["Xing Niu", "Bahareh Sadat Arab", "Seokki Lee", "Su Feng", "Xun Zou", "Dieter Gawlick", "Vasudha Krishnaswamy", "Zhen Hua Liu", "Boris Glavic"], "title": ["Debugging Transactions and Tracking their Provenance with Reenactment"], "date": ["2017-07-31T16:00:05Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1707.09930v1"], "summary": ["  Debugging transactions and understanding their execution are of immense\nimportance for developing OLAP applications, to trace causes of errors in\nproduction systems, and to audit the operations of a database. However,\ndebugging transactions is hard for several reasons: 1) after the execution of a\ntransaction, its input is no longer available for debugging, 2) internal states\nof a transaction are typically not accessible, and 3) the execution of a\ntransaction may be affected by concurrently running transactions. We present a\ndebugger for transactions that enables non-invasive, post-mortem debugging of\ntransactions with provenance tracking and supports what-if scenarios (changes\nto transaction code or data). Using reenactment, a declarative replay technique\nwe have developed, a transaction is replayed over the state of the DB seen by\nits original execution including all its interactions with concurrently\nexecuted transactions from the history. Importantly, our approach uses the\ntemporal database and audit logging capabilities available in many DBMS and\ndoes not require any modifications to the underlying database system nor\ntransactional workload.\n"]},
{"authors": ["Gangli Liu"], "title": ["Understanding tree: a tool for estimating an individual's understanding\n  of conceptual knowledge"], "date": ["2017-07-31T10:30:37Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1708.00335v1"], "summary": ["  People learn whenever and wherever possible, and whatever they like or\nencounter--Mathematics, Drama, Art, Languages, Physics, Philosophy, and so on.\nWith the bursting of knowledge, evaluation of one's understanding of conceptual\nknowledge becomes increasingly difficult. There are a lot of demands for\nevaluating one's understanding of a piece of knowledge, e.g., facilitating\npersonalized recommendations; discovering one's expertises and deficiencies in\na field; recommending topics for a conversation between people with different\neducational or cultural backgrounds in their first encounter; recommending a\nlearning material to practice a meaningful learning etc. Assessment of\nunderstanding of knowledge is conventionally practiced through tests or\ninterviews, but they have some limitations such as low-efficiency and\nnot-comprehensive. We propose a method to estimate one's understanding of\nconceptual knowledge, by keeping track of his/her learning activities. It\novercomes some limitations of traditional methods, hence complements\ntraditional methods.\n"]},
{"authors": ["Bikash Chandra", "S. Sudarshan"], "title": ["Runtime Optimization of Join Location in Parallel Data Management\n  Systems"], "date": ["2017-03-03T13:21:25Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.01148v3"], "summary": ["  Applications running on parallel systems often need to join a streaming\nrelation or a stored relation with data indexed in a parallel data storage\nsystem. Some applications also compute UDFs on the joined tuples. The join can\nbe done at the data storage nodes, corresponding to reduce side joins, or by\nfetching data from the storage system to compute nodes, corresponding to map\nside join. Both may be suboptimal: reduce side joins may cause skew, while map\nside joins may lead to a lot of data being transferred and replicated.\n  In this paper, we present techniques to make runtime decisions between the\ntwo options on a per key basis, in order to improve the throughput of the join,\naccounting for UDF computation if any. Our techniques are based on an extended\nski-rental algorithm and provide worst-case performance guarantees with respect\nto the optimal point in the space considered by us. Our techniques use load\nbalancing taking into account the CPU, network and I/O costs as well as the\nload on compute and storage nodes. We have implemented our techniques on\nHadoop, Spark and the Muppet stream processing engine. Our experiments show\nthat our optimization techniques provide a significant improvement in\nthroughput over existing techniques.\n"]},
{"authors": ["Diego Garc\u00eda-Gil", "Juli\u00e1n Luengo", "Salvador Garc\u00eda", "Francisco Herrera"], "title": ["Enabling Smart Data: Noise filtering in Big Data classification"], "date": ["2017-04-06T10:06:52Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1704.01770v2"], "summary": ["  In any knowledge discovery process the value of extracted knowledge is\ndirectly related to the quality of the data used. Big Data problems, generated\nby massive growth in the scale of data observed in recent years, also follow\nthe same dictate. A common problem affecting data quality is the presence of\nnoise, particularly in classification problems, where label noise refers to the\nincorrect labeling of training instances, and is known to be a very disruptive\nfeature of data. However, in this Big Data era, the massive growth in the scale\nof the data poses a challenge to traditional proposals created to tackle noise,\nas they have difficulties coping with such a large amount of data. New\nalgorithms need to be proposed to treat the noise in Big Data problems,\nproviding high quality and clean data, also known as Smart Data. In this paper,\ntwo Big Data preprocessing approaches to remove noisy examples are proposed: an\nhomogeneous ensemble and an heterogeneous ensemble filter, with special\nemphasis in their scalability and performance traits. The obtained results show\nthat these proposals enable the practitioner to efficiently obtain a Smart\nDataset from any Big Data classification problem.\n"]},
{"authors": ["M. Sadegh Riazi", "Beidi Chen", "Anshumali Shrivastava", "Dan Wallach", "Farinaz Koushanfar"], "title": ["Sub-Linear Privacy-Preserving Near-Neighbor Search with Untrusted Server\n  on Large-Scale Datasets"], "date": ["2016-12-06T14:53:06Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1612.01835v3"], "summary": ["  In Near-Neighbor Search (NNS), a new client queries a database (held by a\nserver) for the most similar data (near-neighbors) given a certain similarity\nmetric. The Privacy-Preserving variant (PP-NNS) requires that neither server\nnor the client shall learn information about the other party's data except what\ncan be inferred from the outcome of NNS. The overwhelming growth in the size of\ncurrent datasets and the lack of a truly secure server in the online world\nrender the existing solutions impractical; either due to their high\ncomputational requirements or non-realistic assumptions which potentially\ncompromise privacy. PP-NNS having query time {\\it sub-linear} in the size of\nthe database has been suggested as an open research direction by Li et al.\n(CCSW'15). In this paper, we provide the first such algorithm, called Secure\nLocality Sensitive Indexing (SLSI) which has a sub-linear query time and the\nability to handle honest-but-curious parties. At the heart of our proposal lies\na secure binary embedding scheme generated from a novel probabilistic\ntransformation over locality sensitive hashing family. We provide information\ntheoretic bound for the privacy guarantees and support our theoretical claims\nusing substantial empirical evidence on real-world datasets.\n"]},
{"authors": ["Willem Conradie", "Sabine Frittella", "Alessandra Palmigiano", "Michele Piazzai", "Apostolos Tzimoulis", "Nachoem M. Wijnberg"], "title": ["Toward an Epistemic-Logical Theory of Categorization"], "date": ["2017-07-27T07:48:07Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1707.08743v1"], "summary": ["  Categorization systems are widely studied in psychology, sociology, and\norganization theory as information-structuring devices which are critical to\ndecision-making processes. In the present paper, we introduce a sound and\ncomplete epistemic logic of categories and agents' categorical perception. The\nKripke-style semantics of this logic is given in terms of data structures based\non two domains: one domain representing objects (e.g. market products) and one\ndomain representing the features of the objects which are relevant to the\nagents' decision-making. We use this framework to discuss and propose\nlogic-based formalizations of some core concepts from psychological,\nsociological, and organizational research in categorization theory.\n"]},
{"authors": ["Clark Alexander", "Sofya Akhmametyeva"], "title": ["The Shape Metric for Clustering Algorithms"], "date": ["2017-07-26T16:21:32Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1707.08524v1"], "summary": ["  We construct a method by which we can calculate the precision with which an\nalgorithm identifies the shape of a cluster. We present our results for several\nwell known clustering algorithms and suggest ways to improve performance for\nnewer algorithms.\n"]},
{"authors": ["Joachim Biskup", "Cornelia Tadros", "Jaouad Zarouali"], "title": ["Confidentiality enforcement by hybrid control of information flows"], "date": ["2017-07-26T15:02:45Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1707.08482v1"], "summary": ["  An information owner, possessing diverse data sources, might want to offer\ninformation services based on these sources to cooperation partners and to this\nend interact with these partners by receiving and sending messages, which the\nowner on his part generates by program execution. Independently from data\nrepresentation or its physical storage, information release to a partner might\nbe restricted by the owner's confidentiality policy on an integrated, unified\nview of the sources. Such a policy should even be enforced if the partner as an\nintelligent and only semi-honest attacker attempts to infer hidden information\nfrom message data, also employing background knowledge. For this problem of\ninference control, we present a framework for a unified, holistic control of\ninformation flow induced by program-based processing of the data sources to\nmessages sent to a cooperation partner. Our framework expands on and combines\nestablished concepts for confidentiality enforcement and its verification and\nis instantiated in a Java environment. More specifically, as a hybrid control\nwe combine gradual release of information via declassification, enforced by\nstatic program analysis using a security type system, with a dynamic monitoring\napproach. The dynamic monitoring employs flow tracking for generalizing values\nto be declassified under confidentiality policy compliance.\n"]},
{"authors": ["Apurba Das", "Srikanta Tirthapura"], "title": ["A Change-Sensitive Algorithm for Maintaining Maximal Bicliques in a\n  Dynamic Bipartite Graph"], "date": ["2017-07-26T02:01:22Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1707.08272v1"], "summary": ["  We consider the maintenance of maximal bicliques from a dynamic bipartite\ngraph that changes over time due to the addition or deletion of edges. When the\nset of edges in a graph changes, we are interested in knowing the change in the\nset of maximal bicliques (the \"change\"), rather than in knowing the set of\nmaximal bicliques that remain unaffected. The challenge in an efficient\nalgorithm is to enumerate the change without explicitly enumerating the set of\nall maximal bicliques. In this work, we present (1) near-tight bounds on the\nmagnitude of change in the set of maximal bicliques of a graph, due to a change\nin the edge set (2) a \"change-sensitive\" algorithm for enumerating the change\nin the set of maximal bicliques, whose time complexity is proportional to the\nmagnitude of change that actually occurred in the set of maximal bicliques in\nthe graph. To our knowledge, these are the first algorithms for enumerating\nmaximal bicliques in a dynamic graph, with such provable performance\nguarantees. Our algorithms are easy to implement, and experimental results show\nthat their performance exceeds that of current baseline implementations by\norders of magnitude.\n"]},
{"authors": ["Amir Mohammad Saba", "Elham Shahab", "Hadi Abdolrahimpour", "Mahsa Hakimi", "Akbar Moazzam"], "title": ["A Comparative Analysis of XML Documents, XML Enabled Databases and\n  Native XML Databases"], "date": ["2017-07-26T00:21:23Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1707.08259v1"], "summary": ["  With the increasing popularity of XML data and a great need for a database\nmanagement system able to store, retrieve and manipulate XML-based data in an\nefficient manner, database research communities and software industries have\ntried to respond to this requirement. XML-enabled database and native XML\ndatabase are two approaches that have been proposed to address this challenge.\nThese two approaches are a legacy database systems which are extended to store,\nretrieve and manipulate XML-based data. The major objective of this paper is to\nexplore and compare between the two approaches and reach to some criteria to\nhave a suitable guideline to select the best approach in each circumstance. In\ngeneral, native XML database systems have more ability in comparison with\nXML-enabled database system for managing XML-based data\n"]},
{"authors": ["Chiara Boldrini", "Raffaele Bruno", "Haitam Laarabi"], "title": ["Car sharing through the data analysis lens"], "date": ["2017-07-25T13:07:47Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1708.00497v1"], "summary": ["  Car sharing is one the pillars of a smart transportation infrastructure, as\nit is expected to reduce traffic congestion, parking demands and pollution in\nour cities. From the point of view of demand modelling, car sharing is a weak\nsignal in the city landscape: only a small percentage of the population uses\nit, and thus it is difficult to study reliably with traditional techniques such\nas households travel diaries. In this work, we depart from these traditional\napproaches and we rely on web-based, digital records about vehicle availability\nin 10 European cities for one of the major active car sharing operators. We\ndiscuss how vehicles are used, what are the main characteristics of car sharing\ntrips, whether events happening in certain areas are predictable or not, and\nhow the spatio-temporal information about vehicle availability can be used to\ninfer how different zones in a city are used by customers. We conclude the\npaper by presenting a direct application of the analysis of the dataset, aimed\nat identifying where to locate maintenance facilities within the car sharing\noperational area.\n"]},
{"authors": ["Parisa Kordjamshidi", "Sameer Singh", "Daniel Khashabi", "Christos Christodoulopoulos", "Mark Summons", "Saurabh Sinha", "Dan Roth"], "title": ["Relational Learning and Feature Extraction by Querying over\n  Heterogeneous Information Networks"], "date": ["2017-07-25T02:32:33Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1707.07794v1"], "summary": ["  Many real world systems need to operate on heterogeneous information networks\nthat consist of numerous interacting components of different types. Examples\ninclude systems that perform data analysis on biological information networks;\nsocial networks; and information extraction systems processing unstructured\ndata to convert raw text to knowledge graphs. Many previous works describe\nspecialized approaches to perform specific types of analysis, mining and\nlearning on such networks. In this work, we propose a unified framework\nconsisting of a data model -a graph with a first order schema along with a\ndeclarative language for constructing, querying and manipulating such networks\nin ways that facilitate relational and structured machine learning. In\nparticular, we provide an initial prototype for a relational and graph\ntraversal query language where queries are directly used as relational features\nfor structured machine learning models. Feature extraction is performed by\nmaking declarative graph traversal queries. Learning and inference models can\ndirectly operate on this relational representation and augment it with new data\nand knowledge that, in turn, is integrated seamlessly into the relational\nstructure to support new predictions. We demonstrate this system's capabilities\nby showcasing tasks in natural language processing and computational biology\ndomains.\n"]},
{"authors": ["Oren Mishali", "Tal Yahav", "Oren Kalinsky", "Benny Kimelfeld"], "title": ["eLinda: Explorer for Linked Data"], "date": ["2017-07-24T16:03:06Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1707.07623v1"], "summary": ["  To realize the premise of the Semantic Web towards knowledgeable machines,\none might often integrate an application with emerging RDF graphs.\nNevertheless, capturing the content of a rich and open RDF graph by existing\ntools requires both time and expertise. We demonstrate eLinda - an explorer for\nLinked Data. The challenge addressed by eLinda is that of understanding the\nrich content of a given RDF graph. The core functionality is an exploration\npath, where each step produces a bar chart (histogram) that visualizes the\ndistribution of classes in a set of nodes (URIs). In turn, each bar represents\na set of nodes that can be further expanded through the bar chart in the path.\nWe allow three types of explorations: subclass distribution, property\ndistribution, and object distribution for a property of choice.\n"]},
{"authors": ["Meng Wang", "Jiaheng Zhang", "Jun Liu", "Wei Hu", "Sen Wang", "Xue Li", "Wenqiang Liu"], "title": ["PDD Graph: Bridging Electronic Medical Records and Biomedical Knowledge\n  Graphs via Entity Linking"], "date": ["2017-07-17T18:04:27Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1707.05340v2"], "summary": ["  Electronic medical records contain multi-format electronic medical data that\nconsist of an abundance of medical knowledge. Facing with patient's symptoms,\nexperienced caregivers make right medical decisions based on their professional\nknowledge that accurately grasps relationships between symptoms, diagnosis and\ncorresponding treatments. In this paper, we aim to capture these relationships\nby constructing a large and high-quality heterogenous graph linking patients,\ndiseases, and drugs (PDD) in EMRs. Specifically, we propose a novel framework\nto extract important medical entities from MIMIC-III (Medical Information Mart\nfor Intensive Care III) and automatically link them with the existing\nbiomedical knowledge graphs, including ICD-9 ontology and DrugBank. The PDD\ngraph presented in this paper is accessible on the Web via the SPARQL endpoint,\nand provides a pathway for medical discovery and applications, such as\neffective treatment recommendations.\n"]},
{"authors": ["Carlo Zaniolo", "Mohan Yang", "Matteo Interlandi", "Ariyam Das", "Alexander Shkapsky", "Tyson Condie"], "title": ["Fixpoint Semantics and Optimization of Recursive Datalog Programs with\n  Aggregates"], "date": ["2017-07-18T15:25:35Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1707.05681v2"], "summary": ["  A very desirable Datalog extension investigated by many researchers in the\nlast thirty years consists in allowing the use of the basic SQL aggregates min,\nmax, count and sum in recursive rules. In this paper, we propose a simple\ncomprehensive solution that extends the declarative least-fixpoint semantics of\nHorn Clauses, along with the optimization techniques used in the bottom-up\nimplementation approach adopted by many Datalog systems. We start by\nidentifying a large class of programs of great practical interest in which the\nuse of min or max in recursive rules does not compromise the declarative\nfixpoint semantics of the programs using those rules. Then, we revisit the\nmonotonic versions of count and sum aggregates proposed in (Mazuran et al.\n2013b) and named, respectively, mcount and msum. Since mcount, and also msum on\npositive numbers, are monotonic in the lattice of set-containment, they\npreserve the fixpoint semantics of Horn Clauses. However, in many applications\nof practical interest, their use can lead to inefficiencies, that can be\neliminated by combining them with max, whereby mcount and msum become the\nstandard count and sum. Therefore, the semantics and optimization techniques of\nDatalog are extended to recursive programs with min, max, count and sum, making\npossible the advanced applications of superior performance and scalability\ndemonstrated by BigDatalog (Shkapsky et al. 2016) and Datalog-MC (Yang et al.\n2017). This paper is under consideration for acceptance in TPLP.\n"]},
{"authors": ["Vivek Shah", "Marcos Antonio Vaz Salles"], "title": ["Actor Database Systems: A Manifesto"], "date": ["2017-07-20T13:52:21Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1707.06507v1"], "summary": ["  Interactive data-intensive applications are becoming ever more pervasive in\ndomains such as finance, web applications, mobile computing, and Internet of\nThings. Typically, these applications are architected to utilize a data tier\nfor persistence. At one extreme, the data tier is a simple key-value storage\nservice, and the application code is concentrated in the middle tier. While\nthis design provides for programmability at the middle tier, it forces\napplications to forego classic data management functionality, such as\ndeclarative querying and transactions. At the other extreme, the application\ncode can be colocated in the data tier itself using stored procedures in a\ndatabase system. While providing rich data management functionality natively,\nthe resulting lack of modularity and state encapsulation creates software\nengineering challenges, such as difficulty in isolation of bugs and failures or\ncomplexity in managing source code dependencies. In addition, this monolithic\narchitectural style makes it harder to scale the application with growing\nrequest volumes and data sizes. In this paper, we advocate a new database\nsystem paradigm bringing to developers the benefits of these two extremes,\nwhile avoiding their pitfalls. To provide modularity and reasoning on\nscalability, we argue that data tiers should leverage the actor abstraction; at\nthe same time, these actor-based data tiers should offer database system\nfeatures to reduce bugs and programming effort involved in state manipulation.\nTowards this aim, we present a vision for actor database systems. We analyze\ncurrent trends justifying the emergence of this abstraction and discuss a set\nof features for these new systems. To illustrate the usefulness of the proposed\nfeature set, we present a detailed case study inspired by a smart supermarket\napplication with self-checkout.\n"]},
{"authors": ["Vivek Shah", "Marcos Vaz Salles"], "title": ["Reactors: A Case for Predictable, Virtualized Actor Database Systems"], "date": ["2017-01-19T12:51:46Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.05397v3"], "summary": ["  The requirements for OLTP database systems are becoming ever more demanding.\nApplications in domains such as finance and computer games increasingly mandate\nthat developers be able to reason about and control transaction latencies in\nin-memory databases. At the same time, infrastructure engineers in these\ndomains need to experiment with and deploy OLTP database architectures that\nensure application scalability and maximize resource utilization in modern\nmachines. In this paper, we propose a relational actor programming model for\nin-memory databases as a novel, holistic approach towards fulfilling these\nchallenging requirements. Conceptually, relational actors, or reactors for\nshort, are application-defined, isolated logical actors that encapsulate\nrelations and process function calls asynchronously. Reactors ease reasoning\nabout correctness by guaranteeing serializability of application-level function\ncalls. In contrast to classic transactional models, however, reactors allow\ndevelopers to take advantage of intra-transaction parallelism and state\nencapsulation in their applications to reduce latency and improve locality.\nMoreover, reactors enable a new degree of flexibility in database deployment.\nWe present ReactDB, a system design exposing reactors that allows for flexible\nvirtualization of database architecture between the extremes of shared-nothing\nand shared-everything without changes to application code. Our experiments with\nReactDB illustrate latency control, multi-core scalability, and low overhead in\nOLTP benchmarks.\n"]},
{"authors": ["Antonis Troumpoukis", "Stasinos Konstantopoulos", "Angelos Charalambidis"], "title": ["An extension of SPARQL for expressing qualitative preferences"], "date": ["2017-07-20T08:00:32Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1707.06406v1"], "summary": ["  In this paper we present SPREFQL, an extension of the SPARQL language that\nallows appending a PREFER clause that expresses \"soft\" preferences over the\nquery results obtained by the main body of the query. The extension does not\nadd expressivity and any SPREFQL query can be transformed to an equivalent\nstandard SPARQL query. However, clearly separating preferences from the \"hard\"\npatterns and filters in the WHERE clause gives queries where the intention of\nthe client is more cleanly expressed, an advantage for both human readability\nand machine optimization. In the paper we formally define the syntax and the\nsemantics of the extension and we also provide empirical evidence that\noptimizations specific to SPREFQL improve run-time efficiency by comparison to\nthe usually applied optimizations on the equivalent standard SPARQL query.\n"]},
{"authors": ["Martin Grohe", "Nicole Schweikardt"], "title": ["First-Order Query Evaluation with Cardinality Conditions"], "date": ["2017-07-19T06:12:34Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1707.05945v1"], "summary": ["  We study an extension of first-order logic that allows to express cardinality\nconditions in a similar way as SQL's COUNT operator. The corresponding logic\nFOC(P) was introduced by Kuske and Schweikardt (LICS'17), who showed that query\nevaluation for this logic is fixed-parameter tractable on classes of structures\n(or databases) of bounded degree. In the present paper, we first show that the\nfixed-parameter tractability of FOC(P) cannot even be generalised to very\nsimple classes of structures of unbounded degree such as unranked trees or\nstrings with a linear order relation.\n  Then we identify a fragment FOC1(P) of FOC(P) which is still sufficiently\nstrong to express standard applications of SQL's COUNT operator. Our main\nresult shows that query evaluation for FOC1(P) is fixed-parameter tractable\nwith almost linear running time on nowhere dense classes of structures. As a\ncorollary, we also obtain a fixed-parameter tractable algorithm for counting\nthe number of tuples satisfying a query over nowhere dense classes of\nstructures.\n"]},
{"authors": ["Lina Yao", "Quan Z. Sheng", "Anne H. H. Ngu", "Xue Li", "Boualem Benatallah"], "title": ["Unveiling Contextual Similarity of Things via Mining Human-Thing\n  Interactions in the Internet of Things"], "date": ["2015-12-24T13:47:27Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1512.08493v3"], "summary": ["  With recent advances in radio-frequency identification (RFID), wireless\nsensor networks, and Web services, physical things are becoming an integral\npart of the emerging ubiquitous Web. Finding correlations of ubiquitous things\nis a crucial prerequisite for many important applications such as things\nsearch, discovery, classification, recommendation, and composition. This\narticle presents DisCor-T, a novel graph-based method for discovering\nunderlying connections of things via mining the rich content embodied in\nhuman-thing interactions in terms of user, temporal and spatial information. We\nmodel these various information using two graphs, namely spatio-temporal graph\nand social graph. Then, random walk with restart (RWR) is applied to find\nproximities among things, and a relational graph of things (RGT) indicating\nimplicit correlations of things is learned. The correlation analysis lays a\nsolid foundation contributing to improved effectiveness in things management.\nTo demonstrate the utility, we develop a flexible feature-based classification\nframework on top of RGT and perform a systematic case study. Our evaluation\nexhibits the strength and feasibility of the proposed approach.\n"]},
{"authors": ["Albert Atserias", "Jos\u00e9 L. Balc\u00e1zar", "Marie Ely Piceno"], "title": ["Relative Entailment Among Probabilistic Implications"], "date": ["2015-01-20T14:41:36Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1501.04826v3"], "summary": ["  We study a natural variant of the implicational fragment of propositional\nlogic. Its formulas are pairs of conjunctions of positive literals, related\ntogether by an implicational-like connective; the semantics of this sort of\nimplication is defined in terms of a threshold on a conditional probability of\nthe consequent, given the antecedent: we are dealing with what the data\nanalysis community calls confidence of partial implications or association\nrules. Existing studies of redundancy among these partial implications have\ncharacterized so far only entailment from one premise and entailment from two\npremises, both in the stand-alone case and in the case of presence of\nadditional classical implications (this is what we call \"relative entailment\").\nBy exploiting a previously noted alternative view of the entailment in terms of\nlinear programming duality, we characterize exactly the cases of entailment\nfrom arbitrary numbers of premises, again both in the stand-alone case and in\nthe case of presence of additional classical implications. As a result, we\nobtain decision algorithms of better complexity; additionally, for each\npotential case of entailment, we identify a critical confidence threshold and\nshow that it is, actually, intrinsic to each set of premises and antecedent of\nthe conclusion.\n"]},
{"authors": ["Gianluca Cima"], "title": ["Preliminary results on Ontology-based Open Data Publishing"], "date": ["2017-05-30T07:16:45Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1705.10480v2"], "summary": ["  Despite the current interest in Open Data publishing, a formal and\ncomprehensive methodology supporting an organization in deciding which data to\npublish and carrying out precise procedures for publishing high-quality data,\nis still missing. In this paper we argue that the Ontology-based Data\nManagement paradigm can provide a formal basis for a principled approach to\npublish high quality, semantically annotated Open Data. We describe two main\napproaches to using an ontology for this endeavor, and then we present some\ntechnical results on one of the approaches, called bottom-up, where the\nspecification of the data to be published is given in terms of the sources, and\nspecific techniques allow deriving suitable annotations for interpreting the\npublished data under the light of the ontology.\n"]},
{"authors": ["\u00c7a\u011fatay Demiralp", "Peter J. Haas", "Srinivasan Parthasarathy", "Tejaswini Pedapati"], "title": ["Foresight: Recommending Visual Insights"], "date": ["2017-07-12T19:18:12Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1707.03877v1"], "summary": ["  Current tools for exploratory data analysis (EDA) require users to manually\nselect data attributes, statistical computations and visual encodings. This can\nbe daunting for large-scale, complex data. We introduce Foresight, a system\nthat helps the user rapidly discover visual insights from large\nhigh-dimensional datasets. Formally, an \"insight\" is a strong manifestation of\na statistical property of the data, e.g., high correlation between two\nattributes, high skewness or concentration about the mean of a single\nattribute, a strong clustering of values, and so on. For each insight type,\nForesight initially presents visualizations of the top k instances in the data,\nbased on an appropriate ranking metric. The user can then look at \"nearby\"\ninsights by issuing \"insight queries\" containing constraints on insight\nstrengths and data attributes. Thus the user can directly explore the space of\ninsights, rather than the space of data dimensions and visual encodings as in\nother visual recommender systems. Foresight also provides \"global\" views of\ninsight space to help orient the user and ensure a thorough exploration\nprocess. Furthermore, Foresight facilitates interactive exploration of large\ndatasets through fast, approximate sketching.\n"]},
{"authors": ["Serkan Ayvaz", "Mehmet Aydar"], "title": ["Using RDF Summary Graph For Keyword-based Semantic Searches"], "date": ["2017-07-12T08:54:48Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1707.03602v1"], "summary": ["  The Semantic Web began to emerge as its standards and technologies developed\nrapidly in the recent years. The continuing development of Semantic Web\ntechnologies has facilitated publishing explicit semantics with data on the Web\nin RDF data model. This study proposes a semantic search framework to support\nefficient keyword-based semantic search on RDF data utilizing near neighbor\nexplorations. The framework augments the search results with the resources in\nclose proximity by utilizing the entity type semantics. Along with the search\nresults, the system generates a relevance confidence score measuring the\ninferred semantic relatedness of returned entities based on the degree of\nsimilarity. Furthermore, the evaluations assessing the effectiveness of the\nframework and the accuracy of the results are presented.\n"]},
{"authors": ["Chuancong Gao", "Jiannan Wang", "Jian Pei", "Rui Li", "Yi Chang"], "title": ["Preference-driven Similarity Join"], "date": ["2017-06-13T21:59:11Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1706.04266v3"], "summary": ["  Similarity join, which can find similar objects (e.g., products, names,\naddresses) across different sources, is powerful in dealing with variety in big\ndata, especially web data. Threshold-driven similarity join, which has been\nextensively studied in the past, assumes that a user is able to specify a\nsimilarity threshold, and then focuses on how to efficiently return the object\npairs whose similarities pass the threshold. We argue that the assumption about\na well set similarity threshold may not be valid for two reasons. The optimal\nthresholds for different similarity join tasks may vary a lot. Moreover, the\nend-to-end time spent on similarity join is likely to be dominated by a\nback-and-forth threshold-tuning process.\n  In response, we propose preference-driven similarity join. The key idea is to\nprovide several result-set preferences, rather than a range of thresholds, for\na user to choose from. Intuitively, a result-set preference can be considered\nas an objective function to capture a user's preference on a similarity join\nresult. Once a preference is chosen, we automatically compute the similarity\njoin result optimizing the preference objective. As the proof of concept, we\ndevise two useful preferences and propose a novel preference-driven similarity\njoin framework coupled with effective optimization techniques. Our approaches\nare evaluated on four real-world web datasets from a diverse range of\napplication scenarios. The experiments show that preference-driven similarity\njoin can achieve high-quality results without a tedious threshold-tuning\nprocess.\n"]},
{"authors": ["Qijun Zhu", "Haibo Hu", "Cheng Xu", "Jianliang Xu", "Wang-Chien Lee"], "title": ["Geo-Social Group Queries with Minimum Acquaintance Constraint"], "date": ["2014-06-28T07:38:50Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1406.7367v2"], "summary": ["  The prosperity of location-based social networking services enables\ngeo-social group queries for group-based activity planning and marketing. This\npaper proposes a new family of geo-social group queries with minimum\nacquaintance constraint (GSGQs), which are more appealing than existing\ngeo-social group queries in terms of producing a cohesive group that guarantees\nthe worst-case acquaintance level. GSGQs, also specified with various spatial\nconstraints, are more complex than conventional spatial queries; particularly,\nthose with a strict $k$NN spatial constraint are proved to be NP-hard. For\nefficient processing of general GSGQ queries on large location-based social\nnetworks, we devise two social-aware index structures, namely SaR-tree and\nSaR*-tree. The latter features a novel clustering technique that considers both\nspatial and social factors. Based on SaR-tree and SaR*-tree, efficient\nalgorithms are developed to process various GSGQs. Extensive experiments on\nreal-world Gowalla and Dianping datasets show that our proposed methods\nsubstantially outperform the baseline algorithms based on R-tree.\n"]},
{"authors": ["Shlomi Dolev", "Patricia Florissi", "Ehud Gudes", "Shantanu Sharma", "Ido Singer"], "title": ["A Survey on Geographically Distributed Big-Data Processing using\n  MapReduce"], "date": ["2017-07-06T17:04:46Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1707.01869v1"], "summary": ["  Hadoop and Spark are widely used distributed processing frameworks for\nlarge-scale data processing in an efficient and fault-tolerant manner on\nprivate or public clouds. These big-data processing systems are extensively\nused by many industries, e.g., Google, Facebook, and Amazon, for solving a\nlarge class of problems, e.g., search, clustering, log analysis, different\ntypes of join operations, matrix multiplication, pattern matching, and social\nnetwork analysis. However, all these popular systems have a major drawback in\nterms of locally distributed computations, which prevent them in implementing\ngeographically distributed data processing. The increasing amount of\ngeographically distributed massive data is pushing industries and academia to\nrethink the current big-data processing systems. The novel frameworks, which\nwill be beyond state-of-the-art architectures and technologies involved in the\ncurrent system, are expected to process geographically distributed data at\ntheir locations without moving entire raw datasets to a single location. In\nthis paper, we investigate and discuss challenges and requirements in designing\ngeographically distributed data processing frameworks and protocols. We\nclassify and study batch processing (MapReduce-based systems), stream\nprocessing (Spark-based systems), and SQL-style processing geo-distributed\nframeworks, models, and algorithms with their overhead issues.\n"]},
{"authors": ["Daniel Torres-Salinas", "Christian Gumpenberger", "Juan Gorraiz"], "title": ["PlumX As a Potential Tool to Assess the Macroscopic Multidimensional\n  Impact of Books"], "date": ["2017-07-05T19:02:22Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1707.03327v1"], "summary": ["  The main purpose of this macro-study is to shed light on the broad impact of\nbooks. For this purpose, the impact of a very large collection of books has\nbeen analyzed by using PlumX, an analytical tool providing a great number of\ndifferent metrics provided by various tools.\n"]},
{"authors": ["Jinfei Liu", "Li Xiong", "Qiuchen Zhang", "Jun Luo"], "title": ["Eclipse: Practicability Beyond 1NN and Skyline"], "date": ["2017-07-05T06:03:02Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1707.01223v1"], "summary": ["  In this paper, we propose a novel Eclipse query which is more practical than\nthe existing 1-NN query and skyline query. In addition, we present a few\nproperties of Eclipse query and the general idea for computing Eclipse.\n"]},
{"authors": ["Kiumars Soltani", "Anand Padmanabhan", "Shaowen Wang"], "title": ["MovePattern: Interactive Framework to Provide Scalable Visualization of\n  Movement Patterns"], "date": ["2017-07-05T03:57:32Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1707.03350v1"], "summary": ["  The rapid growth of movement data sources such as GPS traces, traffic\nnetworks and social media have provided analysts with the opportunity to\nexplore collective patterns of geographical movements in a nearly real-time\nfashion. A fast and interactive visualization framework can help analysts to\nunderstand these massive and dynamically changing datasets. However, previous\nstudies on movement visualization either ignore the unique properties of\ngeographical movement or are unable to handle today's massive data. In this\npaper, we develop MovePattern, a novel framework to 1) efficiently construct a\nconcise multi-level view of movements using a scalable and spatially-aware\nMapReduce-based approach and 2) present a fast and highly interactive webbased\nenvironment which engages vector-based visualization to include on-the-fly\ncustomization and the ability to enhance analytical functions by storing\nmetadata for both places and movements. We evaluate the framework using the\nmovements of Twitter users captured from geo-tagged tweets. The experiments\nconfirmed that our framework is able to aggregate close to 180 million\nmovements in a few minutes. In addition, we run series of stress tests on the\nfront-end of the framework to ensure that simultaneous user queries do not lead\nto long latency in the user response.\n"]},
{"authors": ["Ken-ichiro Ishikawa"], "title": ["Sequential Checking: Reallocation-Free Data-Distribution Algorithm for\n  Scale-out Storage"], "date": ["2017-07-04T10:52:39Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1707.00904v1"], "summary": ["  Using tape or optical devices for scale-out storage is one option for storing\na vast amount of data. However, it is impossible or almost impossible to\nrewrite data with such devices. Thus, scale-out storage using such devices\ncannot use standard data-distribution algorithms because they rewrite data for\nmoving between servers constituting the scale-out storage when the server\nconfiguration is changed. Although using rewritable devices for scale-out\nstorage, when server capacity is huge, rewriting data is very hard when server\nconstitution is changed. In this paper, a data-distribution algorithm called\nSequential Checking is proposed, which can be used for scale-out storage\ncomposed of devices that are hardly able to rewrite data. Sequential Checking\n1) does not need to move data between servers when the server configuration is\nchanged, 2) distribute data, the amount of which depends on the server's\nvolume, 3) select a unique server when datum is written, and 4) select servers\nwhen datum is read (there are few such server(s) in most cases) and find out a\nunique server that stores the newest datum from them. These basic\ncharacteristics were confirmed through proofs and simulations. Data can be read\nby accessing 1.98 servers on average from a storage comprising 256 servers\nunder a realistic condition. And it is confirmed by evaluations in real\nenvironment that access time is acceptable. Sequential Checking makes selecting\nscale-out storage using tape or optical devices or using huge capacity servers\nrealistic.\n"]},
{"authors": ["Juan A. Colmenares", "Reza Dorrigiv", "Daniel G. Waddington"], "title": ["Ingestion, Indexing and Retrieval of High-Velocity Multidimensional\n  Sensor Data on a Single Node"], "date": ["2017-07-04T06:30:37Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1707.00825v1"], "summary": ["  Multidimensional data are becoming more prevalent, partly due to the rise of\nthe Internet of Things (IoT), and with that the need to ingest and analyze data\nstreams at rates higher than before. Some industrial IoT applications require\ningesting millions of records per second, while processing queries on recently\ningested and historical data. Unfortunately, existing database systems suited\nto multidimensional data exhibit low per-node ingestion performance, and even\nif they can scale horizontally in distributed settings, they require large\nnumber of nodes to meet such ingest demands. For this reason, in this paper we\nevaluate a single-node multidimensional data store for high-velocity sensor\ndata. Its design centers around a two-level indexing structure, wherein the\nglobal index is an in-memory R*-tree and the local indices are serialized\nkd-trees. This study is confined to records with numerical indexing fields and\nrange queries, and covers ingest throughput, query response time, and storage\nfootprint. We show that the adopted design streamlines data ingestion and\noffers ingress rates two orders of magnitude higher than those of Percona\nServer, SQLite, and Druid. Our prototype also reports query response times\ncomparable to or better than those of Percona Server and Druid, and compares\nfavorably in terms of storage footprint. In addition, we evaluate a kd-tree\npartitioning based scheme for grouping incoming streamed data records. Compared\nto a random scheme, this scheme produces less overlap between groups of\nstreamed records, but contrary to what we expected, such reduced overlap does\nnot translate into better query performance. By contrast, the local indices\nprove much more beneficial to query performance. We believe the experience\nreported in this paper is valuable to practitioners and researchers alike\ninterested in building database systems for high-velocity multidimensional\ndata.\n"]},
{"authors": ["Vijay Gadepally", "Kyle OBrien", "Adam Dziedzic", "Aaron Elmore", "Jeremy Kepner", "Samuel Madden", "Tim Mattson", "Jennie Rogers", "Zuohao She", "Michael Stonebraker"], "title": ["Version 0.1 of the BigDAWG Polystore System"], "date": ["2017-07-03T18:24:24Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1707.00721v1"], "summary": ["  A polystore system is a database management system (DBMS) composed of\nintegrated heterogeneous database engines and multiple programming languages.\nBy matching data to the storage engine best suited to its needs, complex\nanalytics run faster and flexible storage choices helps improve data\norganization. BigDAWG (Big Data Working Group) is our reference implementation\nof a polystore system. In this paper, we describe the current BigDAWG software\nrelease which supports PostgreSQL, Accumulo and SciDB. We describe the overall\narchitecture, API and initial results of applying BigDAWG to the MIMIC II\nmedical dataset.\n"]},
{"authors": ["Piotr S. Maci\u0105g"], "title": ["Efficient Discovering of Top-K Sequential Patterns in Event-Based\n  Spatio-Temporal Data"], "date": ["2017-07-03T17:47:10Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1707.00670v1"], "summary": ["  We consider the problem of discovering sequential patterns from event-based\nspatio-temporal data. The dataset is described by a set of event types and\ntheir instances. Based on the given dataset, the task is to discover all\nsignificant sequential patterns denoting some attraction relation between event\ntypes occurring in a pattern. Already proposed algorithms discover all\nsignificant sequential patterns based on the significance threshold, which\nminimal value is given by an expert. Due to the nature of described data and\ncomplexity of discovered patterns, it may be very difficult to provide\nreasonable value of significance threshold. We consider the problem of\neffective discovering of K most important patterns in a given dataset (that is\ndiscovering of Top-K patterns).\n"]},
{"authors": ["Rathijit Sen", "Jianqiao Zhu", "Jignesh M. Patel", "Somesh Jha"], "title": ["ROSA: R Optimizations with Static Analysis"], "date": ["2017-04-10T18:08:36Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1704.02996v2"], "summary": ["  R is a popular language and programming environment for data scientists. It\nis increasingly co-packaged with both relational and Hadoop-based data\nplatforms and can often be the most dominant computational component in data\nanalytics pipelines. Recent work has highlighted inefficiencies in executing R\nprograms, both in terms of execution time and memory requirements, which in\npractice limit the size of data that can be analyzed by R. This paper presents\nROSA, a static analysis framework to improve the performance and space\nefficiency of R programs. ROSA analyzes input programs to determine program\nproperties such as reaching definitions, live variables, aliased variables, and\ntypes of variables. These inferred properties enable program transformations\nsuch as C++ code translation, strength reduction, vectorization, code motion,\nin addition to interpretive optimizations such as avoiding redundant object\ncopies and performing in-place evaluations. An empirical evaluation shows\nsubstantial reductions by ROSA in execution time and memory consumption over\nboth CRAN R and Microsoft R Open.\n"]},
{"authors": ["Mohamed Ali Zoghlami", "Olfa Arfaoui", "Minyar Sassi Hidri", "Rahma Ben Ayed"], "title": ["Classification non supervis\u00e9e des donn\u00e9es h\u00e9t\u00e9rog\u00e8nes \u00e0\n  large \u00e9chelle"], "date": ["2017-07-02T14:26:51Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1707.00297v1"], "summary": ["  When it comes to cluster massive data, response time, disk access and quality\nof formed classes becoming major issues for companies. It is in this context\nthat we have come to define a clustering framework for large scale\nheterogeneous data that contributes to the resolution of these issues. The\nproposed framework is based on, firstly, the descriptive analysis based on MCA,\nand secondly, the MapReduce paradigm in a large scale environment. The results\nare encouraging and prove the efficiency of the hybrid deployment on response\nquality and time component as on qualitative and quantitative data.\n"]},
{"authors": ["Ahmad Tasnim Siddiqui", "Mohd. Muntjir"], "title": ["A Modern Approach to Integrate Database Queries for Searching E-Commerce\n  Product"], "date": ["2017-07-01T18:46:00Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1707.01341v1"], "summary": ["  E commerce refers to the utilization of electronic data transmission for\nenhancing business processes and implementing business strategies. Explicit\ncomponents of e commerce include providing after sales services, promoting\nservices or products to services, processing payment, engaging in transaction\nprocesses, identifying customer needs, processing payment and creating services\nor products. In recent times, the use of e commerce has become too common among\nthe people. However, the growing demand of e commerce sites have made essential\nfor the databases to support direct querying of the Web page. This research\naims to explore and evaluate the integration of database queries and their uses\nin searching of electronic commerce products. It has been analyzed that e\ncommerce is one of the most outstanding trends, which have been emerged in the\ncommerce world, for the last decades. Therefore, this study was undertaken to\nexamine the benefits of integrating database queries with e commerce product\nsearches. The findings of this study suggested that database queries are\nextremely valuable for e commerce sites as they make product searches simpler\nand accurate. In this context, the approach of integrating database queries is\nfound to be the most suitable and satisfactory, as it simplifies the searching\nof e commerce products.\n"]},
{"authors": ["Mohd Muntjir", "Ahmad Tasnim Siddiqui"], "title": ["An Enhanced Framework with Advanced Study to Incorporate the Searching\n  of E-Commerce Products Using Modernization of Database Queries"], "date": ["2017-07-01T18:33:39Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1707.00186v1"], "summary": ["  This study aims to inspect and evaluate the integration of database queries\nand their use in e-commerce product searches. It has been observed that\ne-commerce is one of the most prominent trends, which have been emerged in the\nbusiness world, for the past decade. E-commerce has gained tremendous\npopularity, as it offers higher flexibility, cost efficiency, effectiveness,\nand convenience, to both, consumers and businesses. Large number of retailing\ncompanies has adopted this technology, in order to expand their operations,\nacross of the globe; hence they needs to have highly responsive and integrated\ndatabases. In this regard, the approach of database queries is found to be the\nmost appropriate and adequate techniques, as it simplifies the searches of\ne-commerce products.\n"]},
{"authors": ["Mayank Kejriwal"], "title": ["Adaptive Candidate Generation for Scalable Edge-discovery Tasks on Data\n  Graphs"], "date": ["2016-05-02T20:51:43Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1605.00686v2"], "summary": ["  Several `edge-discovery' applications over graph-based data models are known\nto have worst-case quadratic time complexity in the nodes, even if the\ndiscovered edges are sparse. One example is the generic link discovery problem\nbetween two graphs, which has invited research interest in several communities.\nSpecific versions of this problem include link prediction in social networks,\nontology alignment between metadata-rich RDF data, approximate joins, and\nentity resolution between instance-rich data. As large datasets continue to\nproliferate, reducing quadratic complexity to make the task practical is an\nimportant research problem. Within the entity resolution community, the problem\nis commonly referred to as blocking. A particular class of learnable blocking\nschemes is known as Disjunctive Normal Form (DNF) blocking schemes, and has\nemerged as state-of-the art for homogeneous (i.e. same-schema) tabular data.\nDespite the promise of these schemes, a formalism or learning framework has not\nbeen developed for them when input data instances are generic, attributed\ngraphs possessing both node and edge heterogeneity. With such a development,\nthe complexity-reducing scope of DNF schemes becomes applicable to a variety of\nproblems, including entity resolution and type alignment between heterogeneous\ngraphs, and link prediction in networks represented as attributed graphs. This\npaper presents a graph-theoretic formalism for DNF schemes, and investigates\ntheir learnability in an optimization framework. We also briefly describe an\nempirical case study encapsulating some of the principles in this paper.\n"]},
{"authors": ["Haoci Zhang", "Thibault Sellam", "Eugene Wu"], "title": ["Precision Interfaces"], "date": ["2017-04-10T19:15:44Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1704.03022v2"], "summary": ["  Building interactive tools to support data analysis is hard because it is not\nalways clear what to build and how to build it. To address this problem, we\npresent Precision Interfaces, a semi-automatic system to generate task-specific\ndata analytics interfaces. Precision Interface can turn a log of executed\nprograms into an interface, by identifying micro-variations between the\nprograms and mapping them to interface components. This paper focuses on SQL\nquery logs, but we can generalize the approach to other languages. Our system\noperates in two steps: it first build an interaction graph, which describes how\nthe queries can be transformed into each other. Then, it finds a set of UI\ncomponents that covers a maximal number of transformations. To restrict the\ndomain of changes to be detected, our system uses a domain-specific language,\nPILang. We give a full description of Precision Interface's components,\nshowcase an early prototype on real program logs and discuss future research\nopportunities.\n"]},
{"authors": ["Nikos Bikakis", "Melina Skourla", "George Papastefanatos"], "title": ["rdf:SynopsViz - A Framework for Hierarchical Linked Data Visual\n  Exploration and Analysis"], "date": ["2014-08-13T21:16:31Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1408.3148v2"], "summary": ["  The purpose of data visualization is to offer intuitive ways for information\nperception and manipulation, especially for non-expert users. The Web of Data\nhas realized the availability of a huge amount of datasets. However, the volume\nand heterogeneity of available information make it difficult for humans to\nmanually explore and analyse large datasets. In this paper, we present\nrdf:SynopsViz, a tool for hierarchical charting and visual exploration of\nLinked Open Data (LOD). Hierarchical LOD exploration is based on the creation\nof multiple levels of hierarchically related groups of resources based on the\nvalues of one or more properties. The adopted hierarchical model provides\neffective information abstraction and summarization. Also, it allows efficient\n-on the fly- statistic computations, using aggregations over the hierarchy\nlevels.\n"]},
{"authors": ["Lingjiao Chen", "Arun Kumar", "Jeffrey Naughton", "Jignesh M. Patel"], "title": ["Towards Linear Algebra over Normalized Data"], "date": ["2016-12-22T05:41:27Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1612.07448v6"], "summary": ["  Providing machine learning (ML) over relational data is a mainstream\nrequirement for data analytics systems. While almost all the ML tools require\nthe input data to be presented as a single table, many datasets are\nmulti-table, which forces data scientists to join those tables first, leading\nto data redundancy and runtime waste. Recent works on \"factorized\" ML mitigate\nthis issue for a few specific ML algorithms by pushing ML through joins. But\ntheir approaches require a manual rewrite of ML implementations. Such piecemeal\nmethods create a massive development overhead when extending such ideas to\nother ML algorithms. In this paper, we show that it is possible to mitigate\nthis overhead by leveraging a popular formal algebra to represent the\ncomputations of many ML algorithms: linear algebra. We introduce a new logical\ndata type to represent normalized data and devise a framework of algebraic\nrewrite rules to convert a large set of linear algebra operations over\ndenormalized data into operations over normalized data. We show how this\nenables us to automatically \"factorize\" several popular ML algorithms, thus\nunifying and generalizing several prior works. We prototype our framework in\nthe popular ML environment R and an industrial R-over-RDBMS tool. Experiments\nwith both synthetic and real normalized data show that our framework also\nyields significant speed-ups, up to 36x on real data.\n"]},
{"authors": ["Leopoldo Bertossi"], "title": ["The Causality/Repair Connection in Databases: Causality-Programs"], "date": ["2017-04-17T21:58:45Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1704.05136v2"], "summary": ["  In this work, answer-set programs that specify repairs of databases are used\nas a basis for solving computational and reasoning problems about causes for\nquery answers from databases.\n"]},
{"authors": ["Sinica Alboaie", "Doina Cosovan"], "title": ["Private Data System Enabling Self-Sovereign Storage Managed by\n  Executable Choreographies"], "date": ["2017-06-26T15:00:05Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1708.09332v1"], "summary": ["  With the increased use of Internet, governments and large companies store and\nshare massive amounts of personal data in such a way that leaves no space for\ntransparency. When a user needs to achieve a simple task like applying for\ncollege or a driving license, he needs to visit a lot of institutions and\norganizations, thus leaving a lot of private data in many places. The same\nhappens when using the Internet. These privacy issues raised by the centralized\narchitectures along with the recent developments in the area of serverless\napplications demand a decentralized private data layer under user control. We\nintroduce the Private Data System (PDS), a distributed approach which enables\nself-sovereign storage and sharing of private data. The system is composed of\nnodes spread across the entire Internet managing local key-value databases. The\ncommunication between nodes is achieved through executable choreographies,\nwhich are capable of preventing information leakage when executing across\ndifferent organizations with different regulations in place. The user has full\ncontrol over his private data and is able to share and revoke access to\norganizations at any time. Even more, the updates are propagated instantly to\nall the parties which have access to the data thanks to the system design.\nSpecifically, the processing organizations may retrieve and process the shared\ninformation, but are not allowed under any circumstances to store it on long\nterm. PDS offers an alternative to systems that aim to ensure self-sovereignty\nof specific types of data through blockchain inspired techniques but face\nvarious problems, such as low performance. Both approaches propose a\ndistributed database, but with different characteristics. While the\nblockchain-based systems are built to solve consensus problems, PDS's purpose\nis to solve the self-sovereignty aspects raised by the privacy laws, rules and\nprinciples.\n"]},
{"authors": ["Remco Dijkman", "Juntao Gao", "Paul Grefen", "Arthur ter Hofstede"], "title": ["Relational Algebra for In-Database Process Mining"], "date": ["2017-06-26T07:31:36Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1706.08259v1"], "summary": ["  The execution logs that are used for process mining in practice are often\nobtained by querying an operational database and storing the result in a flat\nfile. Consequently, the data processing power of the database system cannot be\nused anymore for this information, leading to constrained flexibility in the\ndefinition of mining patterns and limited execution performance in mining large\nlogs. Enabling process mining directly on a database - instead of via\nintermediate storage in a flat file - therefore provides additional flexibility\nand efficiency. To help facilitate this ideal of in-database process mining,\nthis paper formally defines a database operator that extracts the 'directly\nfollows' relation from an operational database. This operator can both be used\nto do in-database process mining and to flexibly evaluate process mining\nrelated queries, such as: \"which employee most frequently changes the 'amount'\nattribute of a case from one task to the next\". We define the operator using\nthe well-known relational algebra that forms the formal underpinning of\nrelational databases. We formally prove equivalence properties of the operator\nthat are useful for query optimization and present time-complexity properties\nof the operator. By doing so this paper formally defines the necessary\nrelational algebraic elements of a 'directly follows' operator, which are\nrequired for implementation of such an operator in a DBMS.\n"]},
{"authors": ["Neil G. Marchant", "Benjamin I. P. Rubinstein"], "title": ["In Search of an Entity Resolution OASIS: Optimal Asymptotic Sequential\n  Importance Sampling"], "date": ["2017-03-02T04:49:22Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.00617v3"], "summary": ["  Entity resolution (ER) presents unique challenges for evaluation methodology.\nWhile crowdsourcing platforms acquire ground truth, sound approaches to\nsampling must drive labelling efforts. In ER, extreme class imbalance between\nmatching and non-matching records can lead to enormous labelling requirements\nwhen seeking statistically consistent estimates for rigorous evaluation. This\npaper addresses this important challenge with the OASIS algorithm: a sampler\nand F-measure estimator for ER evaluation. OASIS draws samples from a (biased)\ninstrumental distribution, chosen to ensure estimators with optimal asymptotic\nvariance. As new labels are collected OASIS updates this instrumental\ndistribution via a Bayesian latent variable model of the annotator oracle, to\nquickly focus on unlabelled items providing more information. We prove that\nresulting estimates of F-measure, precision, recall converge to the true\npopulation values. Thorough comparisons of sampling methods on a variety of ER\ndatasets demonstrate significant labelling reductions of up to 83% without loss\nto estimate accuracy.\n"]},
{"authors": ["Antoine Amarilli", "Michael Benedikt"], "title": ["When Can We Answer Queries Using Result-Bounded Data Interfaces?"], "date": ["2017-06-24T10:41:20Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1706.07936v1"], "summary": ["  We consider answering queries where the underlying data is available only\nover limited interfaces which provide lookup access to the tuples matching a\ngiven binding, but possibly restricting the number of output tuples returned.\nInterfaces imposing such \"result bounds\" are common in accessing data via the\nweb. Given a query over a set of relations as well as some integrity\nconstraints that relate the queried relations to the data sources, we examine\nthe problem of deciding if the query is answerable over the interfaces; that\nis, whether there exists a plan that returns all answers to the query, assuming\nthe source data satisfies the integrity constraints.\n  The first component of our analysis of answerability is a reduction to a\nquery containment problem with constraints. The second component is a set of\n\"schema simplification\" theorems capturing limitations on how interfaces with\nresult bounds can be useful to obtain complete answers to queries. These\nresults also help to show decidability for the containment problem that\ncaptures answerability, for many classes of constraints. The final component in\nour analysis of answerability is a \"linearization\" method, showing that query\ncontainment with certain guarded dependencies -- including those that emerge\nfrom answerability problems -- can be reduced to query containment for a\nwell-behaved class of linear dependencies. Putting these components together,\nwe get a detailed picture of how to check answerability over result-bounded\nservices.\n"]},
{"authors": ["Mahmoud Abo Khamis", "Hung Q. Ngo", "XuanLong Nguyen", "Dan Olteanu", "Maximilian Schleich"], "title": ["In-Database Learning with Sparse Tensors"], "date": ["2017-03-14T22:27:09Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.04780v2"], "summary": ["  In-database analytics is of great practical importance as it avoids the\ncostly repeated loop data scientists have to deal with on a daily basis: select\nfeatures, export the data, convert data format, train models using an external\ntool, reimport the parameters. It is also a fertile ground of theoretically\nfundamental and challenging problems at the intersection of relational and\nstatistical data models. This paper introduces a unified framework for training\nand evaluating a class of statistical learning models inside a relational\ndatabase. This class includes ridge linear regression, polynomial regression,\nfactorization machines, and principal component analysis. We show that, by\nsynergizing key tools from relational database theory such as schema\ninformation, query structure, recent advances in query evaluation algorithms,\nand from linear algebra such as various tensor and matrix operations, one can\nformulate in-database learning problems and design efficient algorithms to\nsolve them. The algorithms and models proposed in the paper have already been\nimplemented inside the LogicBlox database engine and used in retail-planning\nand forecasting applications, with significant performance benefits over\nout-of-database solutions that require the costly data-export loop.\n"]},
{"authors": ["David B. Keator", "Jinran Chen", "B Nolan Nichols", "Fariba Fana", "Hal Stern", "Tallie Z. Baram", "Steven L. Small"], "title": ["A Semantic Cross-Species Derived Data Management Application"], "date": ["2017-06-23T18:58:40Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1706.07835v1"], "summary": ["  Managing dynamic information in large multi-site, multi-species, and\nmulti-discipline consortia is a challenging task for data management\napplications. Often in academic research studies the goals for informatics\nteams are to build applications that provide extract-transform-load (ETL)\nfunctionality to archive and catalog source data that has been collected by the\nresearch teams. In consortia that cross species and methodological or\nscientific domains, building interfaces that supply data in a usable fashion\nand make intuitive sense to scientists from dramatically different backgrounds\nincreases the complexity for developers. Further, reusing source data from\noutside one's scientific domain is fraught with ambiguities in understanding\nthe data types, analysis methodologies, and how to combine the data with those\nfrom other research teams. We report on the design, implementation, and\nperformance of a semantic data management application to support the NIMH\nfunded Conte Center at the University of California, Irvine. The Center is\ntesting a theory of the consequences of \"fragmented\" (unpredictable, high\nentropy) early-life experiences on adolescent cognitive and emotional outcomes\nin both humans and rodents. It employs cross-species neuroimaging, epigenomic,\nmolecular, and neuroanatomical approaches in humans and rodents to assess the\npotential consequences of fragmented unpredictable experience on brain\nstructure and circuitry. To address this multi-technology, multi-species\napproach, the system uses semantic web techniques based on the Neuroimaging\nData Model (NIDM) to facilitate data ETL functionality. We find this approach\nenables a low-cost, easy to maintain, and semantically meaningful information\nmanagement system, enabling the diverse research teams to access and use the\ndata.\n"]},
{"authors": ["Ben McCamish", "Arash Termehchy", "Behrouz Touri"], "title": ["A Signaling Game Approach to Databases Querying and Interaction"], "date": ["2016-03-13T19:28:22Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1603.04068v4"], "summary": ["  As most database users cannot precisely express their information needs, it\nis challenging for database management systems to understand them. We propose a\nnovel formal framework for representing and understanding information needs in\ndatabase querying and exploration. Our framework considers querying as a\ncollaboration between the user and the database management system to establish\na it mutual language for representing information needs. We formalize this\ncollaboration as a signaling game, where each mutual language is an equilibrium\nfor the game. A query interface is more effective if it establishes a less\nambiguous mutual language faster. We discuss some equilibria, strategies, and\nthe convergence in this game. In particular, we propose a reinforcement\nlearning mechanism and analyze it within our framework. We prove that this\nadaptation mechanism for the query interface improves the effectiveness of\nanswering queries stochastically speaking, and converges almost surely. We\nextend out results for the cases that the user also modifies her strategy\nduring the interaction.\n"]},
{"authors": ["Florian Gross"], "title": ["Index Search Algorithms for Databases and Modern CPUs"], "date": ["2017-06-20T23:01:52Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1706.06697v1"], "summary": ["  Over the years, many different indexing techniques and search algorithms have\nbeen proposed, including CSS-trees, CSB+ trees, k-ary binary search, and fast\narchitecture sensitive tree search. There have also been papers on how best to\nset the many different parameters of these index structures, such as the node\nsize of CSB+ trees.\n  These indices have been proposed because CPU speeds have been increasing at a\ndramatically higher rate than memory speeds, giving rise to the Von Neumann\nCPU--Memory bottleneck. To hide the long latencies caused by memory access, it\nhas become very important to well-utilize the features of modern CPUs. In order\nto drive down the average number of CPU clock cycles required to execute CPU\ninstructions, and thus increase throughput, it has become important to achieve\na good utilization of CPU resources. Some of these are the data and instruction\ncaches, and the translation lookaside buffers. But it also has become important\nto avoid branch misprediction penalties, and utilize vectorization provided by\nCPUs in the form of SIMD instructions.\n  While the layout of index structures has been heavily optimized for the data\ncache of modern CPUs, the instruction cache has been neglected so far. In this\npaper, we present NitroGen, a framework for utilizing code generation for\nspeeding up index traversal in main memory database systems. By bringing\ntogether data and code, we make index structures use the dormant resource of\nthe instruction cache. We show how to combine index compilation with previous\napproaches, such as binary tree search, cache-sensitive tree search, and the\narchitecture-sensitive tree search presented by Kim et al.\n"]},
{"authors": ["Chen Luo", "Anshumali Shrivastava"], "title": ["Arrays of (locality-sensitive) Count Estimators (ACE): High-Speed\n  Anomaly Detection via Cache Lookups"], "date": ["2017-06-20T21:09:22Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1706.06664v1"], "summary": ["  Anomaly detection is one of the frequent and important subroutines deployed\nin large-scale data processing systems. Even being a well-studied topic,\nexisting techniques for unsupervised anomaly detection require storing\nsignificant amounts of data, which is prohibitive from memory and latency\nperspective. In the big-data world existing methods fail to address the new set\nof memory and latency constraints. In this paper, we propose ACE (Arrays of\n(locality-sensitive) Count Estimators) algorithm that can be 60x faster than\nthe ELKI package~\\cite{DBLP:conf/ssd/AchtertBKSZ09}, which has the fastest\nimplementation of the unsupervised anomaly detection algorithms. ACE algorithm\nrequires less than $4MB$ memory, to dynamically compress the full data\ninformation into a set of count arrays. These tiny $4MB$ arrays of counts are\nsufficient for unsupervised anomaly detection. At the core of the ACE\nalgorithm, there is a novel statistical estimator which is derived from the\nsampling view of Locality Sensitive Hashing(LSH). This view is significantly\ndifferent and efficient than the widely popular view of LSH for near-neighbor\nsearch. We show the superiority of ACE algorithm over 11 popular baselines on 3\nbenchmark datasets, including the KDD-Cup99 data which is the largest available\nbenchmark comprising of more than half a million entries with ground truth\nanomaly labels.\n"]},
{"authors": ["A. K. Akanbi", "M. Masinde"], "title": ["A Framework for Accurate Drought Forecasting System Using\n  Semantics-Based Data Integration Middleware"], "date": ["2017-06-20T13:21:50Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1706.07294v1"], "summary": ["  Technological advancement in Wireless Sensor Networks (WSN) has made it\nbecome an invaluable component of a reliable environmental monitoring system;\nthey form the digital skin' through which to 'sense' and collect the context of\nthe surroundings and provides information on the process leading to complex\nevents such as drought. However, these environmental properties are measured by\nvarious heterogeneous sensors of different modalities in distributed locations\nmaking up the WSN, using different abstruse terms and vocabulary in most cases\nto denote the same observed property, causing data heterogeneity. Adding\nsemantics and understanding the relationships that exist between the observed\nproperties, and augmenting it with local indigenous knowledge is necessary for\nan accurate drought forecasting system. In this paper, we propose the framework\nfor the semantic representation of sensor data and integration with indigenous\nknowledge on drought using a middleware for an efficient drought forecasting\nsystem.\n"]},
{"authors": ["Stratos Idreos", "Lukas M. Maas", "Mike S. Kester"], "title": ["Evolutionary Data Systems"], "date": ["2017-06-18T20:07:56Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1706.05714v1"], "summary": ["  Anyone in need of a data system today is confronted with numerous complex\noptions in terms of system architectures, such as traditional relational\ndatabases, NoSQL and NewSQL solutions as well as several sub-categories like\ncolumn-stores, row-stores etc. This overwhelming array of choices makes\nbootstrapping data-driven applications difficult and time consuming, requiring\nexpertise often not accessible due to cost issues (e.g., to scientific labs or\nsmall businesses). In this paper, we present the vision of evolutionary data\nsystems that free systems architects and application designers from the\ncomplex, cumbersome and expensive process of designing and tuning specialized\ndata system architectures that fit only a single, static application scenario.\nSetting up an evolutionary system is as simple as identifying the data. As new\ndata and queries come in, the system automatically evolves so that its\narchitecture matches the properties of the incoming workload at all times.\nInspired by the theory of evolution, at any given point in time, an\nevolutionary system may employ multiple competing solutions down at the low\nlevel of database architectures -- characterized as combinations of data\nlayouts, access methods and execution strategies. Over time, \"the fittest wins\"\nand becomes the dominant architecture until the environment (workload) changes.\nIn our initial prototype, we demonstrate solutions that can seamlessly evolve\n(back and forth) between a key-value store and a column-store architecture in\norder to adapt to changing workloads.\n"]},
{"authors": ["Panagiotis Mandros", "Mario Boley", "Jilles Vreeken"], "title": ["Discovering Reliable Approximate Functional Dependencies"], "date": ["2017-05-25T23:00:46Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1705.09391v2"], "summary": ["  Given a database and a target attribute of interest, how can we tell whether\nthere exists a functional, or approximately functional dependence of the target\non any set of other attributes in the data? How can we reliably, without bias\nto sample size or dimensionality, measure the strength of such a dependence?\nAnd, how can we efficiently discover the optimal or $\\alpha$-approximate\ntop-$k$ dependencies? These are exactly the questions we answer in this paper.\n  As we want to be agnostic on the form of the dependence, we adopt an\ninformation-theoretic approach, and construct a reliable, bias correcting score\nthat can be efficiently computed. Moreover, we give an effective optimistic\nestimator of this score, by which for the first time we can mine the\napproximate functional dependencies from data with guarantees of optimality.\nEmpirical evaluation shows that the derived score achieves a good bias for\nvariance trade-off, can be used within an efficient discovery algorithm, and\nindeed discovers meaningful dependencies. Most important, it remains reliable\nin the face of data sparsity.\n"]},
{"authors": ["Yue Wang", "Alexandra Meliou", "Gerome Miklau"], "title": ["A Consumer-Centric Market for Database Computation in the Cloud"], "date": ["2016-09-07T18:42:09Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1609.02104v6"], "summary": ["  The availability of public computing resources in the cloud has\nrevolutionized data analysis, but requesting cloud resources often involves\ncomplex decisions for consumers. Under the current pricing mechanisms, cloud\nservice providers offer several service options and charge consumers based on\nthe resources they use. Before they can decide which cloud resources to\nrequest, consumers have to estimate the completion time and cost of their\ncomputational tasks for different service options and possibly for different\nservice providers. This estimation is challenging even for expert cloud users.\nWe propose a new market-based framework for pricing computational tasks in the\ncloud. Our framework introduces an agent between consumers and cloud providers.\nThe agent takes data and computational tasks from users, estimates time and\ncost for evaluating the tasks, and returns to consumers contracts that specify\nthe price and completion time. Our framework can be applied directly to\nexisting cloud markets without altering the way cloud providers offer and price\nservices. In addition, it simplifies cloud use for consumers by allowing them\nto compare contracts, rather than choose resources directly. We present design,\nanalytical, and algorithmic contributions focusing on pricing computation\ncontracts, analyzing their properties, and optimizing them in complex\nworkflows. We conduct an experimental evaluation of our market framework over a\nreal-world cloud service and demonstrate empirically that our market ensures\nthree key properties: competitiveness, fairness, and resilience. Finally, we\npresent a fine-grained pricing mechanism for complex workflows and show that it\ncan increase agent profits by more than an order of magnitude in some cases.\n"]},
{"authors": ["Renzo Angles", "Marcelo Arenas", "Pablo Barcelo", "Aidan Hogan", "Juan Reutter", "Domagoj Vrgoc"], "title": ["Foundations of Modern Query Languages for Graph Databases"], "date": ["2016-10-20T01:59:48Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1610.06264v3"], "summary": ["  We survey foundational features underlying modern graph query languages. We\nfirst discuss two popular graph data models: edge-labelled graphs, where nodes\nare connected by directed, labelled edges; and property graphs, where nodes and\nedges can further have attributes. Next we discuss the two most fundamental\ngraph querying functionalities: graph patterns and navigational expressions. We\nstart with graph patterns, in which a graph-structured query is matched against\nthe data. Thereafter we discuss navigational expressions, in which patterns can\nbe matched recursively against the graph to navigate paths of arbitrary length;\nwe give an overview of what kinds of expressions have been proposed, and how\nthey can be combined with graph patterns. We also discuss several semantics\nunder which queries using the previous features can be evaluated, what effects\nthe selection of features and semantics has on complexity, and offer examples\nof such features in three modern languages that are used to query graphs:\nSPARQL, Cypher and Gremlin. We conclude by discussing the importance of\nformalisation for graph query languages; a summary of what is known about\nSPARQL, Cypher and Gremlin in terms of expressivity and complexity; and an\noutline of possible future directions for the area.\n"]},
{"authors": ["Alexander Krause", "Annett Ungeth\u00fcm", "Thomas Kissinger", "Dirk Habich", "Wolfgang Lehner"], "title": ["Asynchronous Graph Pattern Matching on Multiprocessor Systems"], "date": ["2017-06-13T09:32:37Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1706.03968v2"], "summary": ["  Pattern matching on large graphs is the foundation for a variety of\napplication domains. Strict latency requirements and continuously increasing\ngraph sizes demand the usage of highly parallel in-memory graph processing\nengines that need to consider non-uniform memory access (NUMA) and concurrency\nissues to scale up on modern multiprocessor systems. To tackle these aspects,\ngraph partitioning becomes increasingly important. Hence, we present a\ntechnique to process graph pattern matching on NUMA systems in this paper. As a\nscalable pattern matching processing infrastructure, we leverage a\ndata-oriented architecture that preserves data locality and minimizes\nconcurrency-related bottlenecks on NUMA systems. We show in detail, how graph\npattern matching can be asynchronously processed on a multiprocessor system.\n"]},
{"authors": ["Edith Cohen"], "title": ["Multi-Objective Weighted Sampling"], "date": ["2015-09-24T17:27:13Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1509.07445v6"], "summary": ["  {\\em Multi-objective samples} are powerful and versatile summaries of large\ndata sets. For a set of keys $x\\in X$ and associated values $f_x \\geq 0$, a\nweighted sample taken with respect to $f$ allows us to approximate {\\em\nsegment-sum statistics} $\\text{Sum}(f;H) = \\text{sum}_{x\\in H} f_x$, for any\nsubset $H$ of the keys, with statistically-guaranteed quality that depends on\nsample size and the relative weight of $H$. When estimating $\\text{Sum}(g;H)$\nfor $g\\not=f$, however, quality guarantees are lost. A multi-objective sample\nwith respect to a set of functions $F$ provides for each $f\\in F$ the same\nstatistical guarantees as a dedicated weighted sample while minimizing the\nsummary size.\n  We analyze properties of multi-objective samples and present sampling schemes\nand meta-algortithms for estimation and optimization while showcasing two\nimportant application domains. The first are key-value data sets, where\ndifferent functions $f\\in F$ applied to the values correspond to different\nstatistics such as moments, thresholds, capping, and sum. A multi-objective\nsample allows us to approximate all statistics in $F$. The second is metric\nspaces, where keys are points, and each $f\\in F$ is defined by a set of points\n$C$ with $f_x$ being the service cost of $x$ by $C$, and $\\text{Sum}(f;X)$\nmodels centrality or clustering cost of $C$. A multi-objective sample allows us\nto estimate costs for each $f\\in F$. In these domains, multi-objective samples\nare often of small size, are efficiently to construct, and enable scalable\nestimation and optimization. We aim here to facilitate further applications of\nthis powerful technique.\n"]},
{"authors": ["Ophir Lojkine"], "title": ["Optimal parameters for bloom-filtered joins in Spark"], "date": ["2017-06-08T22:29:41Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1706.02785v2"], "summary": ["  In this paper, we present an algorithm that joins relational database tables\nefficiently in a distributed environment using Bloom filters of an optimal\nsize. We propose not to use fixed-size bloom filters as in previous research,\nbut to find an optimal size for the bloom filters, by creating a mathematical\nmodel of the join algorithm, and then finding the optimal parameters using\ntraditional mathematical optimization.\n  This algorithm with optimal parameters beats both previous approaches using\nbloom filters and the default SparkSQL engine not only on star-joins, but also\non traditional database schema. The experiments were conducted on a standard\nTPC-H database stored as parquet files on a distributed file system.\n"]},
{"authors": ["Egon B\u00f6rger", "Klaus-Dieter Schewe", "Qing Wang"], "title": ["Serialisable Multi-Level Transaction Control: A Specification and\n  Verification"], "date": ["2017-06-12T08:34:35Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1706.04043v1"], "summary": ["  We define a programming language independent controller TaCtl for multi-level\ntransactions and an operator $TA$, which when applied to concurrent programs\nwith multi-level shared locations containing hierarchically structured complex\nvalues, turns their behavior with respect to some abstract termination\ncriterion into a transactional behavior. We prove the correctness property that\nconcurrent runs under the transaction controller are serialisable, assuming an\nInverse Operation Postulate to guarantee recoverability. For its applicability\nto a wide range of programs we specify the transaction controller TaCtl and the\noperator $TA$ in terms of Abstract State Machines (ASMs). This allows us to\nmodel concurrent updates at different levels of nested locations in a precise\nyet simple manner, namely in terms of partial ASM updates. It also provides the\npossibility to use the controller TaCtl and the operator $TA$ as a plug-in when\nspecifying concurrent system components in terms of sequential ASMs.\n"]},
{"authors": ["Kijung Shin", "Bryan Hooi", "Jisu Kim", "Christos Faloutsos"], "title": ["DenseAlert: Incremental Dense-Subtensor Detection in Tensor Streams"], "date": ["2017-06-11T16:26:40Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1706.03374v1"], "summary": ["  Consider a stream of retweet events - how can we spot fraudulent lock-step\nbehavior in such multi-aspect data (i.e., tensors) evolving over time? Can we\ndetect it in real time, with an accuracy guarantee? Past studies have shown\nthat dense subtensors tend to indicate anomalous or even fraudulent behavior in\nmany tensor data, including social media, Wikipedia, and TCP dumps. Thus,\nseveral algorithms have been proposed for detecting dense subtensors rapidly\nand accurately. However, existing algorithms assume that tensors are static,\nwhile many real-world tensors, including those mentioned above, evolve over\ntime.\n  We propose DenseStream, an incremental algorithm that maintains and updates a\ndense subtensor in a tensor stream (i.e., a sequence of changes in a tensor),\nand DenseAlert, an incremental algorithm spotting the sudden appearances of\ndense subtensors. Our algorithms are: (1) Fast and 'any time': updates by our\nalgorithms are up to a million times faster than the fastest batch algorithms,\n(2) Provably accurate: our algorithms guarantee a lower bound on the density of\nthe subtensor they maintain, and (3) Effective: our DenseAlert successfully\nspots anomalies in real-world tensors, especially those overlooked by existing\nalgorithms.\n"]},
{"authors": ["Ms. Ganesan Kavitha", "Dr. Lawrance Raj"], "title": ["Educational Data Mining and Learning Analytics - Educational Assistance\n  for Teaching and Learning"], "date": ["2017-06-11T09:03:39Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1706.03327v1"], "summary": ["  Teaching and Learning process of an educational institution needs to be\nmonitored and effectively analysed for enhancement. Teaching and Learning is a\nvital element for an educational institution. It is also one of the criteria\nset by majority of the Accreditation Agencies around the world. Learning\nanalytics and Educational Data Mining are relatively new. Learning analytics\nrefers to the collection of large volume of data about students in an\neducational setting and to analyse the data to predict the students' future\nperformance and identify risk. Educational Data Mining (EDM) is develops\nmethods to analyse the data produced by the students in educational settings\nand these methods helps to understand the students and the setting where they\nlearn. Aim of this research is to collect large collection of data on students'\nperformance in their assessment to discover the students at risk of failing the\nfinal exam. This analysis will help to understand how the students are\nprogressing. The proposed research aimed to utilize the result of the analysis\nto identify the students at risk and provide recommendations for improvement.\nThe proposed research aimed to collect and analyse the result of the assessment\nat the course level to enhance the teaching and learning process. The research\naimed to discuss two feature selection techniques namely information gain and\ngain ratio and adopted to use gain ratio as the feature selection technique.\n"]},
{"authors": ["Marius Rafailescu"], "title": ["Fault Tolerant Consensus Agreement Algorithm"], "date": ["2017-06-11T07:28:48Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1706.03317v1"], "summary": ["  Recently a new fault tolerant and simple mechanism was designed for solving\ncommit consensus problem. It is based on replicated validation of messages sent\nbetween transaction participants and a special dispatcher validator manager\nnode. This paper presents a correctness, safety proofs and performance analysis\nof this algorithm.\n"]},
{"authors": ["Peter Bailis", "Kunle Olukotun", "Christopher Re", "Matei Zaharia"], "title": ["Infrastructure for Usable Machine Learning: The Stanford DAWN Project"], "date": ["2017-05-22T02:28:19Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1705.07538v2"], "summary": ["  Despite incredible recent advances in machine learning, building machine\nlearning applications remains prohibitively time-consuming and expensive for\nall but the best-trained, best-funded engineering organizations. This expense\ncomes not from a need for new and improved statistical models but instead from\na lack of systems and tools for supporting end-to-end machine learning\napplication development, from data preparation and labeling to\nproductionization and monitoring. In this document, we outline opportunities\nfor infrastructure supporting usable, end-to-end machine learning applications\nin the context of the nascent DAWN (Data Analytics for What's Next) project at\nStanford.\n"]},
{"authors": ["Benjamin I. P. Rubinstein", "Francesco Ald\u00e0"], "title": ["Pain-Free Random Differential Privacy with Sensitivity Sampling"], "date": ["2017-06-08T13:06:34Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1706.02562v1"], "summary": ["  Popular approaches to differential privacy, such as the Laplace and\nexponential mechanisms, calibrate randomised smoothing through global\nsensitivity of the target non-private function. Bounding such sensitivity is\noften a prohibitively complex analytic calculation. As an alternative, we\npropose a straightforward sampler for estimating sensitivity of non-private\nmechanisms. Since our sensitivity estimates hold with high probability, any\nmechanism that would be $(\\epsilon,\\delta)$-differentially private under\nbounded global sensitivity automatically achieves\n$(\\epsilon,\\delta,\\gamma)$-random differential privacy (Hall et al., 2012),\nwithout any target-specific calculations required. We demonstrate on worked\nexample learners how our usable approach adopts a naturally-relaxed privacy\nguarantee, while achieving more accurate releases even for non-private\nfunctions that are black-box computer programs.\n"]},
{"authors": ["Marco Guarnieri", "Srdjan Marinovic", "David Basin"], "title": ["Securing Databases from Probabilistic Inference"], "date": ["2017-06-08T08:17:16Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1706.02473v1"], "summary": ["  Databases can leak confidential information when users combine query results\nwith probabilistic data dependencies and prior knowledge. Current research\noffers mechanisms that either handle a limited class of dependencies or lack\ntractable enforcement algorithms. We propose a foundation for Database\nInference Control based on ProbLog, a probabilistic logic programming language.\nWe leverage this foundation to develop Angerona, a provably secure enforcement\nmechanism that prevents information leakage in the presence of probabilistic\ndependencies. We then provide a tractable inference algorithm for a practically\nrelevant fragment of ProbLog. We empirically evaluate Angerona's performance\nshowing that it scales to relevant security-critical problems.\n"]},
{"authors": ["Maikel L. van Eck", "Natalia Sidorova", "Wil M. P. van der Aalst"], "title": ["Guided Interaction Exploration in Artifact-centric Process Models"], "date": ["2017-06-07T09:55:48Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1706.02109v1"], "summary": ["  Artifact-centric process models aim to describe complex processes as a\ncollection of interacting artifacts. Recent development in process mining allow\nfor the discovery of such models. However, the focus is often on the\nrepresentation of the individual artifacts rather than their interactions.\nBased on event data we can automatically discover composite state machines\nrepresenting artifact-centric processes. Moreover, we provide ways of\nvisualizing and quantifying interactions among different artifacts. For\nexample, we are able to highlight strongly correlated behaviours in different\nartifacts. The approach has been fully implemented as a ProM plug-in; the CSM\nMiner provides an interactive artifact-centric process discovery tool focussing\non interactions. The approach has been evaluated using real life data sets,\nincluding the personal loan and overdraft process of a Dutch financial\ninstitution.\n"]},
{"authors": ["Egon B\u00f6rger", "Klaus-Dieter Schewe"], "title": ["Specifying Transaction Control to Serialize Concurrent Program\n  Executions"], "date": ["2017-06-06T13:43:08Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1706.01762v1"], "summary": ["  We define a programming language independent transaction controller and an\noperator which when applied to concurrent programs with shared locations turns\ntheir behavior with respect to some abstract termination criterion into a\ntransactional behavior. We prove the correctness property that concurrent runs\nunder the transaction controller are serialisable. We specify the transaction\ncontroller TaCtl and the operator TA in terms of Abstract State Machines. This\nmakes TaCtl applicable to a wide range of programs and in particular provides\nthe possibility to use it as a plug-in when specifying concurrent system\ncomponents in terms of Abstract State Machines.\n"]},
{"authors": ["Firas Abuzaid", "Geet Sethi", "Peter Bailis", "Matei Zaharia"], "title": ["SimDex: Exploiting Model Similarity in Exact Matrix Factorization\n  Recommendations"], "date": ["2017-06-05T17:56:43Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1706.01449v1"], "summary": ["  We present SimDex, a new technique for serving exact top-K recommendations on\nmatrix factorization models that measures and optimizes for the similarity\nbetween users in the model. Previous serving techniques presume a high degree\nof similarity (e.g., L2 or cosine distance) among users and/or items in MF\nmodels; however, as we demonstrate, the most accurate models are not guaranteed\nto exhibit high similarity. As a result, brute-force matrix multiply\noutperforms recent proposals for top-K serving on several collaborative\nfiltering tasks. Based on this observation, we develop SimDex, a new technique\nfor serving matrix factorization models that automatically optimizes serving\nbased on the degree of similarity between users, and outperforms existing\nmethods in both the high-similarity and low-similarity regimes. SimDexfirst\nmeasures the degree of similarity among users via clustering and uses a\ncost-based optimizer to either construct an index on the model or defer to\nblocked matrix multiply. It leverages highly efficient linear algebra\nprimitives in both cases to deliver predictions either from its index or from\nbrute-force multiply. Overall, SimDex runs an average of 2x and up to 6x faster\nthan highly optimized baselines for the most accurate models on several popular\ncollaborative filtering datasets.\n"]},
{"authors": ["Vraj Shah", "Arun Kumar", "Xiaojin Zhu"], "title": ["Are Key-Foreign Key Joins Safe to Avoid when Learning High-Capacity\n  Classifiers?"], "date": ["2017-04-03T09:16:58Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1704.00485v3"], "summary": ["  Machine learning (ML) over relational data is a booming area of the database\nindustry and academia. While several projects aim to build scalable and fast ML\nsystems, little work has addressed the pains of sourcing data and features for\nML tasks. Real-world relational databases typically have many tables (often,\ndozens) and data scientists often struggle to even obtain and join all possible\ntables that provide features for ML. In this context, Kumar et al. showed\nrecently that key-foreign key dependencies (KFKDs) between tables often lets us\navoid such joins without significantly affecting prediction accuracy--an idea\nthey called avoiding joins safely. While initially controversial, this idea has\nsince been used by multiple companies to reduce the burden of data sourcing for\nML. But their work applied only to linear classifiers. In this work, we verify\nif their results hold for three popular complex classifiers: decision trees,\nSVMs, and ANNs. We conduct an extensive experimental study using both\nreal-world datasets and simulations to analyze the effects of avoiding KFK\njoins on such models. Our results show that these high-capacity classifiers are\nsurprisingly and counter-intuitively more robust to avoiding KFK joins compared\nto linear classifiers, refuting an intuition from the prior work's analysis. We\nexplain this behavior intuitively and identify open questions at the\nintersection of data management and ML theoretical research. All of our code\nand datasets are available for download from\nhttp://cseweb.ucsd.edu/~arunkk/hamlet.\n"]},
{"authors": ["Ying Lu", "Juan A. Colmenares"], "title": ["Efficient Detection of Points of Interest from Georeferenced Visual\n  Content"], "date": ["2017-06-02T17:03:06Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1706.00757v1"], "summary": ["  Many people take photos and videos with smartphones and more recently with\n360-degree cameras at popular places and events, and share them in social\nmedia. Such visual content is produced in large volumes in urban areas, and it\nis a source of information that online users could exploit to learn what has\ngot the interest of the general public on the streets of the cities where they\nlive or plan to visit. A key step to providing users with that information is\nto identify the most popular k spots in specified areas. In this paper, we\npropose a clustering and incremental sampling (C&IS) approach that trades off\naccuracy of top-k results for detection speed. It uses clustering to determine\nareas with high density of visual content, and incremental sampling, controlled\nby stopping criteria, to limit the amount of computational work. It leverages\nspatial metadata, which represent the scenes in the visual content, to rapidly\ndetect the hotspots, and uses a recently proposed Gaussian probability model to\ndescribe the capture intention distribution in the query area. We evaluate the\napproach with metadata, derived from a non-synthetic, user-generated dataset,\nfor regular mobile and 360-degree visual content. Our results show that the\nC&IS approach offers 2.8x-19x reductions in processing time over an optimized\nbaseline, while in most cases correctly identifying 4 out of 5 top locations.\n"]},
{"authors": ["Hoang Thanh Lam", "Johann-Michael Thiebaut", "Mathieu Sinn", "Bei Chen", "Tiep Mai", "Oznur Alkan"], "title": ["One button machine for automating feature engineering in relational\n  databases"], "date": ["2017-06-01T14:44:34Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1706.00327v1"], "summary": ["  Feature engineering is one of the most important and time consuming tasks in\npredictive analytics projects. It involves understanding domain knowledge and\ndata exploration to discover relevant hand-crafted features from raw data. In\nthis paper, we introduce a system called One Button Machine, or OneBM for\nshort, which automates feature discovery in relational databases. OneBM\nautomatically performs a key activity of data scientists, namely, joining of\ndatabase tables and applying advanced data transformations to extract useful\nfeatures from data. We validated OneBM in Kaggle competitions in which OneBM\nachieved performance as good as top 16% to 24% data scientists in three Kaggle\ncompetitions. More importantly, OneBM outperformed the state-of-the-art system\nin a Kaggle competition in terms of prediction accuracy and ranking on Kaggle\nleaderboard. The results show that OneBM can be useful for both data scientists\nand non-experts. It helps data scientists reduce data exploration time allowing\nthem to try and error many ideas in short time. On the other hand, it enables\nnon-experts, who are not familiar with data science, to quickly extract value\nfrom their data with a little effort, time and cost.\n"]},
{"authors": ["Serkan Ayvaz", "Mehmet Aydar"], "title": ["Dynamic Discovery of Type Classes and Relations in Semantic Web Data"], "date": ["2017-05-31T11:58:31Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1706.02591v1"], "summary": ["  The continuing development of Semantic Web technologies and the increasing\nuser adoption in the recent years have accelerated the progress incorporating\nexplicit semantics with data on the Web. With the rapidly growing RDF (Resource\nDescription Framework) data on the Semantic Web, processing large semantic\ngraph data have become more challenging. Constructing a summary graph structure\nfrom the raw RDF can help obtain semantic type relations and reduce the\ncomputational complexity for graph processing purposes. In this paper, we\naddressed the problem of graph summarization in RDF graphs, and we proposed an\napproach for building summary graph structures automatically from RDF graph\ndata. Moreover, we introduced a measure to help discover optimum class\ndissimilarity thresholds and an effective method to discover the type classes\nautomatically. In future work, we plan to investigate further improvement\noptions on the scalability of the proposed method.\n"]},
{"authors": ["Yue Wang", "Yeye He"], "title": ["Synthesizing Mapping Relationships Using Table Corpus"], "date": ["2017-05-25T17:46:55Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1705.09276v2"], "summary": ["  Mapping relationships, such as (country, country-code) or (company,\nstock-ticker), are versatile data assets for an array of applications in data\ncleaning and data integration like auto-correction and auto-join. However,\ntoday there are no good repositories of mapping tables that can enable these\nintelligent applications.\n  Given a corpus of tables such as web tables or spreadsheet tables, we observe\nthat values of these mappings often exist in pairs of columns in same tables.\nMotivated by their broad applicability, we study the problem of synthesizing\nmapping relationships using a large table corpus. Our synthesis process\nleverages compatibility of tables based on co-occurrence statistics, as well as\nconstraints such as functional dependency. Experiment results using web tables\nand enterprise spreadsheets suggest that the proposed approach can produce high\nquality mappings.\n"]},
{"authors": ["Md Farhadur Rahman", "Abolfazl Asudeh", "Nick Koudas", "Gautam Das"], "title": ["Efficient Computation of Subspace Skyline over Categorical Domains"], "date": ["2017-02-28T22:37:27Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.00080v3"], "summary": ["  Platforms such as AirBnB, Zillow, Yelp, and related sites have transformed\nthe way we search for accommodation, restaurants, etc. The underlying datasets\nin such applications have numerous attributes that are mostly Boolean or\nCategorical. Discovering the skyline of such datasets over a subset of\nattributes would identify entries that stand out while enabling numerous\napplications. There are only a few algorithms designed to compute the skyline\nover categorical attributes, yet are applicable only when the number of\nattributes is small.\n  In this paper, we place the problem of skyline discovery over categorical\nattributes into perspective and design efficient algorithms for two cases. (i)\nIn the absence of indices, we propose two algorithms, ST-S and ST-P, that\nexploits the categorical characteristics of the datasets, organizing tuples in\na tree data structure, supporting efficient dominance tests over the candidate\nset. (ii) We then consider the existence of widely used precomputed sorted\nlists. After discussing several approaches, and studying their limitations, we\npropose TA-SKY, a novel threshold style algorithm that utilizes sorted lists.\nMoreover, we further optimize TA-SKY and explore its progressive nature, making\nit suitable for applications with strict interactive requirements. In addition\nto the extensive theoretical analysis of the proposed algorithms, we conduct a\ncomprehensive experimental evaluation of the combination of real (including the\nentire AirBnB data collection) and synthetic datasets to study the practicality\nof the proposed algorithms. The results showcase the superior performance of\nour techniques, outperforming applicable approaches by orders of magnitude.\n"]},
{"authors": ["Yeounoh Chung", "Sanjay Krishnan", "Tim Kraska"], "title": ["A Data Quality Metric (DQM): How to Estimate The Number of Undetected\n  Errors in Data Sets"], "date": ["2016-11-15T15:00:53Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1611.04878v3"], "summary": ["  Data cleaning, whether manual or algorithmic, is rarely perfect leaving a\ndataset with an unknown number of false positives and false negatives after\ncleaning. In many scenarios, quantifying the number of remaining errors is\nchallenging because our data integrity rules themselves may be incomplete, or\nthe available gold-standard datasets may be too small to extrapolate. As the\nuse of inherently fallible crowds becomes more prevalent in data cleaning\nproblems, it is important to have estimators to quantify the extent of such\nerrors. We propose novel species estimators to estimate the number of distinct\nremaining errors in a dataset after it has been cleaned by a set of crowd\nworkers -- essentially, quantifying the utility of hiring additional workers to\nclean the dataset. This problem requires new estimators that are robust to\nfalse positives and false negatives, and we empirically show on three\nreal-world datasets that existing species estimators are unstable for this\nproblem, while our proposed techniques quickly converge.\n"]},
{"authors": ["Niek Tax", "Natalia Sidorova", "Reinder Haakma", "Wil M. P. van der Aalst"], "title": ["Mining Process Model Descriptions of Daily Life through Event\n  Abstraction"], "date": ["2017-05-25T20:32:56Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1705.10202v1"], "summary": ["  Process mining techniques focus on extracting insight in processes from event\nlogs. Process mining has the potential to provide valuable insights in\n(un)healthy habits and to contribute to ambient assisted living solutions when\napplied on data from smart home environments. However, events recorded in smart\nhome environments are on the level of sensor triggers, at which process\ndiscovery algorithms produce overgeneralizing process models that allow for too\nmuch behavior and that are difficult to interpret for human experts. We show\nthat abstracting the events to a higher-level interpretation can enable\ndiscovery of more precise and more comprehensible models. We present a\nframework for the extraction of features that can be used for abstraction with\nsupervised learning methods that is based on the XES IEEE standard for event\nlogs. This framework can automatically abstract sensor-level events to their\ninterpretation at the human activity level, after training it on training data\nfor which both the sensor and human activity events are known. We demonstrate\nour abstraction framework on three real-life smart home event logs and show\nthat the process models that can be discovered after abstraction are more\nprecise indeed.\n"]},
{"authors": ["Panos Parchas", "Nikolaos Papailiou", "Dimitris Papadias", "Francesco Bonchi"], "title": ["Uncertain Graph Sparsification"], "date": ["2016-11-14T09:58:11Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1611.04308v4"], "summary": ["  Uncertain graphs are prevalent in several applications including\ncommunications systems, biological databases and social networks. The ever\nincreasing size of the underlying data renders both graph storage and query\nprocessing extremely expensive. Sparsification has often been used to reduce\nthe size of deterministic graphs by maintaining only the important edges.\nHowever, adaptation of deterministic sparsification methods fails in the\nuncertain setting. To overcome this problem, we introduce the first\nsparsification techniques aimed explicitly at uncertain graphs. The proposed\nmethods reduce the number of edges and redistribute their probabilities in\norder to decrease the graph size, while preserving its underlying structure.\nThe resulting graph can be used to efficiently and accurately approximate any\nquery and mining tasks on the original graph. An extensive experimental\nevaluation with real and synthetic datasets illustrates the effectiveness of\nour techniques on several common graph tasks, including clustering coefficient,\npage rank, reliability and shortest path distance.\n"]},
{"authors": ["Sridevi Baskaran", "Alexander Keller", "Fei Chiang", "Golab Lukasz", "Jaroslaw Szlichta"], "title": ["Efficient Discovery of Ontology Functional Dependencies"], "date": ["2016-11-08T22:03:35Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1611.02737v3"], "summary": ["  Poor data quality has become a pervasive issue due to the increasing\ncomplexity and size of modern datasets. Constraint based data cleaning\ntechniques rely on integrity constraints as a benchmark to identify and correct\nerrors. Data values that do not satisfy the given set of constraints are\nflagged as dirty, and data updates are made to re-align the data and the\nconstraints. However, many errors often require user input to resolve due to\ndomain expertise defining specific terminology and relationships. For example,\nin pharmaceuticals, 'Advil' \\emph{is-a} brand name for 'ibuprofen' that can be\ncaptured in a pharmaceutical ontology. While functional dependencies (FDs) have\ntraditionally been used in existing data cleaning solutions to model syntactic\nequivalence, they are not able to model broader relationships (e.g., is-a)\ndefined by an ontology. In this paper, we take a first step towards extending\nthe set of data quality constraints used in data cleaning by defining and\ndiscovering \\emph{Ontology Functional Dependencies} (OFDs). We lay out\ntheoretical and practical foundations for OFDs, including a set of sound and\ncomplete axioms, and a linear inference procedure. We then develop effective\nalgorithms for discovering OFDs, and a set of optimizations that efficiently\nprune the search space. Our experimental evaluation using real data show the\nscalability and accuracy of our algorithms.\n"]},
{"authors": ["Laurel Orr", "Magda Balazinska", "Dan Suciu"], "title": ["Probabilistic Database Summarization for Interactive Data Exploration"], "date": ["2017-03-10T22:17:22Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.03856v2"], "summary": ["  We present a probabilistic approach to generate a small, query-able summary\nof a dataset for interactive data exploration. Departing from traditional\nsummarization techniques, we use the Principle of Maximum Entropy to generate a\nprobabilistic representation of the data that can be used to give approximate\nquery answers. We develop the theoretical framework and formulation of our\nprobabilistic representation and show how to use it to answer queries. We then\npresent solving techniques and give three critical optimizations to improve\npreprocessing time and query accuracy. Lastly, we experimentally evaluate our\nwork using a 5 GB dataset of flights within the United States and a 210 GB\ndataset from an astronomy particle simulation. While our current work only\nsupports linear queries, we show that our technique can successfully answer\nqueries faster than sampling while introducing, on average, no more error than\nsampling and can better distinguish between rare and nonexistent values.\n"]},
{"authors": ["Omar Almootassem", "Syed Hamza Husain", "Denesh Parthipan", "Qusay H. Mahmoud"], "title": ["A Cloud-based Service for Real-Time Performance Evaluation of NoSQL\n  Databases"], "date": ["2017-05-23T14:32:12Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1705.08317v1"], "summary": ["  We have created a cloud-based service that allows the end users to run tests\non multiple different databases to find which databases are most suitable for\ntheir project. From our research, we could not find another application that\nenables the user to test several databases to gauge the difference between\nthem. This application allows the user to choose which type of test to perform\nand which databases to target. The application also displays the results of\ndifferent tests that were run by other users previously. There is also a map to\nshow the location where all the tests are run to give the user an estimate of\nthe location. Unlike the orthodox static tests and reports conducted to\nevaluate NoSQL databases, we have created a web application to run and analyze\nthese tests in real time. This web application evaluates the performance of\nseveral NoSQL databases. The databases covered are MongoDB, DynamoDB, CouchDB,\nand Firebase. The web service is accessible from: nosqldb.nextproject.ca.\n"]},
{"authors": ["Matt M. T. Yiu", "Helen H. W. Chan", "Patrick P. C. Lee"], "title": ["Erasure Coding for Small Objects in In-Memory KV Storage"], "date": ["2017-01-27T15:39:50Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.08084v2"], "summary": ["  We present MemEC, an erasure-coding-based in-memory key-value (KV) store that\nachieves high availability and fast recovery while keeping low data redundancy\nacross storage servers. MemEC is specifically designed for workloads dominated\nby small objects. By encoding objects in entirety, MemEC is shown to incur 60%\nless storage redundancy for small objects than existing replication- and\nerasure-coding-based approaches. It also supports graceful transitions between\ndecentralized requests in normal mode (i.e., no failures) and coordinated\nrequests in degraded mode (i.e., with failures). We evaluate our MemEC\nprototype via testbed experiments under read-heavy and update-heavy YCSB\nworkloads. We show that MemEC achieves high throughput and low latency in both\nnormal and degraded modes, and supports fast transitions between the two modes.\n"]},
{"authors": ["Ioannis Giannakopoulos", "Dimitrios Tsoumakos", "Nectarios Koziris"], "title": ["A Decision Tree Based Approach Towards Adaptive Profiling of Distributed\n  Applications"], "date": ["2017-04-10T13:46:58Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1704.02855v2"], "summary": ["  The adoption of the distributed paradigm has allowed applications to increase\ntheir scalability, robustness and fault tolerance, but it has also complicated\ntheir structure, leading to an exponential growth of the applications'\nconfiguration space and increased difficulty in predicting their performance.\nIn this work, we describe a novel, automated profiling methodology that makes\nno assumptions on application structure. Our approach utilizes oblique Decision\nTrees in order to recursively partition an application's configuration space in\ndisjoint regions, choose a set of representative samples from each subregion\naccording to a defined policy and return a model for the entire space as a\ncomposition of linear models over each subregion. An extensive evaluation over\nreal-life applications and synthetic performance functions showcases that our\nscheme outperforms other state-of-the-art profiling methodologies. It\nparticularly excels at reflecting abnormalities and discontinuities of the\nperformance function, allowing the user to influence the sampling policy based\non the modeling accuracy and the space coverage.\n"]},
{"authors": ["Elias Stehle", "Hans-Arno Jacobsen"], "title": ["A Memory Bandwidth-Efficient Hybrid Radix Sort on GPUs"], "date": ["2016-11-03T19:33:06Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1611.01137v2"], "summary": ["  Sorting is at the core of many database operations, such as index creation,\nsort-merge joins, and user-requested output sorting. As GPUs are emerging as a\npromising platform to accelerate various operations, sorting on GPUs becomes a\nviable endeavour. Over the past few years, several improvements have been\nproposed for sorting on GPUs, leading to the first radix sort implementations\nthat achieve a sorting rate of over one billion 32-bit keys per second. Yet,\nstate-of-the-art approaches are heavily memory bandwidth-bound, as they require\nsubstantially more memory transfers than their CPU-based counterparts.\n  Our work proposes a novel approach that almost halves the amount of memory\ntransfers and, therefore, considerably lifts the memory bandwidth limitation.\nBeing able to sort two gigabytes of eight-byte records in as little as 50\nmilliseconds, our approach achieves a 2.32-fold improvement over the\nstate-of-the-art GPU-based radix sort for uniform distributions, sustaining a\nminimum speed-up of no less than a factor of 1.66 for skewed distributions.\n  To address inputs that either do not reside on the GPU or exceed the\navailable device memory, we build on our efficient GPU sorting approach with a\npipelined heterogeneous sorting algorithm that mitigates the overhead\nassociated with PCIe data transfers. Comparing the end-to-end sorting\nperformance to the state-of-the-art CPU-based radix sort running 16 threads,\nour heterogeneous approach achieves a 2.06-fold and a 1.53-fold improvement for\nsorting 64 GB key-value pairs with a skewed and a uniform distribution,\nrespectively.\n"]},
{"authors": ["Magnus J\u00e4ndel", "Pontus Svenson", "Ronnie Johansson"], "title": ["Fusing restricted information"], "date": ["2017-05-18T13:05:33Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1706.05913v1"], "summary": ["  Information fusion deals with the integration and merging of data and\ninformation from multiple (heterogeneous) sources. In many cases, the\ninformation that needs to be fused has security classification. The result of\nthe fusion process is then by necessity restricted with the strictest\ninformation security classification of the inputs. This has severe drawbacks\nand limits the possible dissemination of the fusion results. It leads to\ndecreased situational awareness: the organization knows information that would\nenable a better situation picture, but since parts of the information is\nrestricted, it is not possible to distribute the most correct situational\ninformation. In this paper, we take steps towards defining fusion and data\nmining processes that can be used even when all the underlying data that was\nused cannot be disseminated. The method we propose here could be used to\nproduce a classifier where all the sensitive information has been removed and\nwhere it can be shown that an antagonist cannot even in principle obtain\nknowledge about the classified information by using the classifier or situation\npicture.\n"]},
{"authors": ["Niek Tax", "Ilya Verenich", "Marcello La Rosa", "Marlon Dumas"], "title": ["Predictive Business Process Monitoring with LSTM Neural Networks"], "date": ["2016-12-07T07:04:17Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1612.02130v2"], "summary": ["  Predictive business process monitoring methods exploit logs of completed\ncases of a process in order to make predictions about running cases thereof.\nExisting methods in this space are tailor-made for specific prediction tasks.\nMoreover, their relative accuracy is highly sensitive to the dataset at hand,\nthus requiring users to engage in trial-and-error and tuning when applying them\nin a specific setting. This paper investigates Long Short-Term Memory (LSTM)\nneural networks as an approach to build consistently accurate models for a wide\nrange of predictive process monitoring tasks. First, we show that LSTMs\noutperform existing techniques to predict the next event of a running case and\nits timestamp. Next, we show how to use models for predicting the next task in\norder to predict the full continuation of a running case. Finally, we apply the\nsame approach to predict the remaining time, and show that this approach\noutperforms existing tailor-made methods.\n"]},
{"authors": ["Niek Tax", "Xixi Lu", "Natalia Sidorova", "Dirk Fahland", "Wil M. P. van der Aalst"], "title": ["The Imprecisions of Precision Measures in Process Mining"], "date": ["2017-05-03T11:50:45Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1705.03303v2"], "summary": ["  In process mining, precision measures are used to quantify how much a process\nmodel overapproximates the behavior seen in an event log. Although several\nmeasures have been proposed throughout the years, no research has been done to\nvalidate whether these measures achieve the intended aim of quantifying\nover-approximation in a consistent way for all models and logs. This paper\nfills this gap by postulating a number of axioms for quantifying precision\nconsistently for any log and any model. Further, we show through\ncounter-examples that none of the existing measures consistently quantifies\nprecision.\n"]},
{"authors": ["Niek Tax", "Natalia Sidorova", "Reinder Haakma", "Wil M. P. van der Aalst"], "title": ["Mining Local Process Models"], "date": ["2016-06-20T11:28:26Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1606.06066v2"], "summary": ["  In this paper we describe a method to discover frequent behavioral patterns\nin event logs. We express these patterns as \\emph{local process models}. Local\nprocess model mining can be positioned in-between process discovery and episode\n/ sequential pattern mining. The technique presented in this paper is able to\nlearn behavioral patterns involving sequential composition, concurrency, choice\nand loop, like in process mining. However, we do not look at start-to-end\nmodels, which distinguishes our approach from process discovery and creates a\nlink to episode / sequential pattern mining. We propose an incremental\nprocedure for building local process models capturing frequent patterns based\non so-called process trees. We propose five quality dimensions and\ncorresponding metrics for local process models, given an event log. We show\nmonotonicity properties for some quality dimensions, enabling a speedup of\nlocal process model discovery through pruning. We demonstrate through a real\nlife case study that mining local patterns allows us to get insights in\nprocesses where regular start-to-end process discovery techniques are only able\nto learn unstructured, flower-like, models.\n"]},
{"authors": ["Felix Mannhardt", "Niek Tax"], "title": ["Unsupervised Event Abstraction using Pattern Abstraction and Local\n  Process Models"], "date": ["2017-04-11T20:08:14Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1704.03520v2"], "summary": ["  Process mining analyzes business processes based on events stored in event\nlogs. However, some recorded events may correspond to activities on a very low\nlevel of abstraction. When events are recorded on a too low level of\ngranularity, process discovery methods tend to generate overgeneralizing\nprocess models. Grouping low-level events to higher level activities, i.e.,\nevent abstraction, can be used to discover better process models. Existing\nevent abstraction methods are mainly based on common sub-sequences and\nclustering techniques. In this paper, we propose to first discover local\nprocess models and then use those models to lift the event log to a higher\nlevel of abstraction. Our conjecture is that process models discovered on the\nobtained high-level event log return process models of higher quality: their\nfitness and precision scores are more balanced. We show this with preliminary\nresults on several real-life event logs.\n"]},
{"authors": ["Rui Meng", "Hao Xin", "Lei Chen", "Yangqiu Song"], "title": ["Subjective Knowledge Acquisition and Enrichment Powered By Crowdsourcing"], "date": ["2017-05-16T14:25:02Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1705.05720v1"], "summary": ["  Knowledge bases (KBs) have attracted increasing attention due to its great\nsuccess in various areas, such as Web and mobile search.Existing KBs are\nrestricted to objective factual knowledge, such as city population or fruit\nshape, whereas,subjective knowledge, such as big city, which is commonly\nmentioned in Web and mobile queries, has been neglected. Subjective knowledge\ndiffers from objective knowledge in that it has no documented or observed\nground truth. Instead, the truth relies on people's dominant opinion. Thus, we\ncan use the crowdsourcing technique to get opinion from the crowd. In our work,\nwe propose a system, called crowdsourced subjective knowledge acquisition\n(CoSKA),for subjective knowledge acquisition powered by crowdsourcing and\nexisting KBs. The acquired knowledge can be used to enrich existing KBs in the\nsubjective dimension which bridges the gap between existing objective knowledge\nand subjective queries.The main challenge of CoSKA is the conflict between\nlarge scale knowledge facts and limited crowdsourcing resource. To address this\nchallenge, in this work, we define knowledge inference rules and then select\nthe seed knowledge judiciously for crowdsourcing to maximize the inference\npower under the resource constraint. Our experimental results on real knowledge\nbase and crowdsourcing platform verify the effectiveness of CoSKA system.\n"]},
{"authors": ["Xiangnan Ren", "Olivier Cur\u00e9"], "title": ["Strider: A Hybrid Adaptive Distributed RDF Stream Processing Engine"], "date": ["2017-05-16T12:54:23Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1705.05688v1"], "summary": ["  Real-time processing of data streams emanating from sensors is becoming a\ncommon task in Internet of Things scenarios. The key implementation goal\nconsists in efficiently handling massive incoming data streams and supporting\nadvanced data analytics services like anomaly detection. In an on-going,\nindustrial project, we found out that a 24/7 available stream processing engine\nusually faces dynamically changing data and workload characteristics. These\nchanges impact the engine's performance and reliability. We propose Strider, a\nhybrid adaptive distributed RDF Stream Processing engine that optimizes logical\nquery plan according to the state of data streams. Strider has been designed to\nguarantee important industrial properties such as scalability, high\navailability, fault-tolerant, high throughput and acceptable latency. These\nguarantees are obtained by designing the engine's architecture with\nstate-of-the-art Apache components such as Spark and Kafka. We highlight the\nefficiency (e.g., on a single machine machine, up to 60x gain on throughput\ncompared to state-of-the-art systems, a throughput of 3.1 million\ntriples/second on a 9 machines cluster, a major breakthrough in this system's\ncategory) of Strider on real-world and synthetic data sets.\n"]},
{"authors": ["Abhay Bhadani", "Dhanya Jothimani"], "title": ["Big Data: Challenges, Opportunities and Realities"], "date": ["2017-05-14T08:44:28Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1705.04928v1"], "summary": ["  With the advent of Internet of Things (IoT) and Web 2.0 technologies, there\nhas been a tremendous growth in the amount of data generated. This chapter\nemphasizes on the need for big data, technological advancements, tools and\ntechniques being used to process big data are discussed. Technological\nimprovements and limitations of existing storage techniques are also presented.\nSince, the traditional technologies like Relational Database Management System\n(RDBMS) have their own limitations to handle big data, new technologies have\nbeen developed to handle them and to derive useful insights. This chapter\npresents an overview of big data analytics, its application, advantages, and\nlimitations. Few research issues and future directions are presented in this\nchapter.\n"]},
{"authors": ["Furong Li", "Xin Luna Dong", "Anno Langen", "Yang Li"], "title": ["Discovering Multiple Truths with a Hybrid Model"], "date": ["2017-05-14T04:03:15Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1705.04915v1"], "summary": ["  Many data management applications require integrating information from\nmultiple sources. The sources may not be accurate and provide erroneous values.\nWe thus have to identify the true values from conflicting observations made by\nthe sources. The problem is further complicated when there may exist multiple\ntruths (e.g., a book written by several authors). In this paper we propose a\nmodel called Hybrid that jointly makes two decisions: how many truths there\nare, and what they are. It considers the conflicts between values as important\nevidence for ruling out wrong values, while keeps the flexibility of allowing\nmultiple truths. In this way, Hybrid is able to achieve both high precision and\nhigh recall.\n"]},
{"authors": ["Tommaso Soru", "Edgard Marx", "Axel-Cyrille Ngonga Ngomo"], "title": ["ROCKER: A Refinement Operator for Key Discovery"], "date": ["2017-05-11T21:26:06Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1705.04380v1"], "summary": ["  The Linked Data principles provide a decentral approach for publishing\nstructured data in the RDF format on the Web. In contrast to structured data\npublished in relational databases where a key is often provided explicitly,\nfinding a set of properties that allows identifying a resource uniquely is a\nnon-trivial task. Still, finding keys is of central importance for manifold\napplications such as resource deduplication, link discovery, logical data\ncompression and data integration. In this paper, we address this research gap\nby specifying a refinement operator, dubbed ROCKER, which we prove to be\nfinite, proper and non-redundant. We combine the theoretical characteristics of\nthis operator with two monotonicities of keys to obtain a time-efficient\napproach for detecting keys, i.e., sets of properties that describe resources\nuniquely. We then utilize a hash index to compute the discriminability score\nefficiently. Therewith, we ensure that our approach can scale to very large\nknowledge bases. Results show that ROCKER yields more accurate results, has a\ncomparable runtime, and consumes less memory w.r.t. existing state-of-the-art\ntechniques.\n"]},
{"authors": ["Neil D. Lawrence"], "title": ["Data Readiness Levels"], "date": ["2017-05-05T14:53:56Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1705.02245v1"], "summary": ["  Application of models to data is fraught. Data-generating collaborators often\nonly have a very basic understanding of the complications of collating,\nprocessing and curating data. Challenges include: poor data collection\npractices, missing values, inconvenient storage mechanisms, intellectual\nproperty, security and privacy. All these aspects obstruct the sharing and\ninterconnection of data, and the eventual interpretation of data through\nmachine learning or other approaches. In project reporting, a major challenge\nis in encapsulating these problems and enabling goals to be built around the\nprocessing of data. Project overruns can occur due to failure to account for\nthe amount of time required to curate and collate. But to understand these\nfailures we need to have a common language for assessing the readiness of a\nparticular data set. This position paper proposes the use of data readiness\nlevels: it gives a rough outline of three stages of data preparedness and\nspeculates on how formalisation of these levels into a common language for data\nreadiness could facilitate project management.\n"]},
{"authors": ["Shahzad Ahmed", "M. Usman Ali", "Javed Ferzund", "Muhammad Atif Sarwar", "Abbas Rehman", "Atif Mehmood"], "title": ["Modern Data Formats for Big Bioinformatics Data Analytics"], "date": ["2017-05-05T11:35:53Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1707.05364v1"], "summary": ["  Next Generation Sequencing (NGS) technology has resulted in massive amounts\nof proteomics and genomics data. This data is of no use if it is not properly\nanalyzed. ETL (Extraction, Transformation, Loading) is an important step in\ndesigning data analytics applications. ETL requires proper understanding of\nfeatures of data. Data format plays a key role in understanding of data,\nrepresentation of data, space required to store data, data I/O during\nprocessing of data, intermediate results of processing, in-memory analysis of\ndata and overall time required to process data. Different data mining and\nmachine learning algorithms require input data in specific types and formats.\nThis paper explores the data formats used by different tools and algorithms and\nalso presents modern data formats that are used on Big Data Platform. It will\nhelp researchers and developers in choosing appropriate data format to be used\nfor a particular tool or algorithm.\n"]},
{"authors": ["Tianzheng Wang", "Ryan Johnson", "Alan Fekete", "Ippokratis Pandis"], "title": ["Efficiently making (almost) any concurrency control mechanism\n  serializable"], "date": ["2016-05-13T19:14:52Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1605.04292v5"], "summary": ["  Concurrency control (CC) algorithms must trade off strictness for\nperformance. Serializable CC schemes generally pay higher cost to prevent\nanomalies, both in runtime overhead and in efforts wasted by aborting\ntransactions. We propose the serial safety net (SSN), a\nserializability-enforcing certifier which can be applied with minimal overhead\non top of various CC schemes that offer higher performance but admit anomalies,\nsuch as snapshot isolation and read committed. The underlying CC retains\ncontrol of scheduling and transactional accesses, while SSN tracks the\nresulting dependencies. At commit time, SSN performs an efficient validation\ntest by examining only direct dependencies of the committing transaction to\ndetermine whether it can commit safely or must abort to avoid a potential\ndependency cycle.\n  SSN performs robustly for various workloads. It maintains the characteristics\nof the underlying CC without biasing toward certain types of transactions,\nthough the underlying CC might. Besides traditional OLTP workloads, SSN also\nallows efficient handling of heterogeneous workloads with long, read-mostly\ntransactions. SSN can avoid tracking the majority of reads (thus reducing the\noverhead of serializability certification) and still produce serializable\nexecutions with little overhead. The dependency tracking and validation tests\ncan be done efficiently, fully parallel and latch-free, for multi-version\nsystems on modern hardware with substantial core count and large main memory.\n  We demonstrate the efficiency, accuracy and robustness of SSN using extensive\nsimulations and an implementation that overlays snapshot isolation in ERMIA, a\nmemory-optimized OLTP engine that is capable of running different CC schemes.\nEvaluation results confirm that SSN is a promising approach to serializability\nwith robust performance and low overhead for various workloads.\n"]},
{"authors": ["Leopoldo Bertossi", "Mostafa Milani"], "title": ["The Ontological Multidimensional Data Model"], "date": ["2017-03-10T02:48:29Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.03524v2"], "summary": ["  In this extended abstract we describe, mainly by examples, the main elements\nof the Ontological Multidimensional Data Model, which considerably extends a\nrelational reconstruction of the multidimensional data model proposed by\nHurtado and Mendelzon by means of tuple-generating dependencies,\nequality-generating dependencies, and negative constraints as found in\nDatalog+-. We briefly mention some good computational properties of the model.\n"]},
{"authors": ["Bikash Chandra", "Bhupesh Chawda", "Biplab Kar", "K. V. Maheshwara Reddy", "Shetal Shah", "S. Sudarshan"], "title": ["Data Generation for Testing and Grading SQL Queries"], "date": ["2014-11-25T02:06:02Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1411.6704v5"], "summary": ["  Correctness of SQL queries is usually tested by executing the queries on one\nor more datasets. Erroneous queries are often the results of small changes, or\nmutations of the correct query. A mutation Q' of a query Q is killed by a\ndataset D if Q(D) $\\neq$ Q'(D). Earlier work on the XData system showed how to\ngenerate datasets that kill all mutations in a class of mutations that included\njoin type and comparison operation mutations.\n  In this paper, we extend the XData data generation techniques to handle a\nwider variety of SQL queries and a much larger class of mutations. We have also\nbuilt a system for grading SQL queries using the datasets generated by XData.\nWe present a study of the effectiveness of the datasets generated by the\nextended XData approach, using a variety of queries including queries submitted\nby students as part of a database course. We show that the XData datasets\noutperform predefined datasets as well as manual grading done earlier by\nteaching assistants, while also avoiding the drudgery of manual correction.\nThus, we believe that our techniques will be of great value to database course\ninstructors and TAs, particularly to those of MOOCs. It will also be valuable\nto database application developers and testers for testing SQL queries.\n"]},
{"authors": ["Mahmoud Mahdi", "Samir Abdelrahman", "Reem Bahgat", "Ismail Ismail"], "title": ["F-tree: an algorithm for clustering transactional data using frequency\n  tree"], "date": ["2017-05-02T01:55:44Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1705.00761v1"], "summary": ["  Clustering is an important data mining technique that groups similar data\nrecords, recently categorical transaction clustering is received more\nattention. In this research, we study the problem of categorical data\nclustering for transactional data characterized with high dimensionality and\nlarge volume. We propose a novel algorithm for clustering transactional data\ncalled F-Tree, which is based on the idea of the frequent pattern algorithm\nFP-tree; the fastest approaches to the frequent item set mining. And the simple\nidea behind the F-Tree is to generate small high pure clusters, and then merge\nthem. That makes it fast, and dynamic in clustering large transactional\ndatasets with high dimensions. We also present a new solution to solve the\noverlapping problem between clusters, by defining a new criterion function,\nwhich is based on the probability of overlapping between weighted items. Our\nexperimental evaluation on real datasets shows that: Firstly, F-Tree is\neffective in finding interesting clusters. Secondly, the usage of the tree\nstructure reduces the clustering process time of the large data set with high\nattributes. Thirdly, the proposed evaluation metric used efficiently to solve\nthe overlapping of transaction items generates high-quality clustering results.\nFinally, we have concluded that the process of merging pure and small clusters\nincreases the purity of resulted clusters as well as it reduces the time of\nclustering better than the process of generating clusters directly from dataset\nthen refine clusters.\n"]},
{"authors": ["Bernardo Gon\u00e7alves", "H. V. Jagadish"], "title": ["Bsmooth: Learning from user feedback to disambiguate query terms in\n  interactive data retrieval"], "date": ["2016-10-15T22:24:50Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1610.04789v3"], "summary": ["  There is great interest in supporting imprecise queries (e.g., keyword search\nor natural language queries) over databases today. To support such queries, the\ndatabase system is typically required to disambiguate parts of the\nuser-specified query against the database, using whatever resources are\nintrinsically available to it (the database schema, data values distributions,\nnatural language models etc). Often, systems will also have a user-interaction\nlog available, which can serve as an extrinsic resource to supplement their\nmodel based on their own intrinsic resources. This leads to a problem of how\nbest to combine the system's prior ranking with insight derived from the\nuser-interaction log. Statistical inference techniques such as maximum\nlikelihood or Bayesian updates from a subjective prior turn out not to apply in\na straightforward way due to possible noise from user search behavior and to\nencoding biases endemic to the system's models. In this paper, we address such\nlearning problem in interactive data retrieval, with specific focus on type\nclassification for user-specified query terms. We develop a novel Bayesian\nsmoothing algorithm, Bsmooth, which is simple, fast, flexible and accurate. We\nanalytically establish some desirable properties and show, through experiments\nagainst an independent benchmark, that the addition of such a learning layer\nperforms much better than standard methods.\n"]},
{"authors": ["Luan Tran", "Hien To", "Liyue Fan", "Cyrus Shahabi"], "title": ["A Real-Time Framework for Task Assignment in Hyperlocal Spatial\n  Crowdsourcing"], "date": ["2017-04-23T01:12:27Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1704.06868v2"], "summary": ["  Spatial Crowdsourcing (SC) is a novel platform that engages individuals in\nthe act of collecting various types of spatial data. This method of data\ncollection can significantly reduce cost and turnover time, and is particularly\nuseful in urban environmental sensing, where traditional means fail to provide\nfine-grained field data. In this study, we introduce hyperlocal spatial\ncrowdsourcing, where all workers who are located within the spatiotemporal\nvicinity of a task are eligible to perform the task, e.g., reporting the\nprecipitation level at their area and time. In this setting, there is often a\nbudget constraint, either for every time period or for the entire campaign, on\nthe number of workers to activate to perform tasks. The challenge is thus to\nmaximize the number of assigned tasks under the budget constraint, despite the\ndynamic arrivals of workers and tasks. We introduce a taxonomy of several\nproblem variants, such as budget-per-time-period vs. budget-per-campaign and\nbinary-utility vs. distance-based-utility. We study the hardness of the task\nassignment problem in the offline setting and propose online heuristics which\nexploits the spatial and temporal knowledge acquired over time. Our experiments\nare conducted with spatial crowdsourcing workloads generated by the SCAWG tool\nand extensive results show the effectiveness and efficiency of our proposed\nsolutions.\n"]},
{"authors": ["Elena Botoeva", "Diego Calvanese", "Benjamin Cogrel", "Guohui Xiao"], "title": ["Expressivity and Complexity of MongoDB (Extended Version)"], "date": ["2016-03-30T17:47:06Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1603.09291v3"], "summary": ["  A significant number of novel database architectures and data models have\nbeen proposed during the last decade. While some of these new systems have\ngained in popularity, they lack a proper formalization, and a precise\nunderstanding of the expressivity and the computational properties of the\nassociated query languages. In this paper, we aim at filling this gap, and we\ndo so by considering MongoDB, a widely adopted document database managing\ncomplex (tree structured) values represented in a JSON-based data model,\nequipped with a powerful query mechanism. We provide a formalization of the\nMongoDB data model, and of a core fragment, called MQuery, of the MongoDB query\nlanguage. We study the expressivity of MQuery, showing its equivalence with\nnested relational algebra. We further investigate the computational complexity\nof significant fragments of it, obtaining several (tight) bounds in combined\ncomplexity, which range from LOGSPACE to alternating exponential-time with a\npolynomial number of alternations. As a consequence, we obtain also a\ncharacterization of the combined complexity of nested relational algebra query\nevaluation.\n"]},
{"authors": ["Sebastiaan J. van Zelst", "Boudewijn F. van Dongen", "Wil M. P. van der Aalst"], "title": ["Event Stream-Based Process Discovery using Abstract Representations"], "date": ["2017-04-25T12:10:35Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1704.08101v1"], "summary": ["  The aim of process discovery, originating from the area of process mining, is\nto discover a process model based on business process execution data. A\nmajority of process discovery techniques relies on an event log as an input. An\nevent log is a static source of historical data capturing the execution of a\nbusiness process. In this paper we focus on process discovery relying on online\nstreams of business process execution events. Learning process models from\nevent streams poses both challenges and opportunities, i.e. we need to handle\nunlimited amounts of data using finite memory and, preferably, constant time.\nWe propose a generic architecture that allows for adopting several classes of\nexisting process discovery techniques in context of event streams. Moreover, we\nprovide several instantiations of the architecture, accompanied by\nimplementations in the process mining tool-kit ProM (http://promtools.org).\nUsing these instantiations, we evaluate several dimensions of stream-based\nprocess discovery. The evaluation shows that the proposed architecture allows\nus to lift process discovery to the streaming domain.\n"]},
{"authors": ["Sabbir Ahmad", "Rafi Kamal", "Mohammed Eunus Ali", "Jianzhong Qi", "Peter Scheuermann", "Egemen Tanin"], "title": ["The Flexible Group Spatial Keyword Query"], "date": ["2017-04-24T18:33:50Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1704.07405v1"], "summary": ["  We present a new class of service for location based social networks, called\nthe Flexible Group Spatial Keyword Query, which enables a group of users to\ncollectively find a point of interest (POI) that optimizes an aggregate cost\nfunction combining both spatial distances and keyword similarities. In\naddition, our query service allows users to consider the tradeoffs between\nobtaining a sub-optimal solution for the entire group and obtaining an\noptimimized solution but only for a subgroup.\n  We propose algorithms to process three variants of the query: (i) the group\nnearest neighbor with keywords query, which finds a POI that optimizes the\naggregate cost function for the whole group of size n, (ii) the subgroup\nnearest neighbor with keywords query, which finds the optimal subgroup and a\nPOI that optimizes the aggregate cost function for a given subgroup size m (m\n<= n), and (iii) the multiple subgroup nearest neighbor with keywords query,\nwhich finds optimal subgroups and corresponding POIs for each of the subgroup\nsizes in the range [m, n]. We design query processing algorithms based on\nbranch-and-bound and best-first paradigms. Finally, we provide theoretical\nbounds and conduct extensive experiments with two real datasets which verify\nthe effectiveness and efficiency of the proposed algorithms.\n"]},
{"authors": ["Fabien Andr\u00e9", "Anne-Marie Kermarrec", "Nicolas Le Scouarnec"], "title": ["Accelerated Nearest Neighbor Search with Quick ADC"], "date": ["2017-04-24T17:49:37Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1704.07355v1"], "summary": ["  Efficient Nearest Neighbor (NN) search in high-dimensional spaces is a\nfoundation of many multimedia retrieval systems. Because it offers low\nresponses times, Product Quantization (PQ) is a popular solution. PQ compresses\nhigh-dimensional vectors into short codes using several sub-quantizers, which\nenables in-RAM storage of large databases. This allows fast answers to NN\nqueries, without accessing the SSD or HDD. The key feature of PQ is that it can\ncompute distances between short codes and high-dimensional vectors using\ncache-resident lookup tables. The efficiency of this technique, named\nAsymmetric Distance Computation (ADC), remains limited because it performs many\ncache accesses.\n  In this paper, we introduce Quick ADC, a novel technique that achieves a 3 to\n6 times speedup over ADC by exploiting Single Instruction Multiple Data (SIMD)\nunits available in current CPUs. Efficiently exploiting SIMD requires\nalgorithmic changes to the ADC procedure. Namely, Quick ADC relies on two key\nmodifications of ADC: (i) the use 4-bit sub-quantizers instead of the standard\n8-bit sub-quantizers and (ii) the quantization of floating-point distances.\nThis allows Quick ADC to exceed the performance of state-of-the-art systems,\ne.g., it achieves a Recall@100 of 0.94 in 3.4 ms on 1 billion SIFT descriptors\n(128-bit codes).\n"]},
{"authors": ["Mario Boley", "Bryan R. Goldsmith", "Luca M. Ghiringhelli", "Jilles Vreeken"], "title": ["Identifying Consistent Statements about Numerical Data with\n  Dispersion-Corrected Subgroup Discovery"], "date": ["2017-01-26T13:36:43Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.07696v2"], "summary": ["  Existing algorithms for subgroup discovery with numerical targets do not\noptimize the error or target variable dispersion of the groups they find. This\noften leads to unreliable or inconsistent statements about the data, rendering\npractical applications, especially in scientific domains, futile. Therefore, we\nhere extend the optimistic estimator framework for optimal subgroup discovery\nto a new class of objective functions: we show how tight estimators can be\ncomputed efficiently for all functions that are determined by subgroup size\n(non-decreasing dependence), the subgroup median value, and a dispersion\nmeasure around the median (non-increasing dependence). In the important special\ncase when dispersion is measured using the average absolute deviation from the\nmedian, this novel approach yields a linear time algorithm. Empirical\nevaluation on a wide range of datasets shows that, when used within\nbranch-and-bound search, this approach is highly efficient and indeed discovers\nsubgroups with much smaller errors.\n"]},
{"authors": ["Hien To", "Cyrus Shahabi"], "title": ["Location Privacy in Spatial Crowdsourcing"], "date": ["2017-04-23T00:09:11Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1704.06860v1"], "summary": ["  Spatial crowdsourcing (SC) is a new platform that engages individuals in\ncollecting and analyzing environmental, social and other spatiotemporal\ninformation. With SC, requesters outsource their spatiotemporal tasks to a set\nof workers, who will perform the tasks by physically traveling to the tasks'\nlocations. This chapter identifies privacy threats toward both workers and\nrequesters during the two main phases of spatial crowdsourcing, tasking and\nreporting. Tasking is the process of identifying which tasks should be assigned\nto which workers. This process is handled by a spatial crowdsourcing server\n(SC-server). The latter phase is reporting, in which workers travel to the\ntasks' locations, complete the tasks and upload their reports to the SC-server.\nThe challenge is to enable effective and efficient tasking as well as reporting\nin SC without disclosing the actual locations of workers (at least until they\nagree to perform a task) and the tasks themselves (at least to workers who are\nnot assigned to those tasks). This chapter aims to provide an overview of the\nstate-of-the-art in protecting users' location privacy in spatial\ncrowdsourcing. We provide a comparative study of a diverse set of solutions in\nterms of task publishing modes (push vs. pull), problem focuses (tasking and\nreporting), threats (server, requester and worker), and underlying technical\napproaches (from pseudonymity, cloaking, and perturbation to exchange-based and\nencryption-based techniques). The strengths and drawbacks of the techniques are\nhighlighted, leading to a discussion of open problems and future work.\n"]},
{"authors": ["Wenqiang Liu"], "title": ["Truth Discovery to Resolve Object Conflicts in Linked Data"], "date": ["2015-09-01T00:58:16Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1509.00104v8"], "summary": ["  In the community of Linked Data, anyone can publish their data as Linked Data\non the web because of the openness of the Semantic Web. As such, RDF (Resource\nDescription Framework) triples described the same real-world entity can be\nobtained from multiple sources; it inevitably results in conflicting objects\nfor a certain predicate of a real-world entity. The objective of this study is\nto identify one truth from multiple conflicting objects for a certain predicate\nof a real-world entity. An intuitive principle based on common sense is that an\nobject from a reliable source is trustworthy; thus, a source that provide\ntrustworthy object is reliable. Many truth discovery methods based on this\nprinciple have been proposed to estimate source reliability and identify the\ntruth. However, the effectiveness of existing truth discovery methods is\nsignificantly affected by the number of objects provided by each source.\nTherefore, these methods cannot be trivially extended to resolve conflicts in\nLinked Data with a scale-free property, i.e., most of the sources provide few\nconflicting objects, whereas only a few sources have many conflicting objects.\nTo address this challenge, we propose a novel approach called TruthDiscover to\nidentify the truth in Linked Data with a scale-free property. Two strategies\nare adopted in TruthDiscover to reduce the effect of the scale-free property on\ntruth discovery. First, this approach leverages the topological properties of\nthe Source Belief Graph to estimate the priori beliefs of sources, which are\nutilized to smooth the trustworthiness of sources. Second, this approach\nutilizes the Hidden Markov Random Field to model the interdependencies between\nobjects to estimate the trust values of objects accurately. Experiments are\nconducted in the six datasets to evaluate TruthDiscover.\n"]},
{"authors": ["Wenqiang Liu", "Jun Liu", "Jian Zhang", "Haimeng Duan", "Bifan Wei"], "title": ["TruthDiscover: Resolving Object Conflicts on Massive Linked Data"], "date": ["2016-03-07T13:34:36Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1603.02056v2"], "summary": ["  Considerable effort has been made to increase the scale of Linked Data.\nHowever, because of the openness of the Semantic Web and the ease of extracting\nLinked Data from semi-structured sources (e.g., Wikipedia) and unstructured\nsources, many Linked Data sources often provide conflicting objects for a\ncertain predicate of a real-world entity. Existing methods cannot be trivially\nextended to resolve conflicts in Linked Data because Linked Data has a\nscale-free property. In this demonstration, we present a novel system called\nTruthDiscover, to identify the truth in Linked Data with a scale-free property.\nFirst, TruthDiscover leverages the topological properties of the Source Belief\nGraph to estimate the priori beliefs of sources, which are utilized to smooth\nthe trustworthiness of sources. Second, the Hidden Markov Random Field is\nutilized to model interdependencies among objects for estimating the trust\nvalues of objects accurately. TruthDiscover can visualize the process of\nresolving conflicts in Linked Data. Experiments results on four datasets show\nthat TruthDiscover exhibits satisfactory accuracy when confronted with data\nhaving a scale-free property.\n"]},
{"authors": ["Wenqiang Liu", "Jun Liu", "Haimeng Duan", "Xie He", "Bifan Wei"], "title": ["Exploiting Source-Object Network to Resolve Object Conflicts in Linked\n  Data"], "date": ["2016-04-28T13:22:54Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1604.08407v3"], "summary": ["  Considerable effort has been made to increase the scale of Linked Data.\nHowever, an inevitable problem when dealing with data integration from multiple\nsources is that multiple different sources often provide conflicting objects\nfor a certain predicate of the same real-world entity, so-called object\nconflicts problem. Currently, the object conflicts problem has not received\nsufficient attention in the Linked Data community. In this paper, we first\nformalize the object conflicts resolution problem as computing the joint\ndistribution of variables on a heterogeneous information network called the\nSource-Object Network, which successfully captures the all correlations from\nobjects and Linked Data sources. Then, we introduce a novel approach based on\nnetwork effects called ObResolution(Object Resolution), to identify a true\nobject from multiple conflicting objects. ObResolution adopts a pairwise Markov\nRandom Field (pMRF) to model all evidences under a unified framework. Extensive\nexperimental results on six real-world datasets show that our method exhibits\nhigher accuracy than existing approaches and it is robust and consistent in\nvarious domains. \\keywords{Linked Data, Object Conflicts, Linked Data Quality,\nTruth Discovery\n"]},
{"authors": ["Georgios Kellaris", "Stavros Papadopoulos", "Dimitris Papadias"], "title": ["Engineering Methods for Differentially Private Histograms: Efficiency\n  Beyond Utility"], "date": ["2015-04-14T07:29:25Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1504.03440v2"], "summary": ["  Publishing histograms with $\\epsilon$-differential privacy has been studied\nextensively in the literature. Existing schemes aim at maximizing the utility\nof the published data, while previous experimental evaluations analyze the\nprivacy/utility trade-off. In this paper we provide the first experimental\nevaluation of differentially private methods that goes beyond utility,\nemphasizing also on another important aspect, namely efficiency. Towards this\nend, we first observe that all existing schemes are comprised of a small set of\ncommon blocks. We then optimize and choose the best implementation for each\nblock, determine the combinations of blocks that capture the entire literature,\nand propose novel block combinations. We qualitatively assess the quality of\nthe schemes based on the skyline of efficiency and utility, i.e., based on\nwhether a method is dominated on both aspects or not. Using exhaustive\nexperiments on four real datasets with different characteristics, we conclude\nthat there are always trade-offs in terms of utility and efficiency. We\ndemonstrate that the schemes derived from our novel block combinations provide\nthe best trade-offs for time critical applications. Our work can serve as a\nguide to help practitioners engineer a differentially private histogram scheme\ndepending on their application requirements.\n"]},
{"authors": ["Mihal Miu", "Xiaokun Zhang", "M. Ali Akber Dewan", "Junye Wang"], "title": ["Aggregation and visualization of spatial data with application to\n  classification of land use and land cover"], "date": ["2017-04-19T18:01:29Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1704.05860v1"], "summary": ["  Aggregation and visualization of geographical data are an important part of\nenvironmental data mining, environmental modelling, and agricultural\nmanagement. However, it is difficult to aggregate geospatial data of the\nvarious formats, such as maps, census and survey data. This paper presents a\nframework named PlaniSphere, which can aggregate the various geospatial\ndatasets, and synthesizes raw data. We developed an algorithm in PlaniSphere to\naggregate remote sensing images with census data for classification and\nvisualization of land use and land cover (LULC). The results show that the\nframework is able to classify geospatial data sets of LULC from multiple\nformats. National census data sets can be used for calibration of remote\nsensing LULC classifications. This provides a new approach for the\nclassification of remote sensing data. This approach proposed in this paper\nshould be useful for LULC classification in environmental spatial analysis.\n"]},
{"authors": ["Cigdem Aslay", "Francesco Bonchi", "Laks V. S. Lakshmanan", "Wei Lu"], "title": ["Revenue Maximization in Incentivized Social Advertising"], "date": ["2016-12-02T01:11:12Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1612.00531v5"], "summary": ["  Incentivized social advertising, an emerging marketing model, provides\nmonetization opportunities not only to the owners of the social networking\nplatforms but also to their influential users by offering a \"cut\" on the\nadvertising revenue. We consider a social network (the host) that sells\nad-engagements to advertisers by inserting their ads, in the form of promoted\nposts, into the feeds of carefully selected \"initial endorsers\" or seed users:\nthese users receive monetary incentives in exchange for their endorsements. The\nendorsements help propagate the ads to the feeds of their followers. In this\ncontext, the problem for the host is is to allocate ads to influential users,\ntaking into account the propensity of ads for viral propagation, and carefully\napportioning the monetary budget of each of the advertisers between incentives\nto influential users and ad-engagement costs, with the rational goal of\nmaximizing its own revenue. We consider a monetary incentive for the\ninfluential users, which is proportional to their influence potential. We show\nthat revenue maximization in incentivized social advertising corresponds to the\nproblem of monotone submodular function maximization, subject to a partition\nmatroid constraint on the ads-to-seeds allocation, and submodular knapsack\nconstraints on the advertisers' budgets. This problem is NP-hard and we devise\n2 greedy algorithms with provable approximation guarantees, which differ in\ntheir sensitivity to seed user incentive costs. Our approximation algorithms\nrequire repeatedly estimating the expected marginal gain in revenue as well as\nin advertiser payment. By exploiting a connection to the recent advances made\nin scalable estimation of expected influence spread, we devise efficient and\nscalable versions of the greedy algorithms.\n"]},
{"authors": ["Sanjeev Shenoy", "Tsung-Ting Kuo", "Rodney Gabriel", "Julian McAuley", "Chun-Nan Hsu"], "title": ["Deduplication in a massive clinical note dataset"], "date": ["2017-04-19T05:33:21Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1704.05617v1"], "summary": ["  Duplication, whether exact or partial, is a common issue in many datasets. In\nclinical notes data, duplication (and near duplication) can arise for many\nreasons, such as the pervasive use of templates, copy-pasting, or notes being\ngenerated by automated procedures. A key challenge in removing such near\nduplicates is the size of such datasets; our own dataset consists of more than\n10 million notes. To detect and correct such duplicates requires algorithms\nthat both accurate and highly scalable. We describe a solution based on\nMinhashing with Locality Sensitive Hashing. In this paper, we present the\ntheory behind this method and present a database-inspired approach to make the\nmethod scalable. We also present a clustering technique using disjoint sets to\nproduce dense clusters, which speeds up our algorithm.\n"]},
{"authors": ["Pablo Barcelo", "Gerald Berger", "Andreas Pieris"], "title": ["Containment for Rule-Based Ontology-Mediated Queries"], "date": ["2017-03-23T10:44:18Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.07994v3"], "summary": ["  Many efforts have been dedicated to identifying restrictions on ontologies\nexpressed as tuple-generating dependencies (tgds), a.k.a. existential rules,\nthat lead to the decidability for the problem of answering ontology-mediated\nqueries (OMQs). This has given rise to three families of formalisms: guarded,\nnon-recursive, and sticky sets of tgds. In this work, we study the containment\nproblem for OMQs expressed in such formalisms, which is a key ingredient for\nsolving static analysis tasks associated with them. Our main contribution is\nthe development of specially tailored techniques for OMQ containment under the\nclasses of tgds stated above. This enables us to obtain sharp complexity bounds\nfor the problems at hand, which in turn allow us to delimitate its practical\napplicability. We also apply our techniques to pinpoint the complexity of\nproblems associated with two emerging applications of OMQ containment:\ndistribution over components and UCQ rewritability of OMQs.\n"]},
{"authors": ["M Martinez Pedreira", "C Grigoras for the ALICE Collaboration"], "title": ["Scalable Global Grid catalogue for LHC Run3 and beyond"], "date": ["2017-04-18T11:18:21Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1704.05272v1"], "summary": ["  The AliEn (ALICE Environment) file catalogue is a global unique namespace\nproviding mapping between a UNIX-like logical name structure and the\ncorresponding physical files distributed over 80 storage elements worldwide.\nPowerful search tools and hierarchical metadata information are integral parts\nof the system and are used by the Grid jobs as well as local users to store and\naccess all files on the Grid storage elements. The catalogue has been in\nproduction since 2005 and over the past 11 years has grown to more than 2\nbillion logical file names. The backend is a set of distributed relational\ndatabases, ensuring smooth growth and fast access. Due to the anticipated fast\nfuture growth, we are looking for ways to enhance the performance and\nscalability by simplifying the catalogue schema while keeping the functionality\nintact. We investigated different backend solutions, such as distributed key\nvalue stores, as replacement for the relational database. This contribution\ncovers the architectural changes in the system, together with the technology\nevaluation, benchmark results and conclusions.\n"]},
{"authors": ["Arkan A. G. Al-Hamodi", "Songfeng Lu"], "title": ["A novel approach for fast mining frequent itemsets use N-list structure\n  based on MapReduce"], "date": ["2017-04-15T07:23:40Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1704.04599v1"], "summary": ["  Frequent Pattern Mining is a one field of the most significant topics in data\nmining. In recent years, many algorithms have been proposed for mining frequent\nitemsets. A new algorithm has been presented for mining frequent itemsets based\non N-list data structure called Prepost algorithm. The Prepost algorithm is\nenhanced by implementing compact PPC-tree with the general tree. Prepost\nalgorithm can only find a frequent itemsets with required (pre-order and\npost-order) for each node. In this chapter, we improved prepost algorithm based\non Hadoop platform (HPrepost), proposed using the Mapreduce programming model.\nThe main goals of proposed method are efficient mining frequent itemsets\nrequiring less running time and memory usage. We have conduct experiments for\nthe proposed scheme to compare with another algorithms. With dense datasets,\nwhich have a large average length of transactions, HPrepost is more effective\nthan frequent itemsets algorithms in terms of execution time and memory usage\nfor all min-sup. Generally, our algorithm outperforms algorithms in terms of\nruntime and memory usage with small thresholds and large datasets.\n"]},
{"authors": ["Yanhong A. Liu", "Scott D. Stoller"], "title": ["Founded Semantics and Constraint Semantics of Logic Rules"], "date": ["2016-06-20T19:48:20Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1606.06269v3"], "summary": ["  This paper describes a simple new semantics for logic rules, founded\nsemantics, and its straightforward extension to another simple new semantics,\nconstraint semantics. The new semantics support unrestricted negation, as well\nas unrestricted existential and universal quantifications. They are uniquely\nexpressive and intuitive by allowing assumptions about the predicates and rules\nto be specified explicitly. They are completely declarative and easy to\nunderstand and relate cleanly to prior semantics. In addition, founded\nsemantics can be computed in linear time in the size of the ground program.\n"]},
{"authors": ["Nhien-An Le-Khac", "M-Tahar Kechadi"], "title": ["On a Distributed Approach for Density-based Clustering"], "date": ["2017-04-13T23:34:43Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1704.04302v1"], "summary": ["  Efficient extraction of useful knowledge from these data is still a\nchallenge, mainly when the data is distributed, heterogeneous and of different\nquality depending on its corresponding local infrastructure. To reduce the\noverhead cost, most of the existing distributed clustering approaches generate\nglobal models by aggregating local results obtained on each individual node.\nThe complexity and quality of solutions depend highly on the quality of the\naggregation. In this respect, we proposed for distributed density-based\nclustering that both reduces the communication overheads due to the data\nexchange and improves the quality of the global models by considering the\nshapes of local clusters. From preliminary results we show that this algorithm\nis very promising.\n"]},
{"authors": ["Nhien-An Le-Khac", "Sammer Markos", "M-Tahar Kechadi"], "title": ["A Tree-based Approach for Detecting Redundant Business Rules in very\n  Large Financial Datasets"], "date": ["2017-04-13T23:26:26Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1704.04301v1"], "summary": ["  Net Asset Value (NAV) calculation and validation is the principle task of a\nfund administrator. If the NAV of a fund is calculated incorrectly then there\nis huge impact on the fund administrator; such as monetary compensation,\nreputational loss, or loss of business. In general, these companies use the\nsame methodology to calculate the NAV of a fund, however the type of fund in\nquestion dictates the set of business rules used to validate this. Today, most\nFund Administrators depend heavily on human resources due to the lack of an\nautomated standardized solutions, however due to economic climate and the need\nfor efficiency and costs reduction many banks are now looking for an automated\nsolution with minimal human interaction; i.e., straight through processing\n(STP). Within the scope of a collaboration project that focuses on building an\noptimal solution for NAV validation, in this paper, we will present a new\napproach for detecting correlated business rules. We also show how we evaluate\nthis approach using real-world financial data.\n"]},
{"authors": ["Dylan Hutchison", "Bill Howe", "Dan Suciu"], "title": ["LaraDB: A Minimalist Kernel for Linear and Relational Algebra\n  Computation"], "date": ["2017-03-21T17:56:47Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.07342v3"], "summary": ["  Analytics tasks manipulate structured data with variants of relational\nalgebra (RA) and quantitative data with variants of linear algebra (LA). The\ntwo computational models have overlapping expressiveness, motivating a common\nprogramming model that affords unified reasoning and algorithm design. At the\nlogical level we propose Lara, a lean algebra of three operators, that\nexpresses RA and LA as well as relevant optimization rules. We show a series of\nproofs that position Lara %formal and informal at just the right level of\nexpressiveness for a middleware algebra: more explicit than MapReduce but more\ngeneral than RA or LA. At the physical level we find that the Lara operators\nafford efficient implementations using a single primitive that is available in\na variety of backend engines: range scans over partitioned sorted maps.\n  To evaluate these ideas, we implemented the Lara operators as range iterators\nin Apache Accumulo, a popular implementation of Google's BigTable. First we\nshow how Lara expresses a sensor quality control task, and we measure the\nperformance impact of optimizations Lara admits on this task. Second we show\nthat the LaraDB implementation outperforms Accumulo's native MapReduce\nintegration on a core task involving join and aggregation in the form of matrix\nmultiply, especially at smaller scales that are typically a poor fit for\nscale-out approaches. We find that LaraDB offers a conceptually lean framework\nfor optimizing mixed-abstraction analytics tasks, without giving up fast\nrecord-level updates and scans.\n"]},
{"authors": ["Sheng Wang", "Zhifeng Bao", "J. Shane Culpepper", "Timos Sellis", "Gao Cong"], "title": ["Reverse k Nearest Neighbor Search over Trajectories"], "date": ["2017-04-13T03:14:00Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1704.03978v1"], "summary": ["  GPS enables mobile devices to continuously provide new opportunities to\nimprove our daily lives. For example, the data collected in applications\ncreated by Uber or Public Transport Authorities can be used to plan\ntransportation routes, estimate capacities, and proactively identify low\ncoverage areas. In this paper, we study a new kind of query-Reverse k Nearest\nNeighbor Search over Trajectories (RkNNT), which can be used for route planning\nand capacity estimation. Given a set of existing routes DR, a set of passenger\ntransitions DT, and a query route Q, a RkNNT query returns all transitions that\ntake Q as one of its k nearest travel routes. To solve the problem, we first\ndevelop an index to handle dynamic trajectory updates, so that the most\nup-to-date transition data are available for answering a RkNNT query. Then we\nintroduce a filter refinement framework for processing RkNNT queries using the\nproposed indexes. Next, we show how to use RkNNT to solve the optimal route\nplanning problem MaxRkNNT (MinRkNNT), which is to search for the optimal route\nfrom a start location to an end location that could attract the maximum (or\nminimum) number of passengers based on a pre-defined travel distance threshold.\nExperiments on real datasets demonstrate the efficiency and scalability of our\napproaches. To the best of our best knowledge, this is the first work to study\nthe RkNNT problem for route planning.\n"]},
{"authors": ["Shubhadip Mitra", "Priya Saraf", "Richa Sharma", "Arnab Bhattacharya", "Harsh Bhandari", "Sayan Ranu"], "title": ["NetClus: A Scalable Framework for Locating Top-K Sites for Placement of\n  Trajectory-Aware Services"], "date": ["2017-02-09T12:42:14Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.02809v2"], "summary": ["  Facility location queries identify the best locations to set up new\nfacilities for providing service to its users. Majority of the existing works\nin this space assume that the user locations are static. Such limitations are\ntoo restrictive for planning many modern real-life services such as fuel\nstations, ATMs, convenience stores, cellphone base-stations, etc. that are\nwidely accessed by mobile users. The placement of such services should,\ntherefore, factor in the mobility patterns or trajectories of the users rather\nthan simply their static locations. In this work, we introduce the TOPS\n(Trajectory-Aware Optimal Placement of Services) query that locates the best k\nsites on a road network. The aim is to optimize a wide class of objective\nfunctions defined over the user trajectories. We show that the problem is\nNP-hard and even the greedy heuristic with an approximation bound of (1-1/e)\nfails to scale on urban-scale datasets. To overcome this challenge, we develop\na multi-resolution clustering based indexing framework called NetClus.\nEmpirical studies on real road network trajectory datasets show that NetClus\noffers solutions that are comparable in terms of quality with those of the\ngreedy heuristic, while having practical response times and low memory\nfootprints. Additionally, the NetClus framework can absorb dynamic updates in\nmobility patterns, handle constraints such as site-costs and capacity, and\nexisting services, thereby providing an effective solution for modern\nurban-scale scenarios.\n"]},
{"authors": ["Yves van Gennip", "Blake Hunter", "Anna Ma", "Daniel Moyer", "Ryan de Vera", "Andrea L. Bertozzi"], "title": ["Unsupervised record matching with noisy and incomplete data"], "date": ["2017-04-10T17:05:40Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1704.02955v1"], "summary": ["  We consider the problem of duplicate detection: given a large data set in\nwhich each entry has multiple attributes, detect which distinct entries refer\nto the same real world entity. Our method consists of three main steps:\ncreating a similarity score between entries, grouping entries together into\n`unique entities', and refining the groups. We compare various methods for\ncreating similarity scores, considering different combinations of string\nmatching, term frequency-inverse document frequency methods, and n-gram\ntechniques. In particular, we introduce a vectorized soft term\nfrequency-inverse document frequency method, with an optional refinement step.\n  We test our method on the Los Angeles Police Department Field Interview Card\ndata set, the Cora Citation Matching data set, and two sets of restaurant\nreview data. The results show that in certain parameter ranges soft term\nfrequency-inverse document frequency methods can outperform the standard term\nfrequency-inverse document frequency method; they also confirm that our method\nfor automatically determining the number of groups typically works well in many\ncases and allows for accurate results in the absence of a priori knowledge of\nthe number of unique entities in the data set.\n"]},
{"authors": ["Hamed Nilforoshan", "Jiannan Wang", "Eugene Wu"], "title": ["PreCog: Improving Crowdsourced Data Quality Before Acquisition"], "date": ["2017-04-07T21:53:13Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1704.02384v1"], "summary": ["  Quality control in crowdsourcing systems is crucial. It is typically done\nafter data collection, often using additional crowdsourced tasks to assess and\nimprove the quality. These post-hoc methods can easily add cost and latency to\nthe acquisition process--particularly if collecting high-quality data is\nimportant. In this paper, we argue for pre-hoc interface optimizations based on\nfeedback that helps workers improve data quality before it is submitted and is\nwell suited to complement post-hoc techniques. We propose the Precog system\nthat explicitly supports such interface optimizations for common integrity\nconstraints as well as more ambiguous text acquisition tasks where quality is\nill-defined. We then develop the Segment-Predict-Explain pattern for detecting\nlow-quality text segments and generating prescriptive explanations to help the\nworker improve their text input. Our unique combination of segmentation and\nprescriptive explanation are necessary for Precog to collect 2x more\nhigh-quality text data than non-Precog approaches on two real domains.\n"]},
{"authors": ["Antonio Badia", "Daniel Lemire"], "title": ["On Desirable Semantics of Functional Dependencies over Databases with\n  Incomplete Information"], "date": ["2017-03-23T18:29:03Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.08198v2"], "summary": ["  Codd's relational model describes just one possible world. To better cope\nwith incomplete information, extended database models allow several possible\nworlds. Vague tables are one such convenient extended model where attributes\naccept sets of possible values (e.g., the manager is either Jill or Bob).\nHowever, conceptual database design in such cases remains an open problem. In\nparticular, there is no canonical definition of functional dependencies (FDs)\nover possible worlds (e.g., each employee has just one manager). We identify\nseveral desirable properties that the semantics of such FDs should meet\nincluding Armstrong's axioms, the independence from irrelevant attributes,\nseamless satisfaction and implied by strong satisfaction. We show that we can\ndefine FDs such that they have all our desirable properties over vague tables.\nHowever, we also show that no notion of FD can satisfy all our desirable\nproperties over a more general model (disjunctive tables). Our work formalizes\na trade-off between having a general model and having well-behaved FDs.\n"]},
{"authors": ["Christos Kalyvas", "Theodoros Tzouramanis"], "title": ["A Survey of Skyline Query Processing"], "date": ["2017-04-06T11:34:20Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1704.01788v1"], "summary": ["  Living in the Information Age allows almost everyone have access to a large\namount of information and options to choose from in order to fulfill their\nneeds. In many cases, the amount of information available and the rate of\nchange may hide the optimal and truly desired solution. This reveals the need\nof a mechanism that will highlight the best options to choose among every\npossible scenario. Based on this the skyline query was proposed which is a\ndecision support mechanism, that retrieves the valuefor- money options of a\ndataset by identifying the objects that present the optimal combination of the\ncharacteristics of the dataset. This paper surveys the state-of-the-art\ntechniques for skyline query processing, the numerous variations of the initial\nalgorithm that were proposed to solve similar problems and the\napplication-specific approaches that were developed to provide a solution\nefficiently in each case. Aditionally in each section a taxonomy is outlined\nalong with the key aspects of each algorithm and its relation to previous\nstudies.\n"]},
{"authors": ["Xuan Zhou", "Xin Zhou", "Zhengtai Yu", "Kian-Lee Tan"], "title": ["Posterior Snapshot Isolation"], "date": ["2017-04-05T10:30:18Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1704.01355v1"], "summary": ["  Snapshot Isolation (SI) is a widely adopted concurrency control mechanism in\ndatabase systems, which utilizes timestamps to resolve conflicts between\ntransactions. However, centralized allocation of timestamps is a potential\nbottleneck for parallel transaction management. This bottleneck is becoming\nincreasingly visible with the rapidly growing degree of parallelism of today's\ncomputing platforms. This paper introduces Posterior Snapshot Isolation\n(PostSI), an SI mechanism that allows transactions to determine their\ntimestamps autonomously, without relying on centralized coordination. As such,\nPostSI can scale well, rendering it suitable for various multi-core and MPP\nplatforms. Extensive experiments are conducted to demonstrate its advantage\nover existing approaches.\n"]},
{"authors": ["Jonas Schneider"], "title": ["Analytic Performance Model of a Main-Memory Index Structure"], "date": ["2016-09-05T20:40:57Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1609.01319v2"], "summary": ["  Efficient evaluation of multi-dimensional range queries in a main-memory\ndatabase is an important, but difficult task. State-of-the-art techniques rely\non optimised sequential scans or tree-based structures. For range queries with\nsmall result sets, sequential scans exhibit poor asymptotic performance. Also,\nas the dimensionality of the data set increases, the performance of tree-based\nstructures degenerates due to the curse of dimensionality. Recent literature\nproposed the Elf, a main-memory structure that is optimised for the case of\nsuch multi-dimensional low-selectivity queries. The Elf outperforms other\nstate-of-the-art methods in manually tuned scenarios. However, choosing an\noptimal parameter configuration for the Elf is vital, since for poor\nconfigurations, the search performance degrades rapidly. Consequently, further\nknowledge about the behaviour of the Elf in different configurations is\nrequired to achieve robust performance. In this thesis, we therefore propose a\nnumerical cost model for the Elf. Like all main-memory index structures, the\nElf response time is not dominated by disk accesses, refusing a straightforward\nanalysis. Our model predicts the size and shape of the Elf region that is\nexamined during search. We propose that the response time of a search is linear\nto the size of this region. Furthermore, we study the impact of skewed data\ndistributions and correlations on the shape of the Elf. We find that they lead\nto behaviour that is accurately describable through simple reductions in\nattribute cardinality. Our experimental results indicate that for data sets of\nup to 15 dimensions, our cost model predicts the size of the examined Elf\nregion with relative errors below 5%. Furthermore, we find that the size of the\nElf region examined during search predicts the response time with an accuracy\nof 80%.\n"]},
{"authors": ["Thomas Zeume", "Thomas Schwentick"], "title": ["Dynamic Conjunctive Queries"], "date": ["2017-04-05T06:59:15Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1704.01286v1"], "summary": ["  The article investigates classes of queries maintainable by conjunctive\nqueries (CQs) and their extensions and restrictions in the dynamic complexity\nframework of Patnaik and Immerman. Starting from the basic language of\nquantifier-free conjunctions of positive atoms, it studies the impact of\nadditional operators and features - such as union, atomic negation and\nquantification - on the dynamic expressiveness, for the standard semantics as\nwell as for Delta-semantics.\n  Although many different combinations of these features are possible, they\nbasically yield five important fragments for the standard semantics,\ncharacterized by the addition of (1) arbitrary quantification and atomic\nnegation, (2) existential quantification and atomic negation, (3) existential\nquantification, (4) atomic negation (but no quantification), and by (5) no\naddition to the basic language at all. While fragments (3), (4) and (5) can be\nseparated, it remains open whether fragments (1), (2) and (3) are actually\ndifferent. The fragments arising from Delta-semantics are also subsumed by the\nstandard fragments (1), (2) and (4). The main fragments of DynFO that had been\nstudied in previous work, DynQF and DynProp, characterized by quantifier-free\nupdate programs with or without auxiliary functions, respectively, also fit\ninto this hierarchy: DynProp coincides with fragment (4) and DynQF is strictly\nabove fragment (4) and within fragment (3).\n  As a further result, all (statically) FO-definable queries are captured by\nfragment (2) and a complete characterization of these queries in terms of\nnon-recursive dynamic programs with existential update formulas with one\nexistential quantifier is given.\n"]},
{"authors": ["Stephen Smart", "Christan Grant"], "title": ["Storing complex data sharing policies with the Min Mask Sketch"], "date": ["2017-04-04T23:29:54Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1704.01218v1"], "summary": ["  More data is currently being collected and shared by software applications\nthan ever before. In many cases, the user is asked if either all or none of\ntheir data can be shared. We hypothesize that in some cases, users would like\nto share data in more complex ways. In order to implement the sharing of data\nusing more complicated privacy preferences, complex data sharing policies must\nbe used. These complex sharing policies require more space to store than a\nsimple \"all or nothing\" approach to data sharing. In this paper, we present a\nnew probabilistic data structure, called the Min Mask Sketch, to efficiently\nstore these complex data sharing policies. We describe an implementation for\nthe Min Mask Sketch in PostgreSQL and analyze the practicality and feasibility\nof using a probabilistic data structure for storing complex data sharing\npolicies.\n"]},
{"authors": ["Feras Saad", "Leonardo Casarsa", "Vikash Mansinghka"], "title": ["Probabilistic Search for Structured Data via Probabilistic Programming\n  and Nonparametric Bayes"], "date": ["2017-04-04T16:18:07Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1704.01087v1"], "summary": ["  Databases are widespread, yet extracting relevant data can be difficult.\nWithout substantial domain knowledge, multivariate search queries often return\nsparse or uninformative results. This paper introduces an approach for\nsearching structured data based on probabilistic programming and nonparametric\nBayes. Users specify queries in a probabilistic language that combines standard\nSQL database search operators with an information theoretic ranking function\ncalled predictive relevance. Predictive relevance can be calculated by a fast\nsparse matrix algorithm based on posterior samples from CrossCat, a\nnonparametric Bayesian model for high-dimensional, heterogeneously-typed data\ntables. The result is a flexible search technique that applies to a broad class\nof information retrieval problems, which we integrate into BayesDB, a\nprobabilistic programming platform for probabilistic data analysis. This paper\ndemonstrates applications to databases of US colleges, global macroeconomic\nindicators of public health, and classic cars. We found that human evaluators\noften prefer the results from probabilistic search to results from a standard\nbaseline.\n"]},
{"authors": ["Andrei Lebedev", "JooYoung Lee", "Victor Rivera", "Manuel Mazzara"], "title": ["Link Prediction using Top-$k$ Shortest Distances"], "date": ["2017-04-04T14:24:56Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1705.02936v1"], "summary": ["  In this paper, we apply an efficient top-$k$ shortest distance routing\nalgorithm to the link prediction problem and test its efficacy. We compare the\nresults with other base line and state-of-the-art methods as well as with the\nshortest path. Our results show that using top-$k$ distances as a similarity\nmeasure outperforms classical similarity measures such as Jaccard and\nAdamic/Adar.\n"]},
{"authors": ["Arnau Prat-P\u00e9rez", "Joan Guisado-G\u00e1mez", "Xavier Fern\u00e1ndez Salas", "Petr Koupy", "Siegfried Depner", "Davide Basilio Bartolini"], "title": ["Towards a property graph generator for benchmarking"], "date": ["2017-04-03T15:04:01Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1704.00630v1"], "summary": ["  The use of synthetic graph generators is a common practice among\ngraph-oriented benchmark designers, as it allows obtaining graphs with the\nrequired scale and characteristics. However, finding a graph generator that\naccurately fits the needs of a given benchmark is very difficult, thus\npractitioners end up creating ad-hoc ones. Such a task is usually\ntime-consuming, and often leads to reinventing the wheel. In this paper, we\nintroduce the conceptual design of DataSynth, a framework for property graphs\ngeneration with customizable schemas and characteristics. The goal of DataSynth\nis to assist benchmark designers in generating graphs efficiently and at scale,\nsaving from implementing their own generators. Additionally, DataSynth\nintroduces novel features barely explored so far, such as modeling the\ncorrelation between properties and the structure of the graph. This is achieved\nby a novel property-to-node matching algorithm for which we present preliminary\npromising results.\n"]},
{"authors": ["Carl Camilleri", "Joseph Vella", "Vitezslav Nezval"], "title": ["Thespis: Actor-Based Middleware for Causal Consistency"], "date": ["2017-03-25T12:48:55Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.08685v2"], "summary": ["  This paper provides a survey of the current state of the art in\nCausally-Consistent data stores. Furthermore, we present the design of Thespis,\na middleware that innovatively leverages the Actor model to implement causal\nconsistency over an industry-standard data store.\n"]},
{"authors": ["Hasan M. Jamil"], "title": ["Knowledge Rich Natural Language Queries over Structured Biological\n  Databases"], "date": ["2017-03-30T21:37:14Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.10692v1"], "summary": ["  Increasingly, keyword, natural language and NoSQL queries are being used for\ninformation retrieval from traditional as well as non-traditional databases\nsuch as web, document, image, GIS, legal, and health databases. While their\npopularity are undeniable for obvious reasons, their engineering is far from\nsimple. In most part, semantics and intent preserving mapping of a well\nunderstood natural language query expressed over a structured database schema\nto a structured query language is still a difficult task, and research to tame\nthe complexity is intense. In this paper, we propose a multi-level\nknowledge-based middleware to facilitate such mappings that separate the\nconceptual level from the physical level. We augment these multi-level\nabstractions with a concept reasoner and a query strategy engine to dynamically\nlink arbitrary natural language querying to well defined structured queries. We\ndemonstrate the feasibility of our approach by presenting a Datalog based\nprototype system, called BioSmart, that can compute responses to arbitrary\nnatural language queries over arbitrary databases once a syntactic\nclassification of the natural language query is made.\n"]},
{"authors": ["Dominik D. Freydenberger", "Benny Kimelfeld", "Liat Peterfreund"], "title": ["Joining Extractions of Regular Expressions"], "date": ["2017-03-30T08:27:11Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.10350v1"], "summary": ["  Regular expressions with capture variables, also known as \"regex formulas,\"\nextract relations of spans (interval positions) from text. These relations can\nbe further manipulated via Relational Algebra as studied in the context of\ndocument spanners, Fagin et al.'s formal framework for information extraction.\nWe investigate the complexity of querying text by Conjunctive Queries (CQs) and\nUnions of CQs (UCQs) on top of regex formulas. We show that the lower bounds\n(NP-completeness and W[1]-hardness) from the relational world also hold in our\nsetting; in particular, hardness hits already single-character text! Yet, the\nupper bounds from the relational world do not carry over. Unlike the relational\nworld, acyclic CQs, and even gamma-acyclic CQs, are hard to compute. The source\nof hardness is that it may be intractable to instantiate the relation defined\nby a regex formula, simply because it has an exponential number of tuples. Yet,\nwe are able to establish general upper bounds. In particular, UCQs can be\nevaluated with polynomial delay, provided that every CQ has a bounded number of\natoms (while unions and projection can be arbitrary). Furthermore, UCQ\nevaluation is solvable with FPT (Fixed-Parameter Tractable) delay when the\nparameter is the size of the UCQ.\n"]},
{"authors": ["Nhien-An Le-Khac", "Martin Bue", "Michael Whelan", "Tahar Kechadi"], "title": ["A clustering-based data reduction for very large spatio-temporal\n  datasets"], "date": ["2016-09-04T20:35:18Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1609.00988v2"], "summary": ["  Today, huge amounts of data are being collected with spatial and temporal\ncomponents from sources such as meteorological, satellite imagery etc.\nEfficient visualisation as well as discovery of useful knowledge from these\ndatasets is therefore very challenging and becoming a massive economic need.\nData Mining has emerged as the technology to discover hidden knowledge in very\nlarge amounts of data. Furthermore, data mining techniques could be applied to\ndecrease the large size of raw data by retrieving its useful knowledge as\nrepresentatives. As a consequence, instead of dealing with a large size of raw\ndata, we can use these representatives to visualise or to analyse without\nlosing important information. This paper presents a new approach based on\ndifferent clustering techniques for data reduction to help analyse very large\nspatio-temporal data. We also present and discuss preliminary results of this\napproach.\n"]},
{"authors": ["Lamine M. Aouad", "Nhien-An Le-Khac", "Tahar Kechadi"], "title": ["Variance-based Clustering Technique for Distributed Data Mining\n  Applications"], "date": ["2017-03-28T21:59:33Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.09823v1"], "summary": ["  Nowadays, huge amounts of data are naturally collected in distributed sites\ndue to different facts and moving these data through the network for extracting\nuseful knowledge is almost unfeasible for either technical reasons or policies.\nFurthermore, classical par- allel algorithms cannot be applied, specially in\nloosely coupled environments. This requires to develop scalable distributed\nalgorithms able to return the global knowledge by aggregating local results in\nan effective way. In this paper we propose a distributed algorithm based on\nindependent local clustering processes and a global merging based on minimum\nvariance increases and requires a limited communication overhead. We also\nintroduce the notion of distributed sub-clusters perturbation to improve the\nglobal generated distribution. We show that this algorithm improves the quality\nof clustering compared to classical local centralized ones and is able to find\nreal global data nature or distribution.\n"]},
{"authors": ["Yongjoo Park", "Ahmad Shahab Tajik", "Michael Cafarella", "Barzan Mozafari"], "title": ["Database Learning: Toward a Database that Becomes Smarter Every Time"], "date": ["2017-03-16T03:36:28Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.05468v2"], "summary": ["  In today's databases, previous query answers rarely benefit answering future\nqueries. For the first time, to the best of our knowledge, we change this\nparadigm in an approximate query processing (AQP) context. We make the\nfollowing observation: the answer to each query reveals some degree of\nknowledge about the answer to another query because their answers stem from the\nsame underlying distribution that has produced the entire dataset. Exploiting\nand refining this knowledge should allow us to answer queries more\nanalytically, rather than by reading enormous amounts of raw data. Also,\nprocessing more queries should continuously enhance our knowledge of the\nunderlying distribution, and hence lead to increasingly faster response times\nfor future queries.\n  We call this novel idea---learning from past query answers---Database\nLearning. We exploit the principle of maximum entropy to produce answers, which\nare in expectation guaranteed to be more accurate than existing sample-based\napproximations. Empowered by this idea, we build a query engine on top of Spark\nSQL, called Verdict. We conduct extensive experiments on real-world query\ntraces from a large customer of a major database vendor. Our results\ndemonstrate that Verdict supports 73.7% of these queries, speeding them up by\nup to 23.0x for the same accuracy level compared to existing AQP systems.\n"]},
{"authors": ["Nhien-An Le-Khac", "Sammer Markos", "Michael O'Neill", "Anthony Brabazon", "Tahar Kechadi"], "title": ["An efficient Search Tool for an Anti-Money Laundering Application of an\n  Multi-national Bank's Dataset"], "date": ["2016-09-04T20:17:45Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1609.02031v2"], "summary": ["  Today, money laundering (ML) poses a serious threat not only to financial\ninstitutions but also to the nations. This criminal activity is becoming more\nand more sophisticated and seems to have moved from the clichy of drug\ntrafficking to financing terrorism and surely not forgetting personal gain.\nMost of the financial institutions internationally have been implementing\nanti-money laundering solutions (AML) to fight investment fraud activities. In\nAML, the customer identification is an important task which helps AML experts\nto monitor customer habits: some being customer domicile, transactions that\nthey are involved in etc. However, simple query tools provided by current DBMS\nas well as naive approaches in customer searching may produce incorrect and\nambiguous results and their processing time is also very high due to the\ncomplexity of the database system architecture. In this paper, we present a new\napproach for identifying customers registered in an investment bank. This\napproach is developed as a tool that allows AML experts to quickly identify\ncustomers who are managed independently across separate databases. It is tested\non real-world datasets, which are real and large financial datasets. Some\npreliminary experimental results show that this new approach is efficient and\neffective.\n"]},
{"authors": ["Lamine M. Aouad", "Nhien-An Le-Khac", "Tahar Kechadi"], "title": ["Grid-based Approaches for Distributed Data Mining Applications"], "date": ["2017-03-28T21:19:24Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.09807v1"], "summary": ["  The data mining field is an important source of large-scale applications and\ndatasets which are getting more and more common. In this paper, we present\ngrid-based approaches for two basic data mining applications, and a performance\nevaluation on an experimental grid environment that provides interesting\nmonitoring capabilities and configuration tools. We propose a new distributed\nclustering approach and a distributed frequent itemsets generation well-adapted\nfor grid environments. Performance evaluation is done using the Condor system\nand its workflow manager DAGMan. We also compare this performance analysis to a\nsimple analytical model to evaluate the overheads related to the workflow\nengine and the underlying grid system. This will specifically show that\nrealistic performance expectations are currently difficult to achieve on the\ngrid.\n"]},
{"authors": ["Witold Litwin"], "title": ["Stored and Inherited Relations"], "date": ["2017-03-28T13:52:03Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.09574v1"], "summary": ["  The universally applied Codd's relational model has two constructs: a stored\nrelation, with stored attributes only and a view, only with the inherited ones.\nIn 1992, we have proposed third construct, mixing both types of attributes.\nExamples showed the idea attractive. No one followed however. We now revisit\nour proposal. We show that a relational database scheme using also our\nconstruct may be more faithful to reality. It may spare the logical navigation\nor complex value expressions to queries. It may also avoid auxiliary views,\noften necessary in practice at present. Better late than never, existing DBSs\nshould easily accommodate our proposal, with almost no storage and processing\noverhead.\n"]},
{"authors": ["Ninh Pham"], "title": ["Hybrid LSH: Faster Near Neighbors Reporting in High-dimensional Space"], "date": ["2016-07-21T03:24:06Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1607.06179v3"], "summary": ["  We study the $r$-near neighbors reporting problem ($r$-NN), i.e., reporting\n\\emph{all} points in a high-dimensional point set $S$ that lie within a radius\n$r$ of a given query point $q$. Our approach builds upon on the\nlocality-sensitive hashing (LSH) framework due to its appealing asymptotic\nsublinear query time for near neighbor search problems in high-dimensional\nspace. A bottleneck of the traditional LSH scheme for solving $r$-NN is that\nits performance is sensitive to data and query-dependent parameters. On\ndatasets whose data distributions have diverse local density patterns, LSH with\ninappropriate tuning parameters can sometimes be outperformed by a simple\nlinear search.\n  In this paper, we introduce a hybrid search strategy between LSH-based search\nand linear search for $r$-NN in high-dimensional space. By integrating an\nauxiliary data structure into LSH hash tables, we can efficiently estimate the\ncomputational cost of LSH-based search for a given query regardless of the data\ndistribution. This means that we are able to choose the appropriate search\nstrategy between LSH-based search and linear search to achieve better\nperformance. Moreover, the integrated data structure is time efficient and fits\nwell with many recent state-of-the-art LSH-based approaches. Our experiments on\nreal-world datasets show that the hybrid search approach outperforms (or is\ncomparable to) both LSH-based search and linear search for a wide range of\nsearch radii and data distributions in high-dimensional space.\n"]},
{"authors": ["Matteo Interlandi", "Letizia Tanca"], "title": ["On the CALM Principle for Bulk Synchronous Parallel Computation"], "date": ["2014-05-28T14:47:50Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1405.7264v3"], "summary": ["  In the recent years a lot of emphasis has been placed on two apparently\ndisjoined fields: data-parallel and eventually consistent distributed systems.\nIn this paper we propose a theoretical study over an eventually consistent\ndata-parallel computational model. The keystone is provided by the recent\nfinding that a class of programs exists which can be computed in an eventually\nconsistent, coordination-free way: monotonic programs. This principle is called\nCALM and has been proven for distributed asynchronous settings. We make the\ncase that, using the techniques developed by Ameloot et al., CALM does not hold\nin general for data-parallel systems, wherein computation usually proceeds\nsynchronously in rounds and where communication is reliable. We then show that\nusing novel techniques subsuming the one of Ameloot et al., the satisfiability\nof the CALM principle is directly related with the assumptions imposed on the\nbehavior of the system.\n"]},
{"authors": ["Zoi Kaoudi", "Jorge-Arnulfo Quian\u00e9-Ruiz", "Saravanan Thirumuruganathan", "Sanjay Chawla", "Divy Agrawal"], "title": ["A Cost-based Optimizer for Gradient Descent Optimization"], "date": ["2017-03-27T17:24:54Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.09193v1"], "summary": ["  As the use of machine learning (ML) permeates into diverse application\ndomains, there is an urgent need to support a declarative framework for ML.\nIdeally, a user will specify an ML task in a high-level and easy-to-use\nlanguage and the framework will invoke the appropriate algorithms and system\nconfigurations to execute it. An important observation towards designing such a\nframework is that many ML tasks can be expressed as mathematical optimization\nproblems, which take a specific form. Furthermore, these optimization problems\ncan be efficiently solved using variations of the gradient descent (GD)\nalgorithm. Thus, to decouple a user specification of an ML task from its\nexecution, a key component is a GD optimizer. We propose a cost-based GD\noptimizer that selects the best GD plan for a given ML task. To build our\noptimizer, we introduce a set of abstract operators for expressing GD\nalgorithms and propose a novel approach to estimate the number of iterations a\nGD algorithm requires to converge. Extensive experiments on real and synthetic\ndatasets show that our optimizer not only chooses the best GD plan but also\nallows for optimizations that achieve orders of magnitude performance speed-up.\n"]},
{"authors": ["Rada Chirkova", "Jon Doyle", "Juan L. Reutter"], "title": ["A Framework for Assessing Achievability of Data-Quality Constraints"], "date": ["2017-03-27T15:25:01Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.09141v1"], "summary": ["  Assessing and improving the quality of data are fundamental challenges for\ndata-intensive systems that have given rise to applications targeting\ntransformation and cleaning of data. However, while schema design, data\ncleaning, and data migration are now reasonably well understood in isolation,\nnot much attention has been given to the interplay between the tools addressing\nissues in these areas. We focus on the problem of determining whether the\navailable data-processing procedures can be used together to bring about the\ndesired quality of the given data. For instance, consider an organization\nintroducing new data-analysis tasks. Depending on the tasks, it may be a\npriority to determine whether the data can be processed and transformed using\nthe available data-processing tools to satisfy certain properties or quality\nassurances needed for the success of the task. Here, while the organization may\ncontrol some of its tools, some other tools may be external or proprietary,\nwith only basic information available on how they process data. The problem is\nthen, how to decide which tools to apply, and in which order, to make the data\nready for the new tasks?\n  Toward addressing this problem, we develop a new framework that abstracts\ndata-processing tools as black-box procedures with only some of the properties\nexposed, such as the applicability requirements, the parts of the data that the\nprocedure modifies, and the conditions that the data satisfy once the procedure\nhas been applied. We show how common tasks such as data cleaning and data\nmigration are encapsulated into our framework and, as a proof of concept, we\nstudy basic properties of the framework for the case of procedures described by\nstandard relational constraints. While reasoning in this framework may be\ncomputationally infeasible in general, we show that there exist well-behaved\nspecial cases with potential practical applications.\n"]},
{"authors": ["Farid Alborzi", "Surajit Chaudhuri", "Rada Chirkova", "Pallavi Deo", "Christopher Healey", "Gargi Pingale", "Juan Reutter", "Vaira Selvakani"], "title": ["DataSlicer: Task-Based Data Selection for Visual Data Exploration"], "date": ["2017-03-27T15:01:42Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.09218v1"], "summary": ["  In visual exploration and analysis of data, determining how to select and\ntransform the data for visualization is a challenge for data-unfamiliar or\ninexperienced users. Our main hypothesis is that for many data sets and common\nanalysis tasks, there are relatively few \"data slices\" that result in effective\nvisualizations. By focusing human users on appropriate and suitably transformed\nparts of the underlying data sets, these data slices can help the users carry\ntheir task to correct completion.\n  To verify this hypothesis, we develop a framework that permits us to capture\nexemplary data slices for a user task, and to explore and parse\nvisual-exploration sequences into a format that makes them distinct and easy to\ncompare. We develop a recommendation system, DataSlicer, that matches a\n\"currently viewed\" data slice with the most promising \"next effective\" data\nslices for the given exploration task. We report the results of controlled\nexperiments with an implementation of the DataSlicer system, using four common\nanalytical task types. The experiments demonstrate statistically significant\nimprovements in accuracy and exploration speed versus users without access to\nour system.\n"]},
{"authors": ["Thibault Sellam", "Martin Kersten"], "title": ["80 New Packages to Mine Database Query Logs"], "date": ["2017-03-25T19:00:23Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.08732v1"], "summary": ["  The query log of a DBMS is a powerful resource. It enables many practical\napplications, including query optimization and user experience enhancement. And\nyet, mining SQL queries is a difficult task. The fundamental problem is that\nqueries are symbolic objects, not vectors of numbers. Therefore, many popular\nstatistical concepts, such as means, regression, or decision trees do not\napply. Most authors limit themselves to ad hoc algorithms or approaches based\non neighborhoods, such as k Nearest Neighbors. Our project is to challenge this\nlimitation. We introduce methods to manipulate SQL queries as if they were\nvectors, thereby unlocking the whole statistical toolbox. We present three\nfamilies of methods: feature maps, kernel methods, and Bayesian models. The\nfirst technique directly encodes queries into vectors. The second one\ntransforms the queries implicitly. The last one exploits probabilistic\ngraphical models as an alternative to vector spaces. We present the benefits\nand drawbacks of each solution, highlight how they relate to each other, and\nmake the case for future investigation.\n"]},
{"authors": ["Dong Wen", "Lu Qin", "Xuemin Lin", "Ying Zhang", "Lijun Chang"], "title": ["Enumerating k-Vertex Connected Components in Large Graphs"], "date": ["2017-03-25T09:36:47Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.08668v1"], "summary": ["  Cohesive subgraph detection is an important graph problem that is widely\napplied in many application domains, such as social community detection,\nnetwork visualization, and network topology analysis. Most of existing cohesive\nsubgraph metrics can guarantee good structural properties but may cause the\nfree-rider effect. Here, by free-rider effect, we mean that some irrelevant\nsubgraphs are combined as one subgraph if they only share a small number of\nvertices and edges. In this paper, we study k-vertex connected component\n(k-VCC) which can effectively eliminate the free-rider effect but less studied\nin the literature. A k-VCC is a connected subgraph in which the removal of any\nk-1 vertices will not disconnect the subgraph. In addition to eliminating the\nfree-rider effect, k-VCC also has other advantages such as bounded diameter,\nhigh cohesiveness, bounded graph overlapping, and bounded subgraph number. We\npropose a polynomial time algorithm to enumerate all k-VCCs of a graph by\nrecursively partitioning the graph into overlapped subgraphs. We find that the\nkey to improving the algorithm is reducing the number of local connectivity\ntestings. Therefore, we propose two effective optimization strategies, namely\nneighbor sweep and group sweep, to largely reduce the number of local\nconnectivity testings. We conduct extensive performance studies using seven\nlarge real datasets to demonstrate the effectiveness of this model as well as\nthe efficiency of our proposed algorithms.\n"]},
{"authors": ["Peter Bailis", "Edward Gan", "Samuel Madden", "Deepak Narayanan", "Kexin Rong", "Sahaana Suri"], "title": ["MacroBase: Prioritizing Attention in Fast Data"], "date": ["2016-03-02T03:40:41Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1603.00567v4"], "summary": ["  As data volumes continue to rise, manual inspection is becoming increasingly\nuntenable. In response, we present MacroBase, a data analytics engine that\nprioritizes end-user attention in high-volume fast data streams. MacroBase\nenables efficient, accurate, and modular analyses that highlight and aggregate\nimportant and unusual behavior, acting as a search engine for fast data.\nMacroBase is able to deliver order-of-magnitude speedups over alternatives by\noptimizing the combination of explanation and classification tasks and by\nleveraging a new reservoir sampler and heavy-hitters sketch specialized for\nfast data streams. As a result, MacroBase delivers accurate results at speeds\nof up to 2M events per second per query on a single core. The system has\ndelivered meaningful results in production, including at a telematics company\nmonitoring hundreds of thousands of vehicles.\n"]},
{"authors": ["Charles A. Packer", "Lawrence B. Holder"], "title": ["GraphZip: Dictionary-based Compression for Mining Graph Streams"], "date": ["2017-03-24T22:08:00Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.08614v1"], "summary": ["  A massive amount of data generated today on platforms such as social\nnetworks, telecommunication networks, and the internet in general can be\nrepresented as graph streams. Activity in a network's underlying graph\ngenerates a sequence of edges in the form of a stream; for example, a social\nnetwork may generate a graph stream based on the interactions (edges) between\ndifferent users (nodes) over time. While many graph mining algorithms have\nalready been developed for analyzing relatively small graphs, graphs that begin\nto approach the size of real-world networks stress the limitations of such\nmethods due to their dynamic nature and the substantial number of nodes and\nconnections involved.\n  In this paper we present GraphZip, a scalable method for mining interesting\npatterns in graph streams. GraphZip is inspired by the Lempel-Ziv (LZ) class of\ncompression algorithms, and uses a novel dictionary-based compression approach\nin conjunction with the minimum description length principle to discover\nmaximally-compressing patterns in a graph stream. We experimentally show that\nGraphZip is able to retrieve complex and insightful patterns from large\nreal-world graphs and artificially-generated graphs with ground truth patterns.\nAdditionally, our results demonstrate that GraphZip is both highly efficient\nand highly effective compared to existing state-of-the-art methods for mining\ngraph streams.\n"]},
{"authors": ["Vineet John"], "title": ["Redynis: Traffic-aware dynamic repartitioning for a distributed\n  key-value store"], "date": ["2017-03-24T14:35:08Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.08425v1"], "summary": ["  Most modern data stores tend to be distributed, to enable the scaling of the\ndata across multiple instances of commodity hardware. Although this ensures a\nnear unlimited potential for storage, the data itself is not always ideally\npartitioned, and the cost of a network round-trip may cause a degradation of\nend-user experience with respect to response latency. The problem being solved\nis bringing the data objects closer to the frequent sources of requests using a\ndynamic repartitioning algorithm. This is important if the objective is to\nmitigate the overhead of network latency, and especially so if the partitions\nare widely geo-distributed. The intention is to bring these features to an\nexisting distributed key-value store product, Redis.\n"]},
{"authors": ["Shiyu Ji", "Kun Wan"], "title": ["An Asymptotically Tighter Bound on Sampling for Frequent Itemsets Mining"], "date": ["2017-03-24T02:59:51Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.08273v1"], "summary": ["  In this paper we present a new error bound on sampling algorithms for\nfrequent itemsets mining. We show that the new bound is asymptotically tighter\nthan the state-of-art bounds, i.e., given the chosen samples, for small enough\nerror probability, the new error bound is roughly half of the existing bounds.\nBased on the new bound, we give a new approximation algorithm, which is much\nsimpler compared to the existing approximation algorithms, but can also\nguarantee the worst approximation error with precomputed sample size. We also\ngive an algorithm which can approximate the top-$k$ frequent itemsets with high\naccuracy and efficiency.\n"]},
{"authors": ["Gr\u00e9gory M. Essertel", "Ruby Y. Tahboub", "James M. Decker", "Kevin J. Brown", "Kunle Olukotun", "Tiark Rompf"], "title": ["Flare: Native Compilation for Heterogeneous Workloads in Apache Spark"], "date": ["2017-03-23T20:04:55Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.08219v1"], "summary": ["  The need for modern data analytics to combine relational, procedural, and\nmap-reduce-style functional processing is widely recognized. State-of-the-art\nsystems like Spark have added SQL front-ends and relational query optimization,\nwhich promise an increase in expressiveness and performance. But how good are\nthese extensions at extracting high performance from modern hardware platforms?\n  While Spark has made impressive progress, we show that for relational\nworkloads, there is still a significant gap compared with best-of-breed query\nengines. And when stepping outside of the relational world, query optimization\ntechniques are ineffective if large parts of a computation have to be treated\nas user-defined functions (UDFs).\n  We present Flare: a new back-end for Spark that brings performance closer to\nthe best SQL engines, without giving up the added expressiveness of Spark. We\ndemonstrate order of magnitude speedups both for relational workloads such as\nTPC-H, as well as for a range of machine learning kernels that combine\nrelational and iterative functional processing.\n  Flare achieves these results through (1) compilation to native code, (2)\nreplacing parts of the Spark runtime system, and (3) extending the scope of\noptimization and code generation to large classes of UDFs.\n"]},
{"authors": ["Xi Wu", "Fengan Li", "Arun Kumar", "Kamalika Chaudhuri", "Somesh Jha", "Jeffrey F. Naughton"], "title": ["Bolt-on Differential Privacy for Scalable Stochastic Gradient\n  Descent-based Analytics"], "date": ["2016-06-15T11:14:29Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1606.04722v3"], "summary": ["  While significant progress has been made separately on analytics systems for\nscalable stochastic gradient descent (SGD) and private SGD, none of the major\nscalable analytics frameworks have incorporated differentially private SGD.\nThere are two inter-related issues for this disconnect between research and\npractice: (1) low model accuracy due to added noise to guarantee privacy, and\n(2) high development and runtime overhead of the private algorithms. This paper\ntakes a first step to remedy this disconnect and proposes a private SGD\nalgorithm to address \\emph{both} issues in an integrated manner. In contrast to\nthe white-box approach adopted by previous work, we revisit and use the\nclassical technique of {\\em output perturbation} to devise a novel \"bolt-on\"\napproach to private SGD. While our approach trivially addresses (2), it makes\n(1) even more challenging. We address this challenge by providing a novel\nanalysis of the $L_2$-sensitivity of SGD, which allows, under the same privacy\nguarantees, better convergence of SGD when only a constant number of passes can\nbe made over the data. We integrate our algorithm, as well as other\nstate-of-the-art differentially private SGD, into Bismarck, a popular scalable\nSGD-based analytics system on top of an RDBMS. Extensive experiments show that\nour algorithm can be easily integrated, incurs virtually no overhead, scales\nwell, and most importantly, yields substantially better (up to 4X) test\naccuracy than the state-of-the-art algorithms on many real datasets.\n"]},
{"authors": ["Kaori Abe", "Teppei Suzuki", "Shunya Ueta", "Akio Nakamura", "Yutaka Satoh", "Hirokatsu Kataoka"], "title": ["Changing Fashion Cultures"], "date": ["2017-03-23T03:48:08Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.07920v1"], "summary": ["  The paper presents a novel concept that analyzes and visualizes worldwide\nfashion trends. Our goal is to reveal cutting-edge fashion trends without\ndisplaying an ordinary fashion style. To achieve the fashion-based analysis, we\ncreated a new fashion culture database (FCDB), which consists of 76 million\ngeo-tagged images in 16 cosmopolitan cities. By grasping a fashion trend of\nmixed fashion styles,the paper also proposes an unsupervised fashion trend\ndescriptor (FTD) using a fashion descriptor, a codeword vetor, and temporal\nanalysis. To unveil fashion trends in the FCDB, the temporal analysis in FTD\neffectively emphasizes consecutive features between two different times. In\nexperiments, we clearly show the analysis of fashion trends and fashion-based\ncity similarity. As the result of large-scale data collection and an\nunsupervised analyzer, the proposed approach achieves world-level fashion\nvisualization in a time series. The code, model, and FCDB will be publicly\navailable after the construction of the project page.\n"]},
{"authors": ["Matthias Ruhl", "Mukund Sundararajan", "Qiqi Yan"], "title": ["Hierarchical Summarization of Metric Changes"], "date": ["2017-03-22T18:02:29Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.07795v1"], "summary": ["  We study changes in metrics that are defined on a cartesian product of trees.\nSuch metrics occur naturally in many practical applications, where a global\nmetric (such as revenue) can be broken down along several hierarchical\ndimensions (such as location, gender, etc).\n  Given a change in such a metric, our goal is to identify a small set of\nnon-overlapping data segments that account for the change. An organization\ninterested in improving the metric can then focus their attention on these data\nsegments.\n  Our key contribution is an algorithm that mimics the operation of a\nhierarchical organization of analysts. The algorithm has been successfully\napplied, for example within Google Adwords to help advertisers triage the\nperformance of their advertising campaigns.\n  We show that the algorithm is optimal for two dimensions, and has an\napproximation ratio $\\log^{d-2}(n+1)$ for $d \\geq 3$ dimensions, where $n$ is\nthe number of input data segments. For the Adwords application, we can show\nthat our algorithm is in fact a $2$-approximation.\n  Mathematically, we identify a certain data pattern called a \\emph{conflict}\nthat both guides the design of the algorithm, and plays a central role in the\nhardness results. We use these conflicts to both derive a lower bound of\n$1.144^{d-2}$ (again $d\\geq3$) for our algorithm, and to show that the problem\nis NP-hard, justifying the focus on approximation.\n"]},
{"authors": ["Yuanzhen Ji", "Jun Sun", "Anisoara Nica", "Zbigniew Jerzak", "Gregor Hackenbroich", "Christof Fetzer"], "title": ["Quality-Driven Disorder Handling for M-way Sliding Window Stream Joins"], "date": ["2017-03-22T12:27:21Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.07617v1"], "summary": ["  Sliding window join is one of the most important operators for stream\napplications. To produce high quality join results, a stream processing system\nmust deal with the ubiquitous disorder within input streams which is caused by\nnetwork delay, asynchronous source clocks, etc. Disorder handling involves an\ninevitable tradeoff between the latency and the quality of produced join\nresults. To meet different requirements of stream applications, it is desirable\nto provide a user-configurable result-latency vs. result-quality tradeoff.\nExisting disorder handling approaches either do not provide such\nconfigurability, or support only user-specified latency constraints.\n  In this work, we advocate the idea of quality-driven disorder handling, and\npropose a buffer-based disorder handling approach for sliding window joins,\nwhich minimizes sizes of input-sorting buffers, thus the result latency, while\nrespecting user-specified result-quality requirements. The core of our approach\nis an analytical model which directly captures the relationship between sizes\nof input buffers and the produced result quality. Our approach is generic. It\nsupports m-way sliding window joins with arbitrary join conditions. Experiments\non real-world and synthetic datasets show that, compared to the state of the\nart, our approach can reduce the result latency incurred by disorder handling\nby up to 95% while providing the same level of result quality.\n"]},
{"authors": ["Yingjun Wu", "Wentian Guo", "Chee-Yong Chan", "Kian-Lee Tan"], "title": ["Fast Failure Recovery for Main-Memory DBMSs on Multicores"], "date": ["2016-04-12T03:00:33Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1604.03226v2"], "summary": ["  Main-memory database management systems (DBMS) can achieve excellent\nperformance when processing massive volume of on-line transactions on modern\nmulti-core machines. But existing durability schemes, namely, tuple-level and\ntransaction-level logging-and-recovery mechanisms, either degrade the\nperformance of transaction processing or slow down the process of failure\nrecovery. In this paper, we show that, by exploiting application semantics, it\nis possible to achieve speedy failure recovery without introducing any costly\nlogging overhead to the execution of concurrent transactions. We propose\nPACMAN, a parallel database recovery mechanism that is specifically designed\nfor lightweight, coarse-grained transaction-level logging. PACMAN leverages a\ncombination of static and dynamic analyses to parallelize the log recovery: at\ncompile time, PACMAN decomposes stored procedures by carefully analyzing\ndependencies within and across programs; at recovery time, PACMAN exploits the\navailability of the runtime parameter values to attain an execution schedule\nwith a high degree of parallelism. As such, recovery performance is remarkably\nincreased. We evaluated PACMAN in a fully-fledged main-memory DBMS running on a\n40-core machine. Compared to several state-of-the-art database recovery\nmechanisms, PACMAN can significantly reduce recovery time without compromising\nthe efficiency of transaction processing.\n"]},
{"authors": ["R\u00e9mi Cura", "Bertrand Dumenieu", "Julien Perret", "Maurizio Gribaudi"], "title": ["Historical collaborative geocoding"], "date": ["2017-03-21T10:45:09Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.07138v2"], "summary": ["  The latest developments in digital have provided large data sets that can\nincreasingly easily be accessed and used. These data sets often contain\nindirect localisation information, such as historical addresses. Historical\ngeocoding is the process of transforming the indirect localisation information\nto direct localisation that can be placed on a map, which enables spatial\nanalysis and cross-referencing. Many efficient geocoders exist for current\naddresses, but they do not deal with the temporal aspect and are based on a\nstrict hierarchy (..., city, street, house number) that is hard or impossible\nto use with historical data. Indeed historical data are full of uncertainties\n(temporal aspect, semantic aspect, spatial precision, confidence in historical\nsource, ...) that can not be resolved, as there is no way to go back in time to\ncheck. We propose an open source, open data, extensible solution for geocoding\nthat is based on the building of gazetteers composed of geohistorical objects\nextracted from historical topographical maps. Once the gazetteers are\navailable, geocoding an historical address is a matter of finding the\ngeohistorical object in the gazetteers that is the best match to the historical\naddress. The matching criteriae are customisable and include several dimensions\n(fuzzy semantic, fuzzy temporal, scale, spatial precision ...). As the goal is\nto facilitate historical work, we also propose web-based user interfaces that\nhelp geocode (one address or batch mode) and display over current or historical\ntopographical maps, so that they can be checked and collaboratively edited. The\nsystem is tested on Paris city for the 19-20th centuries, shows high returns\nrate and is fast enough to be used interactively.\n"]},
{"authors": ["Landy Rajaonarivo", "Matthieu Courgeon", "Eric Maisel", "Pierre De Loor"], "title": ["Inline Co-Evolution between Users and Information Presentation for Data\n  Exploration"], "date": ["2017-03-22T08:05:30Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.07555v1"], "summary": ["  This paper presents an intelligent user interface model dedicated to the\nexploration of complex databases. This model is implemented on a 3D metaphor :\na virtual museum. In this metaphor, the database elements are embodied as\nmuseum objects. The objects are grouped in rooms according to their semantic\nproperties and relationships and the rooms organization forms the museum. Rooms\norganization is not predefi-ned but defined incrementally by taking into\naccount not only the relationships between objects, but also the users centers\nof interest. The latter are evaluated in real-time through user interactions\nwithin the virtual museum. This interface allows for a personal reading and\nfavors the discovery of unsuspec-ted links between data. In this paper, we\npresent our model's formalization as well as its application to the context of\ncultural heritage.\n"]},
{"authors": ["Vishal Jain", "Mahesh Kumar Madan"], "title": ["Multi Agent Driven Data Mining For Knowledge Discovery in Cloud\n  Computing"], "date": ["2017-03-21T18:06:29Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.07371v1"], "summary": ["  Today, huge amount of data is available on the web. Now there is a need to\nconvert that data in knowledge which can be useful for different purposes. This\npaper depicts the use of data mining process, OLAP with the combination of\nmulti agent system to find the knowledge from data in cloud computing. For\nthis, I am also trying to explain one case study of online shopping of one\nBakery Shop. May be we can increase the sale of items by using the model, which\nI am trying to represent.\n"]},
{"authors": ["Kouakou Ive Arsene Koffi", "Konan Marcellin Brou", "Souleymane Oumtanaga"], "title": ["Developpement de Methodes Automatiques pour la Reutilisation des\n  Composants Logiciels"], "date": ["2017-03-21T15:34:28Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.09749v1"], "summary": ["  The large amount of information and the increasing complexity of applications\nconstrain developers to have stand-alone and reusable components from libraries\nand component markets.Our approach consists in developing methods to evaluate\nthe quality of the software component of these libraries, on the one hand and\nmoreover to optimize the financial cost and the adaptation's time of these\nselected components. Our objective function defines a metric that maximizes the\nvalue of the software component quality by minimizing the financial cost and\nmaintenance time. This model should make it possible to classify the components\nand order them in order to choose the most optimized.\n  MOTS-CLES : d{\\'e}veloppement de m{\\'e}thode, r{\\'e}utilisation, composants\nlogiciels, qualit{\\'e} de composant\n  KEYWORDS:method development, reuse, software components, component quality .\n"]},
{"authors": ["Lorena Etcheverry", "Alejandro A. Vaisman"], "title": ["Efficient Analytical Queries on Semantic Web Data Cubes"], "date": ["2017-03-21T13:41:51Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.07213v1"], "summary": ["  The amount of multidimensional data published on the semantic web (SW) is\nconstantly increasing, due to initiatives such as Open Data and Open Government\nData, among other ones. Models, languages, and tools, that allow to obtain\nvaluable information efficiently, are thus required. Multidimensional data are\ntypically represented as data cubes, and exploited using Online Analytical\nProcessing (OLAP) techniques. The RDF Data Cube Vocabulary, also denoted QB, is\nthe current W3C standard to represent statistical data on the SW.Since QB does\nnot include key features needed for OLAP analysis, in previous work we have\nproposed an extension, denoted QB4OLAP, to overcome this problem without the\nneed of modifying already published data. Once data cubes are represented on\nthe SW, we need tools to analyze them. However, writing efficient analytical\nqueries over SW cubes demands a deep knowledge of RDF and SPARQL. These skills\nare not common in typical analytical users. Also, OLAP languages like MDX are\nfar from being easily understood by the final user. The lack of friendly tools\nto exploit multidimensional data on the SW is a barrier that needs to be broken\nto promote the publication of such data. We address this problem in this paper.\nOur approach is based on allowing analytical users to write queries using OLAP\noperations over cubes, without dealing with SW standards. For this, we devised\nCQL (standing for Cube Query Language), a simple, high-level query language\nthat operates over cubes. Using the metadata provided by QB4OLAP, we translate\nCQL queries into SPARQL. Then, we propose query improvement strategies to\nproduce efficient SPARQL queries, adapting SPARQL query optimization\ntechniques. We evaluate our approach using the Star-Schema benchmark, showing\nthat our proposal outperforms others. A web application that allows querying SW\ndata cubes using CQL, completes our contributions.\n"]},
{"authors": ["Niek Tax", "Benjamin Dalmas", "Natalia Sidorova", "Wil M P van der Aalst", "Sylvie Norre"], "title": ["Interest-Driven Discovery of Local Process Models"], "date": ["2017-03-21T09:57:59Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.07116v1"], "summary": ["  Local Process Models (LPM) describe structured fragments of process behavior\noccurring in the context of less structured business processes. Traditional LPM\ndiscovery aims to generate a collection of process models that describe highly\nfrequent behavior, but these models do not always provide useful answers for\nquestions posed by process analysts aiming at business process improvement. We\npropose a framework for goal-driven LPM discovery, based on utility functions\nand constraints. We describe four scopes on which these utility functions and\nconstrains can be defined, and show that utility functions and constraints on\ndifferent scopes can be combined to form composite utility\nfunctions/constraints. Finally, we demonstrate the applicability of our\napproach by presenting several actionable business insights discovered with LPM\ndiscovery on two real life data sets.\n"]},
{"authors": ["R\u00f3bert Beck", "L\u00e1szl\u00f3 Dobos", "Tam\u00e1s Budav\u00e1ri", "Alexander S. Szalay", "Istv\u00e1n Csabai"], "title": ["Photo-z-SQL: integrated, flexible photometric redshift computation in a\n  database"], "date": ["2016-11-04T22:48:06Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1611.01560v2"], "summary": ["  We present a flexible template-based photometric redshift estimation\nframework, implemented in C#, that can be seamlessly integrated into a SQL\ndatabase (or DB) server and executed on-demand in SQL. The DB integration\neliminates the need to move large photometric datasets outside a database for\nredshift estimation, and utilizes the computational capabilities of DB\nhardware. The code is able to perform both maximum likelihood and Bayesian\nestimation, and can handle inputs of variable photometric filter sets and\ncorresponding broad-band magnitudes. It is possible to take into account the\nfull covariance matrix between filters, and filter zero points can be\nempirically calibrated using measurements with given redshifts. The list of\nspectral templates and the prior can be specified flexibly, and the expensive\nsynthetic magnitude computations are done via lazy evaluation, coupled with a\ncaching of results. Parallel execution is fully supported. For large upcoming\nphotometric surveys such as the LSST, the ability to perform in-place photo-z\ncalculation would be a significant advantage. Also, the efficient handling of\nvariable filter sets is a necessity for heterogeneous databases, for example\nthe Hubble Source Catalog, and for cross-match services such as SkyQuery. We\nillustrate the performance of our code on two reference photo-z estimation\ntesting datasets, and provide an analysis of execution time and scalability\nwith respect to different configurations. The code is available for download at\nhttps://github.com/beckrob/Photo-z-SQL.\n"]},
{"authors": ["Andrea Detti", "Michele Orru", "Riccardo Paolillo", "Giulio Rossi", "Pierpaolo Loreti", "Lorenzo Bracciale", "Nicola Blefari Melazzi"], "title": ["Application of Information Centric Networking to NoSQL Databases: the\n  Spatio-Temporal use case"], "date": ["2017-03-18T20:31:37Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.06348v1"], "summary": ["  This paper explores methodologies, advantages and challenges related to the\nuse of the Information Centric Network technology for developing NoSQL\ndistributed databases, which are expected to play a central role in the\nforthcoming IoT and BigData era. ICN services make possible to simplify the\ndevelopment of the database software, improve performance, and provide\ndata-level access control. We use our findings for devising a NoSQL\nspatio-temporal database, named OpenGeoBase, and evaluate its performance with\na real data set related to Intelligent Transport System applications.\n"]},
{"authors": ["Yang Cao", "Masatoshi Yoshikawa", "Yonghui Xiao", "Li Xiong"], "title": ["Quantifying Differential Privacy under Temporal Correlations"], "date": ["2016-10-24T18:54:18Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1610.07543v2"], "summary": ["  Differential Privacy (DP) has received increased attention as a rigorous\nprivacy framework. Existing studies employ traditional DP mechanisms (e.g., the\nLaplace mechanism) as primitives, which assume that the data are independent,\nor that adversaries do not have knowledge of the data correlations. However,\ncontinuously generated data in the real world tend to be temporally correlated,\nand such correlations can be acquired by adversaries. In this paper, we\ninvestigate the potential privacy loss of a traditional DP mechanism under\ntemporal correlations in the context of continuous data release. First, we\nmodel the temporal correlations using Markov model and analyze the privacy\nleakage of a DP mechanism when adversaries have knowledge of such temporal\ncorrelations. Our analysis reveals that the privacy leakage of a DP mechanism\nmay accumulate and increase over time. We call it temporal privacy leakage.\nSecond, to measure such privacy leakage, we design an efficient algorithm for\ncalculating it in polynomial time. Although the temporal privacy leakage may\nincrease over time, we also show that its supremum may exist in some cases.\nThird, to bound the privacy loss, we propose mechanisms that convert any\nexisting DP mechanism into one against temporal privacy leakage. Experiments\nwith synthetic data confirm that our approach is efficient and effective.\n"]},
{"authors": ["Kunal Gupta", "Astha Sachdev", "Ashish Sureka"], "title": ["Empirical Analysis on Comparing the Performance of Alpha Miner Algorithm\n  in SQL Query Language and NoSQL Column-Oriented Databases Using Apache\n  Phoenix"], "date": ["2017-03-16T06:32:11Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.05481v1"], "summary": ["  Process-Aware Information Systems (PAIS) is an IT system that support\nbusiness processes and generate large amounts of event logs from the execution\nof business processes. An event log is represented as a tuple of CaseID,\nTimestamp, Activity and Actor. Process Mining is a new and emerging field that\naims at analyzing the event logs to discover, enhance and improve business\nprocesses and check conformance between run time and design time business\nprocesses. The large volume of event logs generated are stored in the\ndatabases. Relational databases perform well for a certain class of\napplications. However, there are a certain class of applications for which\nrelational databases are not able to scale. To handle such class of\napplications, NoSQL database systems emerged. Discovering a process model\n(workflow model) from event logs is one of the most challenging and important\nProcess Mining task. The $\\alpha$-miner algorithm is one of the first and most\nwidely used Process Discovery technique. Our objective is to investigate which\nof the databases (Relational or NoSQL) performs better for a Process Discovery\napplication under Process Mining. We implement the $\\alpha$-miner algorithm on\nrelational (row-oriented) and NoSQL (column-oriented) databases in database\nquery languages so that our algorithm is tightly coupled to the database. We\npresent a performance benchmarking and comparison of the $\\alpha$-miner\nalgorithm on row-oriented database and NoSQL column-oriented database so that\nwe can compare which database can efficiently store massive event logs and\nanalyze it in seconds to discover a process model.\n"]},
{"authors": ["Bilal Abbasi", "Jeff Calder", "Adam M. Oberman"], "title": ["Anomaly detection and classification for streaming data using PDEs"], "date": ["2016-08-15T18:03:51Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1608.04348v2"], "summary": ["  Nondominated sorting, also called Pareto Depth Analysis (PDA), is widely used\nin multi-objective optimization and has recently found important applications\nin multi-criteria anomaly detection. Recently, a partial differential equation\n(PDE) continuum limit was discovered for nondominated sorting leading to a very\nfast approximate sorting algorithm called PDE-based ranking. We propose in this\npaper a fast real-time streaming version of the PDA algorithm for anomaly\ndetection that exploits the computational advantages of PDE continuum limits.\nFurthermore, we derive new PDE continuum limits for sorting points within their\nnondominated layers and show how the new PDEs can be used to classify anomalies\nbased on which criterion was more significantly violated. We also prove\nstatistical convergence rates for PDE-based ranking, and present the results of\nnumerical experiments with both synthetic and real data.\n"]},
{"authors": ["Ryan Spring", "Anshumali Shrivastava"], "title": ["A New Unbiased and Efficient Class of LSH-Based Samplers and Estimators\n  for Partition Function Computation in Log-Linear Models"], "date": ["2017-03-15T14:01:21Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.05160v1"], "summary": ["  Log-linear models are arguably the most successful class of graphical models\nfor large-scale applications because of their simplicity and tractability.\nLearning and inference with these models require calculating the partition\nfunction, which is a major bottleneck and intractable for large state spaces.\nImportance Sampling (IS) and MCMC-based approaches are lucrative. However, the\ncondition of having a \"good\" proposal distribution is often not satisfied in\npractice.\n  In this paper, we add a new dimension to efficient estimation via sampling.\nWe propose a new sampling scheme and an unbiased estimator that estimates the\npartition function accurately in sub-linear time. Our samples are generated in\nnear-constant time using locality sensitive hashing (LSH), and so are\ncorrelated and unnormalized. We demonstrate the effectiveness of our proposed\napproach by comparing the accuracy and speed of estimating the partition\nfunction against other state-of-the-art estimation techniques including IS and\nthe efficient variant of Gumbel-Max sampling. With our efficient sampling\nscheme, we accurately train real-world language models using only 1-2% of\ncomputations.\n"]},
{"authors": ["Wil M. P. van der Aalst", "Guangming Li", "Marco Montali"], "title": ["Object-Centric Behavioral Constraints"], "date": ["2017-03-14T22:18:36Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.05740v1"], "summary": ["  Today's process modeling languages often force the analyst or modeler to\nstraightjacket real-life processes into simplistic or incomplete models that\nfail to capture the essential features of the domain under study. Conventional\nbusiness process models only describe the lifecycles of individual instances\n(cases) in isolation. Although process models may include data elements (cf.\nBPMN), explicit connections to real data models (e.g., an entity relationship\nmodel or a UML class model) are rarely made. Therefore, we propose a novel\napproach that extends data models with a behavioral perspective. Data models\ncan easily deal with many-to-many and one-to-many relationships. This is\nexploited to create process models that can also model complex interactions\nbetween different types of instances. Classical multiple-instance problems are\ncircumvented by using the data model for event correlation. The declarative\nnature of the proposed language makes it possible to model behavioral\nconstraints over activities like cardinality constraints in data models. The\nresulting object-centric behavioral constraint (OCBC) model is able to describe\nprocesses involving interacting instances and complex data dependencies. In\nthis paper, we introduce the OCBC model and notation, providing a number of\nexamples that give a flavour of the approach. We then define a set-theoretic\nsemantics exploiting cardinality constraints within and across time points. We\nfinally formalize conformance checking in our setting, arguing that evaluating\nconformance against OCBC models requires diagnostics that go beyond what is\nprovided by contemporary conformance checking approaches.\n"]},
{"authors": ["Jorge Baier", "Dietrich Daroch", "Juan Reutter", "Domagoj Vrgo\u010d"], "title": ["Evaluating navigational RDF queries over the Web"], "date": ["2017-01-23T15:31:17Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.06454v2"], "summary": ["  Semantic Web, and its underlying data format RDF, lend themselves naturally\nto navigational querying due to their graph-like structure. This is\nparticularly evident when considering RDF data on the Web, where various\nseparately published datasets reference each other and form a giant graph known\nas the Web of Linked Data. And while navigational queries over singular RDF\ndatasets are supported through SPARQL property paths, not much is known about\nevaluating them over Linked Data. In this paper we propose a method for\nevaluating property path queries over the Web based on the classical AI search\nalgorithm A*, show its optimality in the open world setting of the Web, and\ntest it using real world queries which access a variety of RDF datasets\navailable online.\n"]},
{"authors": ["Sergio Abriola", "Mar\u00eda Emilia Descotte", "Raul Fervari", "Santiago Figueira"], "title": ["Axiomatizations for downward XPath on Data Trees"], "date": ["2016-05-13T17:49:22Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1605.04271v2"], "summary": ["  We give sound and complete axiomatizations for XPath with data tests by\n\"equality\" or \"inequality\", and containing the single \"child\" axis. This\ndata-aware logic predicts over data trees, which are tree-like structures whose\nevery node contains a label from a finite alphabet and a data value from an\ninfinite domain. The language allows us to compare data values of two nodes but\ncannot access the data values themselves (i.e. there is no comparison by\nconstants). Our axioms are in the style of equational logic, extending the\naxiomatization of data-oblivious XPath, by B. ten Cate, T. Litak and M. Marx.\nWe axiomatize the full logic with tests by \"equality\" and \"inequality\", and\nalso a simpler fragment with \"equality\" tests only. Our axiomatizations apply\nboth to node expressions and path expressions. The proof of completeness relies\non a novel normal form theorem for XPath with data tests.\n"]},
{"authors": ["Lucas Braun", "Renato Marroquin", "Kai-En Tsay", "Donald Kossmann"], "title": ["MTBase: Optimizing Cross-Tenant Database Queries"], "date": ["2017-03-13T08:35:39Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.04290v1"], "summary": ["  In the last decade, many business applications have moved into the cloud. In\nparticular, the \"database-as-a-service\" paradigm has become mainstream. While\nexisting multi-tenant data management systems focus on single-tenant query\nprocessing, we believe that it is time to rethink how queries can be processed\nacross multiple tenants in such a way that we do not only gain more valuable\ninsights, but also at minimal cost. As we will argue in this paper, standard\nSQL semantics are insufficient to process cross-tenant queries in an\nunambiguous way, which is why existing systems use other, expensive means like\nETL or data integration. We first propose MTSQL, a set of extensions to\nstandard SQL, which fixes the ambiguity problem. Next, we present MTBase, a\nquery processing middleware that efficiently processes MTSQL on top of SQL. As\nwe will see, there is a canonical, provably correct, rewrite algorithm from\nMTSQL to SQL, which may however result in poor query execution performance,\neven on high-performance database products. We further show that with\ncarefully-designed optimizations, execution times can be reduced in such ways\nthat the difference to single-tenant queries becomes marginal.\n"]},
{"authors": ["S. Matthew English", "Ehsan Nezhadian"], "title": ["Application of Bitcoin Data-Structures & Design Principles to Supply\n  Chain Management"], "date": ["2017-03-13T00:32:14Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.04206v1"], "summary": ["  Heretofore the concept of \"blockchain\" has not been precisely defined.\nAccordingly the potential useful applications of this technology have been\nlargely inflated. This work sidesteps the question of what constitutes a\nblockchain as such and focuses on the architectural components of the Bitcoin\ncryptocurrency, insofar as possible, in isolation. We consider common problems\ninherent in the design of effective supply chain management systems. With each\nidentified problem we propose a solution that utilizes one or more component\naspects of Bitcoin. This culminates in five design principles for increased\nefficiency in supply chain management systems through the application of\nincentive mechanisms and data structures native to the Bitcoin cryptocurrency\nprotocol.\n"]},
{"authors": ["Ilias Tachmazidis", "Sotiris Batsakis", "John Davies", "Alistair Duke", "Mauro Vallati", "Grigoris Antoniou", "Sandra Stincic Clarke"], "title": ["A Hypercat-enabled Semantic Internet of Things Data Hub: Technical\n  Report"], "date": ["2017-03-01T17:10:27Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.00391v2"], "summary": ["  An increasing amount of information is generated from the rapidly increasing\nnumber of sensor networks and smart devices. A wide variety of sources generate\nand publish information in different formats, thus highlighting\ninteroperability as one of the key prerequisites for the success of Internet of\nThings (IoT). The BT Hypercat Data Hub provides a focal point for the sharing\nand consumption of available datasets from a wide range of sources. In this\nwork, we propose a semantic enrichment of the BT Hypercat Data Hub, using\nwell-accepted Semantic Web standards and tools. We propose an ontology that\ncaptures the semantics of the imported data and present the BT SPARQL Endpoint\nby means of a mapping between SPARQL and SQL queries. Furthermore, federated\nSPARQL queries allow queries over multiple hub-based and external data sources.\nFinally, we provide two use cases in order to illustrate the advantages\nafforded by our semantic approach.\n"]},
{"authors": ["Tien Tuan Anh Dinh", "Ji Wang", "Gang Chen", "Rui Liu", "Beng Chin Ooi", "Kian-Lee Tan"], "title": ["BLOCKBENCH: A Framework for Analyzing Private Blockchains"], "date": ["2017-03-12T02:10:06Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.04057v1"], "summary": ["  Blockchain technologies are taking the world by storm. Public blockchains,\nsuch as Bitcoin and Ethereum, enable secure peer-to-peer applications like\ncrypto-currency or smart contracts. Their security and performance are well\nstudied. This paper concerns recent private blockchain systems designed with\nstronger security (trust) assumption and performance requirement. These systems\ntarget and aim to disrupt applications which have so far been implemented on\ntop of database systems, for example banking, finance applications. Multiple\nplatforms for private blockchains are being actively developed and fine tuned.\nHowever, there is a clear lack of a systematic framework with which different\nsystems can be analyzed and compared against each other. Such a framework can\nbe used to assess blockchains' viability as another distributed data processing\nplatform, while helping developers to identify bottlenecks and accordingly\nimprove their platforms.\n  In this paper, we first describe BlockBench, the first evaluation framework\nfor analyzing private blockchains. It serves as a fair means of comparison for\ndifferent platforms and enables deeper understanding of different system design\nchoices. Any private blockchain can be integrated to BlockBench via simple APIs\nand benchmarked against workloads that are based on real and synthetic smart\ncontracts. BlockBench measures overall and component-wise performance in terms\nof throughput, latency, scalability and fault-tolerance. Next, we use\nBlockBench to conduct comprehensive evaluation of three major private\nblockchains: Ethereum, Parity and Hyperledger Fabric. The results demonstrate\nthat these systems are still far from displacing current database systems in\ntraditional data processing workloads. Furthermore, there are gaps in\nperformance among the three systems which are attributed to the design choices\nat different layers of the software stack.\n"]},
{"authors": ["Antoine Amarilli", "Mika\u00ebl Monet", "Pierre Senellart"], "title": ["Conjunctive Queries on Probabilistic Graphs: Combined Complexity"], "date": ["2017-03-09T09:50:57Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.03201v1"], "summary": ["  Query evaluation over probabilistic databases is known to be intractable in\nmany cases, even in data complexity, i.e., when the query is fixed. Although\nsome restrictions of the queries [19] and instances [4] have been proposed to\nlower the complexity, these known tractable cases usually do not apply to\ncombined complexity, i.e., when the query is not fixed. This leaves open the\nquestion of which query and instance languages ensure the tractability of\nprobabilistic query evaluation in combined complexity.\n  This paper proposes the first general study of the combined complexity of\nconjunctive query evaluation on probabilistic instances over binary signatures,\nwhich we can alternatively phrase as a probabilistic version of the graph\nhomomorphism problem, or of a constraint satisfaction problem (CSP) variant. We\nstudy the complexity of this problem depending on whether instances and queries\ncan use features such as edge labels, disconnectedness, branching, and edges in\nboth directions. We show that the complexity landscape is surprisingly rich,\nusing a variety of technical tools: automata-based compilation to d-DNNF\nlineages as in [4], \\b{eta}-acyclic lineages using [10], the X-property for\ntractable CSP from [24], graded DAGs [27] and various coding techniques for\nhardness proofs.\n"]},
{"authors": ["Luigi Santocanale"], "title": ["The quasiequational theory of relational lattices, in the pure lattice\n  signature (embeddability into relational lattices is undecidable)"], "date": ["2016-07-11T14:59:42Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1607.02988v3"], "summary": ["  The natural join and the inner union operations combine relations of a\ndatabase. Tropashko and Spight realized that these two operations are themeet\nand join operations in a class of lattices, known by now as the relational\nlattices. They proposed then lattice theory as an algebraic approach to\nthetheory of databases alternative to the relational algebra. Litak et al.\nproposed an axiomatization of relational lattices over the signature that\nextends thepure lattice signature with a constant and argued that the\nquasiequational theory of relational lattices over this extended signature is\nundecidable.We prove in this paper that embeddability is undecidable for\nrelational lattices. More precisely, it is undecidable whether a finite\nsubdirectly-irreduciblelattice can be embedded into a relational lattice. Our\nproof is a reduction from the coverability problem of a multimodal frame by a\nuniversal product frameand, indirectly, from the representability problem for\nrelation algebras. As corollaries we obtain the following results: the\nquasiequational theoryof relational lattices over the pure lattice signature is\nundecidable and has no finite base; there is a quasiequation over the pure\nlattice signature which holds in all the finite relational lattices but fails\nin an infinite relational lattice.\n"]},
{"authors": ["Mahmoud Abo Khamis", "Hung Q. Ngo", "Atri Rudra"], "title": ["Juggling Functions Inside a Database"], "date": ["2017-03-09T06:08:01Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.03147v1"], "summary": ["  We define and study the Functional Aggregate Query (FAQ) problem, which\ncaptures common computational tasks across a very wide range of domains\nincluding relational databases, logic, matrix and tensor computation,\nprobabilistic graphical models, constraint satisfaction, and signal processing.\nSimply put, an FAQ is a declarative way of defining a new function from a\ndatabase of input functions.\n  We present \"InsideOut\", a dynamic programming algorithm, to evaluate an FAQ.\nThe algorithm rewrites the input query into a set of easier-to-compute FAQ\nsub-queries. Each sub-query is then evaluated using a worst-case optimal\nrelational join algorithm. The topic of designing algorithms to optimally\nevaluate the classic multiway join problem has seen exciting developments in\nthe past few years. Our framework tightly connects these new ideas in database\ntheory with a vast number of application areas in a coherent manner, showing\npotentially that a good database engine can be a general-purpose constraint\nsolver, relational data store, graphical model inference engine, and\nmatrix/tensor computation processor all at once.\n  The InsideOut algorithm is very simple, as shall be described in this paper.\nYet, in spite of solving an extremely general problem, its runtime either is as\ngood as or improves upon the best known algorithm for the applications that FAQ\nspecializes to. These corollaries include computational tasks in graphical\nmodel inference, matrix/tensor operations, relational joins, and logic. Better\nyet, InsideOut can be used within any database engine, because it is basically\na principled way of rewriting queries. Indeed, it is already part of the\nLogicBlox database engine, helping efficiently answer traditional database\nqueries, graphical model inference queries, and train a large class of machine\nlearning models inside the database itself.\n"]},
{"authors": ["Francesco Bonchi", "Sara Hajian", "Bud Mishra", "Daniele Ramazzotti"], "title": ["Exposing the Probabilistic Causal Structure of Discrimination"], "date": ["2015-10-02T10:31:29Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1510.00552v3"], "summary": ["  Discrimination discovery from data is an important task aiming at identifying\npatterns of illegal and unethical discriminatory activities against\nprotected-by-law groups, e.g., ethnic minorities. While any legally-valid proof\nof discrimination requires evidence of causality, the state-of-the-art methods\nare essentially correlation-based, albeit, as it is well known, correlation\ndoes not imply causation.\n  In this paper we take a principled causal approach to the data mining problem\nof discrimination detection in databases. Following Suppes' probabilistic\ncausation theory, we define a method to extract, from a dataset of historical\ndecision records, the causal structures existing among the attributes in the\ndata. The result is a type of constrained Bayesian network, which we dub\nSuppes-Bayes Causal Network (SBCN). Next, we develop a toolkit of methods based\non random walks on top of the SBCN, addressing different anti-discrimination\nlegal concepts, such as direct and indirect discrimination, group and\nindividual discrimination, genuine requirement, and favoritism. Our experiments\non real-world datasets confirm the inferential power of our approach in all\nthese different tasks.\n"]},
{"authors": ["Chang Yao", "Meihui Zhang", "Qian Lin", "Beng Chin Ooi", "Jiatao Xu"], "title": ["Scaling Distributed Transaction Processing and Recovery based on\n  Dependency Logging"], "date": ["2017-03-08T06:21:07Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.02722v1"], "summary": ["  DGCC protocol has been shown to achieve good performance on multi-core\nin-memory system. However, distributed transactions complicate the dependency\nresolution, and therefore, an effective transaction partitioning strategy is\nessential to reduce expensive multi-node distributed transactions. During\nfailure recovery, log must be examined from the last checkpoint onwards and the\naffected transactions are re-executed based on the way they are partitioned and\nexecuted. Existing approaches treat both transaction management and recovery as\ntwo separate problems, even though recovery is dependent on the sequence in\nwhich transactions are executed.\n  In this paper, we propose to treat the transaction management and recovery\nproblems as one. We first propose an efficient Distributed Dependency Graph\nbased Concurrency Control (DistDGCC) protocol for handling transactions\nspanning multiple nodes, and propose a new novel and efficient logging protocol\ncalled Dependency Logging that also makes use of dependency graphs for\nefficient logging and recovery. DistDGCC optimizes the average cost for each\ndistributed transaction by processing transactions in batch. Moreover, it also\nreduces the effects of thread blocking caused by distributed transactions and\nconsequently improves the runtime performance. Further, dependency logging\nexploits the same data structure that is used by DistDGCC to reduce the logging\noverhead, as well as the logical dependency information to improve the recovery\nparallelism. Extensive experiments are conducted to evaluate the performance of\nour proposed technique against state-of-the-art techniques. Experimental\nresults show that DistDGCC is efficient and scalable, and dependency logging\nsupports fast recovery with marginal runtime overhead. Hence, the overall\nsystem performance is significantly improved as a result.\n"]},
{"authors": ["Fabio Porto", "Amir Khatibi", "Jo\u00e3o R. Nobre", "Eduardo Ogasawara", "Patrick Valduriez", "Dennis Shasha"], "title": ["Constellation Queries over Big Data"], "date": ["2017-03-07T23:45:46Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.02638v1"], "summary": ["  A geometrical pattern is a set of points with all pairwise distances (or,\nmore generally, relative distances) specified. Finding matches to such patterns\nhas applications to spatial data in seismic, astronomical, and transportation\ncontexts. For example, a particularly interesting geometric pattern in\nastronomy is the Einstein cross, which is an astronomical phenomenon in which a\nsingle quasar is observed as four distinct sky objects (due to gravitational\nlensing) when captured by earth telescopes. Finding such crosses, as well as\nother geometric patterns, is a challenging problem as the potential number of\nsets of elements that compose shapes is exponentially large in the size of the\ndataset and the pattern. In this paper, we denote geometric patterns as\nconstellation queries and propose algorithms to find them in large data\napplications. Our methods combine quadtrees, matrix multiplication, and\nunindexed join processing to discover sets of points that match a geometric\npattern within some additive factor on the pairwise distances. Our distributed\nexperiments show that the choice of composition algorithm (matrix\nmultiplication or nested loops) depends on the freedom introduced in the query\ngeometry through the distance additive factor. Three clearly identified blocks\nof threshold values guide the choice of the best composition algorithm.\nFinally, solving the problem for relative distances requires a novel\ncontinuous-to-discrete transformation. To the best of our knowledge this paper\nis the first to investigate constellation queries at scale.\n"]},
{"authors": ["Carlos Vega", "Paula Roquero", "Rafael Leira", "Ivan Gonzalez", "Javier Aracil"], "title": ["Loginson: a transform and load system for very large scale log analysis\n  in large IT infrastructures"], "date": ["2017-03-07T21:30:17Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.02602v1"], "summary": ["  Nowadays, most systems and applications produce log records that are useful\nfor security and monitoring purposes such as debugging programming errors,\nchecking system status, and detecting configuration problems or even attacks.\nTo this end, a log repository becomes necessary whereby logs can be accessed\nand visualized in a timely manner. This paper presents Loginson, a\nhigh-performance log centralization system for large-scale log collection and\nprocessing in large IT infrastructures. Besides log collection, Loginson\nprovides high-level analytics through a visual interface for the purpose of\ntroubleshooting critical incidents. We note that Loginson outperforms all of\nthe other log centralization solutions by taking full advantage of the vertical\nscalability, and therefore decreasing Capital Expenditure (CAPEX) and Operating\nExpense (OPEX) costs for deployment scenarios with a huge volume of log data.\n"]},
{"authors": ["Peyman Behzadnia", "Yi-Cheng Tu", "Bo Zeng", "Wei Yuan"], "title": ["Energy-Aware Disk Storage Management: Online Approach with Application\n  in DBMS"], "date": ["2017-03-07T20:50:46Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.02591v1"], "summary": ["  Energy consumption has become a first-class optimization goal in design and\nimplementation of data-intensive computing systems. This is particularly true\nin the design of database management systems (DBMS), which was found to be the\nmajor consumer of energy in the software stack of modern data centers. Among\nall database components, the storage system is one of the most power-hungry\nelements. In previous work, dynamic power management (DPM) techniques that make\nreal-time decisions to transition the disks to low-power modes are normally\nused to save energy in storage systems. In this paper, we tackle the\nlimitations of DPM proposals in previous contributions. We introduced a DPM\noptimization model integrated with model predictive control (MPC) strategy to\nminimize power consumption of the disk-based storage system while satisfying\ngiven performance requirements. It dynamically determines the state of disks\nand plans for inter-disk data fragment migration to achieve desirable balance\nbetween power consumption and query response time. Via analyzing our\noptimization model to identify structural properties of optimal solutions, we\npropose a fast-solution heuristic DPM algorithm that can be integrated in\nlarge-scale disk storage systems for efficient state configuration and data\nmigration. We evaluate our proposed ideas by running simulations using\nextensive set of synthetic workloads based on popular TPC benchmarks. Our\nresults show that our solution significantly outperforms the best existing\nalgorithm in both energy savings and response time.\n"]},
{"authors": ["Silu Huang", "Liqi Xu", "Jialin Liu", "Aaron Elmore", "Aditya Parameswaran"], "title": ["OrpheusDB: Bolt-on Versioning for Relational Databases"], "date": ["2017-03-07T17:09:13Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.02475v1"], "summary": ["  Data science teams often collaboratively analyze datasets, generating dataset\nversions at each stage of iterative exploration and analysis. There is a\npressing need for a system that can support dataset versioning, enabling such\nteams to efficiently store, track, and query across dataset versions. We\nintroduce OrpheusDB, a dataset version control system that \"bolts on\"\nversioning capabilities to a traditional relational database system, thereby\ngaining the analytics capabilities of the database \"for free\". We develop and\nevaluate multiple data models for representing versioned data, as well as a\nlight-weight partitioning scheme, LyreSplit, to further optimize the models for\nreduced query latencies. With LyreSplit, OrpheusDB is on average 1000x faster\nin finding effective (and better) partitionings than competing approaches,\nwhile also reducing the latency of version retrieval by up to 20x relative to\nschemes without partitioning. LyreSplit can be applied in an online fashion as\nnew versions are added, alongside an intelligent migration scheme that reduces\nmigration time by 10x on average.\n"]},
{"authors": ["Mehdi Naseriparsa", "Md. Saiful Islam", "Chengfei Liu", "Irene Moser"], "title": ["No-But-Semantic-Match: Computing Semantically Matched XML Keyword Search\n  Results"], "date": ["2017-03-07T04:54:44Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.02212v1"], "summary": ["  Users are rarely familiar with the content of a data source they are\nquerying, and therefore cannot avoid using keywords that do not exist in the\ndata source. Traditional systems may respond with an empty result, causing\ndissatisfaction, while the data source in effect holds semantically related\ncontent. In this paper we study this no-but-semantic-match problem on XML\nkeyword search and propose a solution which enables us to present the top-k\nsemantically related results to the user. Our solution involves two steps: (a)\nextracting semantically related candidate queries from the original query and\n(b) processing candidate queries and retrieving the top-k semantically related\nresults. Candidate queries are generated by replacement of non-mapped keywords\nwith candidate keywords obtained from an ontological knowledge base. Candidate\nresults are scored using their cohesiveness and their similarity to the\noriginal query. Since the number of queries to process can be large, with each\nresult having to be analyzed, we propose pruning techniques to retrieve the\ntop-$k$ results efficiently. We develop two query processing algorithms based\non our pruning techniques. Further, we exploit a property of the candidate\nqueries to propose a technique for processing multiple queries in batch, which\nimproves the performance substantially. Extensive experiments on two real\ndatasets verify the effectiveness and efficiency of the proposed approaches.\n"]},
{"authors": ["Johes Bater", "Gregory Elliott", "Craig Eggen", "Satyender Goel", "Abel Kho", "Jennie Rogers"], "title": ["SMCQL: Secure Querying for Federated Databases"], "date": ["2016-06-22T02:45:39Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1606.06808v5"], "summary": ["  People and machines are collecting data at an unprecedented rate. Despite\nthis newfound abundance of data, progress has been slow in sharing it for open\nscience, business, and other data-intensive endeavors. Many such efforts are\nstymied by privacy concerns and regulatory compliance issues. For example, many\nhospitals are interested in pooling their medical records for research, but\nnone may disclose arbitrary patient records to researchers or other healthcare\nproviders. In this context we propose the Private Data Network (PDN), a\nfederated database for querying over the collective data of mutually\ndistrustful parties. In a PDN, each member database does not reveal its tuples\nto its peers nor to the query writer. Instead, the user submits a query to an\nhonest broker that plans and coordinates its execution over multiple private\ndatabases using secure multiparty computation (SMC). Here, each database's\nquery execution is oblivious, and its program counters and memory traces are\nagnostic to the inputs of others. We introduce a framework for executing PDN\nqueries named SMCQL. This system translates SQL statements into SMC primitives\nto compute query results over the union of its source databases without\nrevealing sensitive information about individual tuples to peer data providers\nor the honest broker. Only the honest broker and the querier receive the\nresults of a PDN query. For fast, secure query evaluation, we explore a\nheuristics-driven optimizer that minimizes the PDN's use of secure computation\nand partitions its query evaluation into scalable slices.\n"]},
{"authors": ["Andr\u00e9 Petermann", "Martin Junghanns", "Erhard Rahm"], "title": ["DIMSpan - Transactional Frequent Subgraph Mining with Distributed\n  In-Memory Dataflow Systems"], "date": ["2017-03-06T14:57:03Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.01910v1"], "summary": ["  Transactional frequent subgraph mining identifies frequent subgraphs in a\ncollection of graphs. This research problem has wide applicability and\nincreasingly requires higher scalability over single machine solutions to\naddress the needs of Big Data use cases. We introduce DIMSpan, an advanced\napproach to frequent subgraph mining that utilizes the features provided by\ndistributed in-memory dataflow systems such as Apache Spark or Apache Flink. It\ndetermines the complete set of frequent subgraphs from arbitrary string-labeled\ndirected multigraphs as they occur in social, business and knowledge networks.\nDIMSpan is optimized to runtime and minimal network traffic but memory-aware.\nAn extensive performance evaluation on large graph collections shows the\nscalability of DIMSpan and the effectiveness of its pruning and optimization\ntechniques.\n"]},
{"authors": ["Charles H. Goonetilleke", "J. Wenny Rahayu", "Md. Saiful Islam"], "title": ["Frequent Query Matching in Dynamic Data Warehousing"], "date": ["2017-03-06T05:14:07Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.01727v1"], "summary": ["  With the need for flexible and on-demand decision support, Dynamic Data\nWarehouses (DDW) provide benefits over traditional data warehouses due to their\ndynamic characteristics in structuring and access mechanism. A DDW is a data\nframework that accommodates data source changes easily to allow seamless\nquerying to users. Materialized Views (MV) are proven to be an effective\nmethodology to enhance the process of retrieving data from a DDW as results are\npre-computed and stored in it. However, due to the static nature of\nmaterialized views, the level of dynamicity that can be provided at the MV\naccess layer is restricted. As a result, the collection of materialized views\nis not compatible with ever-changing reporting requirements. It is important\nthat the MV collection is consistent with current and upcoming queries. The\nsolution to the above problem must consider the following aspects: (a) MV must\nbe matched against an OLAP query in order to recognize whether the MV can\nanswer the query, (b) enable scalability in the MV collection, an intuitive\nmechanism to prune it and retrieve closely matching MVs must be incorporated,\n(c) MV collection must be able to evolve in correspondence to the regularly\nchanging user query patterns. Therefore, the primary objective of this paper is\nto explore these aspects and provide a well-rounded solution for the MV access\nlayer to remove the mismatch between the MV collection and reporting\nrequirements. Our contribution to solve the problem includes a Query Matching\nTechnique, a Domain Matching Technique and Maintenance of the MV collection. We\ndeveloped an experimental platform using real data-sets to evaluate the\neffectiveness in terms of performance and precision of the proposed techniques.\n"]},
{"authors": ["William W. Agresti"], "title": ["Defining Domain-Independent Discovery Informatics"], "date": ["2017-03-03T18:17:53Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.01298v1"], "summary": ["  This paper presents a personal account of the early legacy of discovery\ninformatics, especially surrounding the first published definition of\ndomain-independent DI. The state of DI is traced across various reference\nsources and the literature on the fourth paradigm of the scientific method.\nObservations are offered on DI, concluding that it will retain its appeal as a\nhighly apt descriptor for research and practice activities that are inherent in\nour human nature.\n"]},
{"authors": ["Jian Dai", "Fei He", "Wang-Chien Lee", "Gang Chen", "Beng Chin Ooi"], "title": ["DTNC: A New Server-side Data Cleansing Framework for Cellular Trajectory\n  Services"], "date": ["2017-03-01T03:41:40Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1703.00123v2"], "summary": ["  It is essential for the cellular network operators to provide cellular\nlocation services to meet the needs of their users and mobile applications.\nHowever, cellular locations, estimated by network-based methods at the\nserver-side, bear with {\\it high spatial errors} and {\\it arbitrary missing\nlocations}. Moreover, auxiliary sensor data at the client-side are not\navailable to the operators. In this paper, we study the {\\em cellular\ntrajectory cleansing problem} and propose an innovative data cleansing\nframework, namely \\underline{D}ynamic \\underline{T}ransportation\n\\underline{N}etwork based \\underline{C}leansing (DTNC) to improve the quality\nof cellular locations delivered in online cellular trajectory services. We\nmaintain a dynamic transportation network (DTN), which associates a network\nedge with a probabilistic distribution of travel times updated continuously. In\naddition, we devise an object motion model, namely, {\\em travel-time-aware\nhidden semi-Markov model} ({\\em TT-HsMM}), which is used to infer the most\nprobable traveled edge sequences on DTN. To validate our ideas, we conduct a\ncomprehensive evaluation using real-world cellular data provided by a major\ncellular network operator and a GPS dataset collected by smartphones as the\nground truth. In the experiments, DTNC displays significant advantages over six\nstate-of-the-art techniques.\n"]},
{"authors": ["Ahmet Iscen", "Teddy Furon", "Vincent Gripon", "Michael Rabbat", "Herv\u00e9 J\u00e9gou"], "title": ["Memory vectors for similarity search in high-dimensional spaces"], "date": ["2014-12-10T14:56:41Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1412.3328v7"], "summary": ["  We study an indexing architecture to store and search in a database of\nhigh-dimensional vectors from the perspective of statistical signal processing\nand decision theory. This architecture is composed of several memory units,\neach of which summarizes a fraction of the database by a single representative\nvector. The potential similarity of the query to one of the vectors stored in\nthe memory unit is gauged by a simple correlation with the memory unit's\nrepresentative vector. This representative optimizes the test of the following\nhypothesis: the query is independent from any vector in the memory unit vs. the\nquery is a simple perturbation of one of the stored vectors.\n  Compared to exhaustive search, our approach finds the most similar database\nvectors significantly faster without a noticeable reduction in search quality.\nInterestingly, the reduction of complexity is provably better in\nhigh-dimensional spaces. We empirically demonstrate its practical interest in a\nlarge-scale image search scenario with off-the-shelf state-of-the-art\ndescriptors.\n"]},
{"authors": ["Vladimir Dzyuba", "Matthijs van Leeuwen", "Luc De Raedt"], "title": ["Flexible constrained sampling with guarantees for pattern mining"], "date": ["2016-10-28T15:21:53Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1610.09263v2"], "summary": ["  Pattern sampling has been proposed as a potential solution to the infamous\npattern explosion. Instead of enumerating all patterns that satisfy the\nconstraints, individual patterns are sampled proportional to a given quality\nmeasure. Several sampling algorithms have been proposed, but each of them has\nits limitations when it comes to 1) flexibility in terms of quality measures\nand constraints that can be used, and/or 2) guarantees with respect to sampling\naccuracy. We therefore present Flexics, the first flexible pattern sampler that\nsupports a broad class of quality measures and constraints, while providing\nstrong guarantees regarding sampling accuracy. To achieve this, we leverage the\nperspective on pattern mining as a constraint satisfaction problem and build\nupon the latest advances in sampling solutions in SAT as well as existing\npattern mining algorithms. Furthermore, the proposed algorithm is applicable to\na variety of pattern languages, which allows us to introduce and tackle the\nnovel task of sampling sets of patterns. We introduce and empirically evaluate\ntwo variants of Flexics: 1) a generic variant that addresses the well-known\nitemset sampling task and the novel pattern set sampling task as well as a wide\nrange of expressive constraints within these tasks, and 2) a specialized\nvariant that exploits existing frequent itemset techniques to achieve\nsubstantial speed-ups. Experiments show that Flexics is both accurate and\nefficient, making it a useful tool for pattern-based data exploration.\n"]},
{"authors": ["Fengan Li", "Lingjiao Chen", "Arun Kumar", "Jeffrey F. Naughton", "Jignesh M. Patel", "Xi Wu"], "title": ["When Lempel-Ziv-Welch Meets Machine Learning: A Case Study of\n  Accelerating Machine Learning using Coding"], "date": ["2017-02-22T18:58:25Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.06943v2"], "summary": ["  In this paper we study the use of coding techniques to accelerate machine\nlearning (ML). Coding techniques, such as prefix codes, have been extensively\nstudied and used to accelerate low-level data processing primitives such as\nscans in a relational database system. However, there is little work on how to\nexploit them to accelerate ML algorithms. In fact, applying coding techniques\nfor faster ML faces a unique challenge: one needs to consider both how the\ncodes fit into the optimization algorithm used to train a model, and the\ninterplay between the model structure and the coding scheme. Surprisingly and\nintriguingly, our study demonstrates that a slight variant of the classical\nLempel-Ziv-Welch (LZW) coding scheme is a good fit for several popular ML\nalgorithms, resulting in substantial runtime savings. Comprehensive experiments\non several real-world datasets show that our LZW-based ML algorithms exhibit\nspeedups of up to 31x compared to a popular and state-of-the-art ML library,\nwith no changes to ML accuracy, even though the implementations of our LZW\nvariants are not heavily tuned. Thus, our study reveals a new avenue for\naccelerating ML algorithms using coding techniques and we hope this opens up a\nnew direction for more research.\n"]},
{"authors": ["Christoph Berkholz", "Jens Keppeler", "Nicole Schweikardt"], "title": ["Answering FO+MOD queries under updates on bounded degree databases"], "date": ["2017-02-28T12:46:03Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.08764v1"], "summary": ["  We investigate the query evaluation problem for fixed queries over fully\ndynamic databases, where tuples can be inserted or deleted. The task is to\ndesign a dynamic algorithm that immediately reports the new result of a fixed\nquery after every database update. We consider queries in first-order logic\n(FO) and its extension with modulo-counting quantifiers (FO+MOD), and show that\nthey can be efficiently evaluated under updates, provided that the dynamic\ndatabase does not exceed a certain degree bound.\n  In particular, we construct a data structure that allows to answer a Boolean\nFO+MOD query and to compute the size of the result of a non-Boolean query\nwithin constant time after every database update. Furthermore, after every\nupdate we are able to immediately enumerate the new query result with constant\ndelay between the output tuples. The time needed to build the data structure is\nlinear in the size of the database. Our results extend earlier work on the\nevaluation of first-order queries on static databases of bounded degree and\nrely on an effective Hanf normal form for FO+MOD recently obtained by Heimberg,\nKuske, and Schweikardt (LICS 2016).\n"]},
{"authors": ["Paulo J. L. Adeodato", "F\u00e1bio C. Pereira", "Rosalvo F. Oliveira Neto"], "title": ["Optimal Categorical Attribute Transformation for Granularity Change in\n  Relational Databases for Binary Decision Problems in Educational Data Mining"], "date": ["2017-02-28T11:13:17Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.08745v1"], "summary": ["  This paper presents an approach for transforming data granularity in\nhierarchical databases for binary decision problems by applying regression to\ncategorical attributes at the lower grain levels. Attributes from a lower\nhierarchy entity in the relational database have their information content\noptimized through regression on the categories histogram trained on a small\nexclusive labelled sample, instead of the usual mode category of the\ndistribution. The paper validates the approach on a binary decision task for\nassessing the quality of secondary schools focusing on how logistic regression\ntransforms the students and teachers attributes into school attributes.\nExperiments were carried out on Brazilian schools public datasets via 10-fold\ncross-validation comparison of the ranking score produced also by logistic\nregression. The proposed approach achieved higher performance than the usual\ndistribution mode transformation and equal to the expert weighing approach\nmeasured by the maximum Kolmogorov-Smirnov distance and the area under the ROC\ncurve at 0.01 significance level.\n"]},
{"authors": ["Jeff Johnson", "Matthijs Douze", "Herv\u00e9 J\u00e9gou"], "title": ["Billion-scale similarity search with GPUs"], "date": ["2017-02-28T10:42:31Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.08734v1"], "summary": ["  Similarity search finds application in specialized database systems handling\ncomplex data such as images or videos, which are typically represented by\nhigh-dimensional features and require specific indexing structures. This paper\ntackles the problem of better utilizing GPUs for this task. While GPUs excel at\ndata-parallel tasks, prior approaches are bottlenecked by algorithms that\nexpose less parallelism, such as k-min selection, or make poor use of the\nmemory hierarchy.\n  We propose a design for k-selection that operates at up to 55% of theoretical\npeak performance, enabling a nearest neighbor implementation that is 8.5x\nfaster than prior GPU state of the art. We apply it in different similarity\nsearch scenarios, by proposing optimized design for brute-force, approximate\nand compressed-domain search based on product quantization. In all these\nsetups, we outperform the state of the art by large margins. Our implementation\nenables the construction of a high accuracy k-NN graph on 95 million images\nfrom the Yfcc100M dataset in 35 minutes, and of a graph connecting 1 billion\nvectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced\nour approach for the sake of comparison and reproducibility.\n"]},
{"authors": ["Nizar Massouh", "Francesca Babiloni", "Tatiana Tommasi", "Jay Young", "Nick Hawes", "Barbara Caputo"], "title": ["Learning Deep Visual Object Models From Noisy Web Data: How to Make it\n  Work"], "date": ["2017-02-28T10:02:36Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.08513v1"], "summary": ["  Deep networks thrive when trained on large scale data collections. This has\ngiven ImageNet a central role in the development of deep architectures for\nvisual object classification. However, ImageNet was created during a specific\nperiod in time, and as such it is prone to aging, as well as dataset bias\nissues. Moving beyond fixed training datasets will lead to more robust visual\nsystems, especially when deployed on robots in new environments which must\ntrain on the objects they encounter there. To make this possible, it is\nimportant to break free from the need for manual annotators. Recent work has\nbegun to investigate how to use the massive amount of images available on the\nWeb in place of manual image annotations. We contribute to this research thread\nwith two findings: (1) a study correlating a given level of noisily labels to\nthe expected drop in accuracy, for two deep architectures, on two different\ntypes of noise, that clearly identifies GoogLeNet as a suitable architecture\nfor learning from Web data; (2) a recipe for the creation of Web datasets with\nminimal noise and maximum visual variability, based on a visual and natural\nlanguage processing concept expansion strategy. By combining these two results,\nwe obtain a method for learning powerful deep object models automatically from\nthe Web. We confirm the effectiveness of our approach through object\ncategorization experiments using our Web-derived version of ImageNet on a\npopular robot vision benchmark database, and on a lifelong object discovery\ntask on a mobile robot.\n"]},
{"authors": ["Clark C. Evans", "Kyrylo Simonov"], "title": ["Query Combinators"], "date": ["2017-02-27T18:11:07Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.08409v1"], "summary": ["  We introduce Rabbit, a combinator-based query language. Rabbit is designed to\nlet data analysts and other accidental programmers query complex structured\ndata.\n  We combine the functional data model and the categorical semantics of\ncomputations to develop denotational semantics of database queries. In Rabbit,\na query is modeled as a Kleisli arrow for a monadic container determined by the\nquery cardinality. In this model, monadic composition can be used to navigate\nthe database, while other query combinators can aggregate, filter, sort and\npaginate data; construct compound data; connect self-referential data; and\nreorganize data with grouping and data cube operations. A context-aware query\nmodel, with the input context represented as a comonadic container, can express\nquery parameters and window functions. Rabbit semantics enables pipeline\nnotation, encouraging its users to construct database queries as a series of\ndistinct steps, each individually crafted and tested. We believe that Rabbit\ncan serve as a practical tool for data analytics.\n"]},
{"authors": ["Jonathan A. Marshall", "Lawrence C. Rafsky"], "title": ["Exact clustering in linear time"], "date": ["2017-02-17T16:44:21Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.05425v2"], "summary": ["  The time complexity of data clustering has been viewed as fundamentally\nquadratic, slowing with the number of data items, as each item is compared for\nsimilarity to preceding items. Clustering of large data sets has been\ninfeasible without resorting to probabilistic methods or to capping the number\nof clusters. Here we introduce MIMOSA, a novel class of algorithms which\nachieve linear time computational complexity on clustering tasks. MIMOSA\nalgorithms mark and match partial-signature keys in a hash table to obtain\nexact, error-free cluster retrieval. Benchmark measurements, on clustering a\ndata set of 10,000,000 news articles by news topic, found that a MIMOSA\nimplementation finished more than four orders of magnitude faster than a\nstandard centroid implementation.\n"]},
{"authors": ["Haoyuan Xing", "Sofoklis Floratos", "Spyros Blanas", "Suren Byna", " Prabhat", "Kesheng Wu", "Paul Brown"], "title": ["ArrayBridge: Interweaving declarative array processing with\n  high-performance computing"], "date": ["2017-02-27T15:21:45Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.08327v1"], "summary": ["  Scientists are increasingly turning to datacenter-scale computers to produce\nand analyze massive arrays. Despite decades of database research that extols\nthe virtues of declarative query processing, scientists still write, debug and\nparallelize imperative HPC kernels even for the most mundane queries. This\nimpedance mismatch has been partly attributed to the cumbersome data loading\nprocess; in response, the database community has proposed in situ mechanisms to\naccess data in scientific file formats. Scientists, however, desire more than a\npassive access method that reads arrays from files.\n  This paper describes ArrayBridge, a bi-directional array view mechanism for\nscientific file formats, that aims to make declarative array manipulations\ninteroperable with imperative file-centric analyses. Our prototype\nimplementation of ArrayBridge uses HDF5 as the underlying array storage library\nand seamlessly integrates into the SciDB open-source array database system. In\naddition to fast querying over external array objects, ArrayBridge produces\narrays in the HDF5 file format just as easily as it can read from it.\nArrayBridge also supports time travel queries from imperative kernels through\nthe unmodified HDF5 API, and automatically deduplicates between array versions\nfor space efficiency. Our extensive performance evaluation in NERSC, a\nlarge-scale scientific computing facility, shows that ArrayBridge exhibits\nstatistically indistinguishable performance and I/O scalability to the native\nSciDB storage engine.\n"]},
{"authors": ["Sylvain Hall\u00e9"], "title": ["From Complex Event Processing to Simple Event Processing"], "date": ["2017-02-26T16:51:28Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.08051v1"], "summary": ["  Many problems in Computer Science can be framed as the computation of queries\nover sequences, or \"streams\" of data units called events. The field of Complex\nEvent Processing (CEP) relates to the techniques and tools developed to\nefficiently process these queries. However, most CEP systems developed so far\nhave concentrated on relatively narrow types of queries, which consist of\nsliding windows, aggregation functions, and simple sequential patterns computed\nover events that have a fixed tuple structure. Many of them boast throughput,\nbut in counterpart, they are difficult to setup and cumbersome to extend with\nuser-defined elements.\n  This paper describes a variety of use cases taken from real-world scenarios\nthat present features seldom considered in classical CEP problems. It also\nprovides a broad review of current solutions, that includes tools and\ntechniques going beyond typical surveys on CEP. From a critical analysis of\nthese solutions, design principles for a new type of event stream processing\nsystem are exposed. The paper proposes a simple, generic and extensible\nframework for the processing of event streams of diverse types; it describes in\ndetail a stream processing engine, called BeepBeep, that implements these\nprinciples. BeepBeep's modular architecture, which borrows concepts from many\nother systems, is complemented with an extensible query language, called eSQL.\nThe end result is an open, versatile, and reasonably efficient query engine\nthat can be used in situations that go beyond the capabilities of existing\nsystems.\n"]},
{"authors": ["Caetano Sauer", "Goetz Graefe", "Theo H\u00e4rder"], "title": ["Instant restore after a media failure"], "date": ["2017-02-26T14:58:04Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.08042v1"], "summary": ["  Media failures usually leave database systems unavailable for several hours\nuntil recovery is complete, especially in applications with large devices and\nhigh transaction volume. Previous work introduced a technique called\nsingle-pass restore, which increases restore bandwidth and thus substantially\ndecreases time to repair. Instant restore goes further as it permits read/write\naccess to any data on a device undergoing restore--even data not yet\nrestored--by restoring individual data segments on demand. Thus, the restore\nprocess is guided primarily by the needs of applications, and the observed mean\ntime to repair is effectively reduced from several hours to a few seconds.\n  This paper presents an implementation and evaluation of instant restore. The\ntechnique is incrementally implemented on a system starting with the\ntraditional ARIES design for logging and recovery. Experiments show that the\ntransaction latency perceived after a media failure can be cut down to less\nthan a second and that the overhead imposed by the technique on normal\nprocessing is minimal. The net effect is that a few \"nines\" of availability are\nadded to the system using simple and low-overhead software techniques.\n"]},
{"authors": ["Mohammad Sadoghi", "Souvik Bhattacherjee", "Bishwaranjan Bhattacharjee", "Mustafa Canim"], "title": ["L-Store: A Real-time OLTP and OLAP System"], "date": ["2016-01-15T21:44:39Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1601.04084v2"], "summary": ["  Arguably data is the new natural resource in the enterprise world with an\nunprecedented degree of proliferation. But to derive real-time actionable\ninsights from the data, it is important to bridge the gap between managing the\ndata that is being updated at a high velocity (i.e., OLTP) and analyzing a\nlarge volume of data (i.e., OLAP). However, there has been a divide where\nspecialized solutions were often deployed to support either OLTP or OLAP\nworkloads but not both; thus, limiting the analysis to stale and possibly\nirrelevant data. In this paper, we present Lineage-based Data Store (L-Store)\nthat combines the real-time processing of transactional and analytical\nworkloads within a single unified engine by introducing a novel lineage-based\nstorage architecture. By exploiting the lineage, we develop a contention-free\nand lazy staging of columnar data from a write-optimized form (suitable for\nOLTP) into a read-optimized form (suitable for OLAP) in a transactionally\nconsistent approach that also supports querying and retaining the current and\nhistoric data. Our working prototype of L-Store demonstrates its superiority\ncompared to state-of-the-art approaches under a comprehensive experimental\nevaluation.\n"]},
{"authors": ["Zeinab Bahmani", "Leopoldo Bertossi"], "title": ["Enforcing Relational Matching Dependencies with Datalog for Entity\n  Resolution"], "date": ["2016-11-21T19:02:19Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1611.06951v2"], "summary": ["  Entity resolution (ER) is about identifying and merging records in a database\nthat represent the same real-world entity. Matching dependencies (MDs) have\nbeen introduced and investigated as declarative rules that specify ER policies.\nAn ER process induced by MDs over a dirty instance leads to multiple clean\ninstances, in general. General \"answer sets programs\" have been proposed to\nspecify the MD-based cleaning task and its results. In this work, we extend MDs\nto \"relational MDs\", which capture more application semantics, and identify\nclasses of relational MDs for which the general ASP can be automatically\nrewritten into a stratified Datalog program, with the single clean instance as\nits standard model.\n"]},
{"authors": ["Riccardo Guidotti", "Giulio Rossetti", "Luca Pappalardo", "Fosca Giannotti", "Dino Pedreschi"], "title": ["Next Basket Prediction using Recurring Sequential Patterns"], "date": ["2017-02-23T10:12:40Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.07158v1"], "summary": ["  Nowadays, a hot challenge for supermarket chains is to offer personalized\nservices for their customers. Next basket prediction, i.e., supplying the\ncustomer a shopping list for the next purchase according to her current needs,\nis one of these services. Current approaches are not capable to capture at the\nsame time the different factors influencing the customer's decision process:\nco-occurrency, sequentuality, periodicity and recurrency of the purchased\nitems. To this aim, we define a pattern Temporal Annotated Recurring Sequence\n(TARS) able to capture simultaneously and adaptively all these factors. We\ndefine the method to extract TARS and develop a predictor for next basket named\nTBP (TARS Based Predictor) that, on top of TARS, is able to to understand the\nlevel of the customer's stocks and recommend the set of most necessary items.\nBy adopting the TBP the supermarket chains could crop tailored suggestions for\neach individual customer which in turn could effectively speed up their\nshopping sessions. A deep experimentation shows that TARS are able to explain\nthe customer purchase behavior, and that TBP outperforms the state-of-the-art\ncompetitors.\n"]},
{"authors": ["Ruoxi Shi", "Hongzhi Wang", "Tao Wang", "Yutai Hou", "Yiwen Tang"], "title": ["Similarity Search Combining Query Relaxation and Diversification"], "date": ["2016-11-15T03:26:19Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1611.04689v2"], "summary": ["  We study the similarity search problem which aims to find the similar query\nresults according to a set of given data and a query string. To balance the\nresult number and result quality, we combine query result diversity with query\nrelaxation. Relaxation guarantees the number of the query results, returning\nmore relevant elements to the query if the results are too few, while the\ndiversity tries to reduce the similarity among the returned results. By making\na trade-off of similarity and diversity, we improve the user experience. To\nachieve this goal, we define a novel goal function combining similarity and\ndiversity. Aiming at this goal, we propose three algorithms. Among them,\nalgorithms genGreedy and genCluster perform relaxation first and select part of\nthe candidates to diversify. The third algorithm CB2S splits the dataset into\nsmaller pieces using the clustering algorithm of k-means and processes queries\nin several small sets to retrieve more diverse results. The balance of\nsimilarity and diversity is determined through setting a threshold, which has a\ndefault value and can be adjusted according to users' preference. The\nperformance and efficiency of our system are demonstrated through extensive\nexperiments based on various datasets.\n"]},
{"authors": ["Georgios Drakopoulos", "Andreas Kanavos", "Christos Makris", "Vasileios Megalooikonomou"], "title": ["The Storage And Analytics Potential Of HBase Over The Cloud: A Survey"], "date": ["2016-08-02T12:57:52Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1608.00796v4"], "summary": ["  Apache HBase, a mainstay of the emerging Hadoop ecosystem, is a NoSQL\nkey-value and column family hybrid database which, unlike a traditional RDBMS,\nis intentionally designed to scalably host large, semistructured, and\nheterogeneous data. Prime examples of such data are biosignals which are\ncharacterized by large volume, high volatility, and inherent\nmultidimensionality. This paper reviews how biomedical engineering has recently\ntaken advantage of HBase, with an emphasis over cloud, in order to reliably\nhost cardiovascular and respiratory time series. Moreover, the deployment of\noffline biomedical analytics over HBase is explored.\n"]},
{"authors": ["Georgios Drakopoulos", "Vasileios Megalooikonomou"], "title": ["A Graph Framework for Multimodal Medical Information Processing"], "date": ["2016-07-30T15:42:34Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1608.00134v2"], "summary": ["  Multimodal medical information processing is currently the epicenter of\nintense interdisciplinary research, as proper data fusion may lead to more\naccurate diagnoses. Moreover, multimodality may disambiguate cases of\nco-morbidity. This paper presents a framework for retrieving, analyzing, and\nstoring medical information as a multilayer graph, an abstract format suitable\nfor data fusion and further processing. At the same time, this paper addresses\nthe need for reliable medical information through co-author graph ranking. A\nuse case pertaining to frailty based on Python and Neo4j serves as an\nillustration of the proposed framework.\n"]},
{"authors": ["Elias Alevizos", "Anastasios Skarlatidis", "Alexander Artikis", "George Paliouras"], "title": ["Probabilistic Complex Event Recognition: A Survey"], "date": ["2017-02-21T13:41:35Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.06379v1"], "summary": ["  Complex Event Recognition applications exhibit various types of uncertainty,\nranging from incomplete and erroneous data streams to imperfect complex event\npatterns. We review Complex Event Recognition techniques that handle, to some\nextent, uncertainty. We examine techniques based on automata, probabilistic\ngraphical models and first-order logic, which are the most common ones, and\napproaches based on Petri Nets and Grammars, which are less frequently used. A\nnumber of limitations are identified with respect to the employed languages,\ntheir probabilistic models and their performance, as compared to the purely\ndeterministic cases. Based on those limitations, we highlight promising\ndirections for future work.\n"]},
{"authors": ["Christoph Berkholz", "Jens Keppeler", "Nicole Schweikardt"], "title": ["Answering Conjunctive Queries under Updates"], "date": ["2017-02-21T13:15:27Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.06370v1"], "summary": ["  We consider the task of enumerating and counting answers to $k$-ary\nconjunctive queries against relational databases that may be updated by\ninserting or deleting tuples. We exhibit a new notion of q-hierarchical\nconjunctive queries and show that these can be maintained efficiently in the\nfollowing sense. During a linear time preprocessing phase, we can build a data\nstructure that enables constant delay enumeration of the query results; and\nwhen the database is updated, we can update the data structure and restart the\nenumeration phase within constant time. For the special case of self-join free\nconjunctive queries we obtain a dichotomy: if a query is not q-hierarchical,\nthen query enumeration with sublinear$^\\ast$ delay and sublinear update time\n(and arbitrary preprocessing time) is impossible.\n  For answering Boolean conjunctive queries and for the more general problem of\ncounting the number of solutions of k-ary queries we obtain complete\ndichotomies: if the query's homomorphic core is q-hierarchical, then size of\nthe the query result can be computed in linear time and maintained with\nconstant update time. Otherwise, the size of the query result cannot be\nmaintained with sublinear update time. All our lower bounds rely on the\nOMv-conjecture, a conjecture on the hardness of online matrix-vector\nmultiplication that has recently emerged in the field of fine-grained\ncomplexity to characterise the hardness of dynamic problems. The lower bound\nfor the counting problem additionally relies on the orthogonal vectors\nconjecture, which in turn is implied by the strong exponential time hypothesis.\n  $^\\ast)$ By sublinear we mean $O(n^{1-\\varepsilon})$ for some\n$\\varepsilon>0$, where $n$ is the size of the active domain of the current\ndatabase.\n"]},
{"authors": ["Md. Saiful Islam", "Wenny Rahayu", "Chengfei Liu", "Tarique Anwar", "Bela Stantic"], "title": ["Computing Influence of a Product through Uncertain Reverse Skyline"], "date": ["2017-02-21T09:06:04Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.06298v1"], "summary": ["  Understanding the influence of a product is crucially important for making\ninformed business decisions. This paper introduces a new type of skyline\nqueries, called uncertain reverse skyline, for measuring the influence of a\nprobabilistic product in uncertain data settings. More specifically, given a\ndataset of probabilistic products P and a set of customers C, an uncertain\nreverse skyline of a probabilistic product q retrieves all customers c in C\nwhich include q as one of their preferred products. We present efficient\npruning ideas and techniques for processing the uncertain reverse skyline query\nof a probabilistic product using R-Tree data index. We also present an\nefficient parallel approach to compute the uncertain reverse skyline and\ninfluence score of a probabilistic product. Our approach significantly\noutperforms the baseline approach derived from the existing literature. The\nefficiency of our approach is demonstrated by conducting extensive experiments\nwith both real and synthetic datasets.\n"]},
{"authors": ["Sudhakar Singh", "Rakhi Garg", "P. K. Mishra"], "title": ["Review of Apriori Based Algorithms on MapReduce Framework"], "date": ["2017-02-21T07:34:06Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.06284v1"], "summary": ["  The Apriori algorithm that mines frequent itemsets is one of the most popular\nand widely used data mining algorithms. Now days many algorithms have been\nproposed on parallel and distributed platforms to enhance the performance of\nApriori algorithm. They differ from each other on the basis of load balancing\ntechnique, memory system, data decomposition technique and data layout used to\nimplement them. The problems with most of the distributed framework are\noverheads of managing distributed system and lack of high level parallel\nprogramming language. Also with grid computing there is always potential\nchances of node failures which cause multiple re-executions of tasks. These\nproblems can be overcome by the MapReduce framework introduced by Google.\nMapReduce is an efficient, scalable and simplified programming model for large\nscale distributed data processing on a large cluster of commodity computers and\nalso used in cloud computing. In this paper, we present the overview of\nparallel Apriori algorithm implemented on MapReduce framework. They are\ncategorized on the basis of Map and Reduce functions used to implement them\ne.g. 1-phase vs. k-phase, I/O of Mapper, Combiner and Reducer, using\nfunctionality of Combiner inside Mapper etc. This survey discusses and analyzes\nthe various implementations of Apriori on MapReduce framework on the basis of\ntheir distinguishing characteristics. Moreover, it also includes the advantages\nand limitations of MapReduce framework.\n"]},
{"authors": ["Peng Cheng", "Xiang Lian", "Lei Chen", "Cyrus Shahabi"], "title": ["Prediction-Based Task Assignment in Spatial Crowdsourcing (Technical\n  Report)"], "date": ["2015-12-27T15:17:00Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1512.08518v5"], "summary": ["  Spatial crowdsourcing refers to a system that periodically assigns a number\nof location-based workers with spatial tasks nearby (e.g., taking photos or\nvideos at some spatial locations). Previous works on the spatial crowdsourcing\nusually designed task assignment strategies that maximize some assignment\nscores, which are however only based on available workers/tasks in the system\nat the time point of assigning workers/tasks. These strategies may achieve\nlocal optimality, due to the neglect of future workers/tasks that may join the\nsystem. In contrast, in this paper, we aim to achieve \"globally\" optimal task\nassignments, by considering not only those present, but also future (via\npredictions), workers/tasks. Specifically, we formalize an important problem,\nnamely prediction-based spatial crowdsourcing (PB-SC), which expects to obtain\na \"globally\" optimal strategy for worker-and-task assignments, over both\npresent and predicted task/worker locations, such that the total assignment\nquality score is maximized under the constraint of the traveling budget. In\nthis paper, we design an effective grid-based prediction method to estimate\nspatial distributions of workers/tasks in the future, and then utilize the\npredicted ones in our procedure of task assignments. We prove that the PB-SC\nproblem is NP-hard, and thus intractable. Therefore, we propose efficient\napproximate algorithms to tackle the PB-SC problem, including greedy and\ndivide-and-conquer (D&C) approaches, which can efficiently assign workers to\nspatial tasks with high quality scores and low budget consumptions, by\nconsidering both current and future task/worker distributions. Through\nextensive experiments, we demonstrate the efficiency and effectiveness of our\nPB-SC processing approaches on real/synthetic data.\n"]},
{"authors": ["Maryam Fanaeepour", "Benjamin I. P. Rubinstein"], "title": ["End-to-End Differentially-Private Parameter Tuning in Spatial Histograms"], "date": ["2017-02-18T12:43:46Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.05607v1"], "summary": ["  Differentially-private histograms have emerged as a key tool for location\nprivacy. While past mechanisms have included theoretical & experimental\nanalysis, it has recently been observed that much of the existing literature\ndoes not fully provide differential privacy. The missing component, private\nparameter tuning, is necessary for rigorous evaluation of these mechanisms.\nInstead works frequently tune on training data to optimise parameters without\nconsideration of privacy; in other cases selection is performed arbitrarily and\nindependent of data, degrading utility. We address this open problem by\nderiving a principled tuning mechanism that privately optimises data-dependent\nerror bounds. Theoretical results establish privacy and utility while extensive\nexperimentation demonstrates that we can practically achieve true end-to-end\nprivacy.\n"]},
{"authors": ["Xuelian Lin", "Shuai Ma", "Han Zhang", "Tianyu Wo", "Jinpeng Huai"], "title": ["One-Pass Error Bounded Trajectory Simplification"], "date": ["2017-02-18T10:47:20Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.05597v1"], "summary": ["  Nowadays, various sensors are collecting, storing and transmitting tremendous\ntrajectory data, and it is known that raw trajectory data seriously wastes the\nstorage, network band and computing resource. Line simplification (LS)\nalgorithms are an effective approach to attacking this issue by compressing\ndata points in a trajectory to a set of continuous line segments, and are\ncommonly used in practice. However, existing LS algorithms are not sufficient\nfor the needs of sensors in mobile devices. In this study, we first develop a\none-pass error bounded trajectory simplification algorithm (OPERB), which scans\neach data point in a trajectory once and only once. We then propose an\naggressive one-pass error bounded trajectory simplification algorithm\n(OPERB-A), which allows interpolating new data points into a trajectory under\ncertain conditions. Finally, we experimentally verify that our approaches\n(OPERB and OPERB-A) are both efficient and effective, using four real-life\ntrajectory datasets.\n"]},
{"authors": ["Abdullah Alfarrarjeh", "Cyrus Shahabi"], "title": ["Hybrid Indexes to Expedite Spatial-Visual Search"], "date": ["2017-02-17T01:16:25Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.05200v1"], "summary": ["  Due to the growth of geo-tagged images, recent web and mobile applications\nprovide search capabilities for images that are similar to a given query image\nand simultaneously within a given geographical area. In this paper, we focus on\ndesigning index structures to expedite these spatial-visual searches. We start\nby baseline indexes that are straightforward extensions of the current popular\nspatial (R*-tree) and visual (LSH) index structures. Subsequently, we propose\nhybrid index structures that evaluate both spatial and visual features in\ntandem. The unique challenge of this type of query is that there are\ninaccuracies in both spatial and visual features. Therefore, different\ntraversals of the index structures may produce different images as output, some\nof which more relevant to the query than the others. We compare our hybrid\nstructures with a set of baseline indexes in both performance and result\naccuracy using three real world datasets from Flickr, Google Street View, and\nGeoUGV.\n"]},
{"authors": ["Arlei Silva", "Ambuj Singh", "Ananthram Swami"], "title": ["Spectral Algorithms for Temporal Graph Cuts"], "date": ["2017-02-15T19:37:22Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.04746v1"], "summary": ["  The sparsest cut problem consists of identifying a small set of edges that\nbreaks the graph into balanced sets of vertices. The normalized cut problem\nbalances the total degree, instead of the size, of the resulting sets.\nApplications of graph cuts include community detection and computer vision.\nHowever, cut problems were originally proposed for static graphs, an assumption\nthat does not hold in many modern applications where graphs are highly dynamic.\nIn this paper, we introduce the sparsest and normalized cut problems in\ntemporal graphs, which generalize their standard definitions by enforcing the\nsmoothness of cuts over time. We propose novel formulations and algorithms for\ncomputing temporal cuts using spectral graph theory, multiplex graphs,\ndivide-and-conquer and low-rank matrix approximation. Furthermore, we extend\nour formulation to dynamic graph signals, where cuts also capture node values,\nas graph wavelets. Experiments show that our solutions are accurate and\nscalable, enabling the discovery of dynamic communities and the analysis of\ndynamic graph processes.\n"]},
{"authors": ["Shenggang Ying", "Mingsheng Ying", "Yuan Feng"], "title": ["Quantum Privacy-Preserving Data Analytics"], "date": ["2017-02-14T23:23:49Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.04420v1"], "summary": ["  Data analytics (such as association rule mining and decision tree mining) can\ndiscover useful statistical knowledge from a big data set. But protecting the\nprivacy of the data provider and the data user in the process of analytics is a\nserious issue. Usually, the privacy of both parties cannot be fully protected\nsimultaneously by a classical algorithm. In this paper, we present a quantum\nprotocol for data mining that can much better protect privacy than the known\nclassical algorithms: (1) if both the data provider and the data user are\nhonest, the data user can know nothing about the database except the\nstatistical results, and the data provider can get nearly no information about\nthe results mined by the data user; (2) if the data user is dishonest and tries\nto disclose private information of the other, she/he will be detected with a\nhigh probability; (3) if the data provider tries to disclose the privacy of the\ndata user, she/he cannot get any useful information since the data user hides\nhis privacy among noises.\n"]},
{"authors": ["Niranjan Kamat", "Arnab Nandi"], "title": ["Perfect and Maximum Randomness in Stratified Sampling over Joins"], "date": ["2016-01-19T22:18:56Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1601.05118v2"], "summary": ["  Supporting sampling in the presence of joins is an important problem in data\nanalysis, but is inherently challenging due to the need to avoid correlation\nbetween output tuples. Current solutions provide either correlated or\nnon-correlated samples. Sampling might not always be feasible in the\nnon-correlated sampling-based approaches -- the sample size or intermediate\ndata size might be exceedingly large. On the other hand, a correlated sample\nmay not be representative of the join. This paper presents a \\emph{unified}\nstrategy towards join sampling, while considering sample correlation every step\nof the way. We provide two key contributions. First, in the case where a\n\\emph{correlated} sample is \\emph{acceptable}, we provide techniques, for all\njoin types, to sample base relations so that their join is \\emph{as random as\npossible}. Second, in the case where a correlated sample is \\emph{not\nacceptable}, we provide enhancements to the state-of-the-art algorithms to\nreduce their execution time and intermediate data size.\n"]},
{"authors": ["Xin Huang", "Laks V. S. Lakshmanan"], "title": ["Attribute Truss Community Search"], "date": ["2016-09-01T02:19:36Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1609.00090v3"], "summary": ["  Recently, community search over graphs has attracted significant attention\nand many algorithms have been developed for finding dense subgraphs from large\ngraphs that contain given query nodes. In applications such as analysis of\nprotein protein interaction (PPI) networks, citation graphs, and collaboration\nnetworks, nodes tend to have attributes. Unfortunately, previously developed\ncommunity search algorithms ignore these attributes and result in communities\nwith poor cohesion w.r.t. their node attributes. In this paper, we study the\nproblem of attribute-driven community search, that is, given an undirected\ngraph $G$ where nodes are associated with attributes, and an input query $Q$\nconsisting of nodes $V_q$ and attributes $W_q$, find the communities containing\n$V_q$, in which most community members are densely inter-connected and have\nsimilar attributes.\n  We formulate our problem of finding attributed truss communities (ATC), as\nfinding all connected and close k-truss subgraphs containing $V_q$, that are\nlocally maximal and have the largest attribute relevance score among such\nsubgraphs. We design a novel attribute relevance score function and establish\nits desirable properties. The problem is shown to be NP-hard. However, we\ndevelop an efficient greedy algorithmic framework, which finds a maximal\n$k$-truss containing $V_q$, and then iteratively removes the nodes with the\nleast popular attributes and shrinks the graph so as to satisfy community\nconstraints. We also build an elegant index to maintain the known $k$-truss\nstructure and attribute information, and propose efficient query processing\nalgorithms. Extensive experiments on large real-world networks with\nground-truth communities shows the efficiency and effectiveness of our proposed\nmethods.\n"]},
{"authors": ["Ezra N. Hoch", "Yaniv Ben-Yehuda", "Noam Lewis", "Avi Vigder"], "title": ["Bizur: A Key-value Consensus Algorithm for Scalable File-systems"], "date": ["2017-02-14T14:51:45Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.04242v1"], "summary": ["  Bizur is a consensus algorithm exposing a key-value interface. It is used by\na distributed file-system that scales to 100s of servers, delivering millions\nof IOPS, both data and metadata, with consistent low-latency.\n  Bizur is aimed for services that require strongly consistent state, but do\nnot require a distributed log; for example, a distributed lock manager or a\ndistributed service locator. By avoiding a distributed log scheme, Bizur\noutperforms distributed log based consensus algorithms, producing more IOPS and\nguaranteeing lower latencies during normal operation and especially during\nfailures.\n  Paxos-like algorithms (e.g., Zab and Raft) which are used by existing\ndistributed file-systems, can have artificial contention points due to their\ndependence on a distributed log. The distributed log is needed when replicating\na general service, but when the desired service is key-value based, the\ncontention points created by the distributed log can be avoided.\n  Bizur does exactly that, by reaching consensus independently on independent\nkeys. This independence allows Bizur to handle failures more efficiently and to\nscale much better than other consensus algorithms, allowing the file-system\nthat utilizes Bizur to scale with it.\n"]},
{"authors": ["Zeyi Wen", "Dong Deng", "Rui Zhang", "Kotagiri Ramamohanarao"], "title": ["A Technical Report: Entity Extraction using Both Character-based and\n  Token-based Similarity"], "date": ["2017-02-12T12:46:40Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.03519v1"], "summary": ["  Entity extraction is fundamental to many text mining tasks such as\norganisation name recognition. A popular approach to entity extraction is based\non matching sub-string candidates in a document against a dictionary of\nentities. To handle spelling errors and name variations of entities, usually\nthe matching is approximate and edit or Jaccard distance is used to measure\ndissimilarity between sub-string candidates and the entities. For approximate\nentity extraction from free text, existing work considers solely\ncharacter-based or solely token-based similarity and hence cannot\nsimultaneously deal with minor variations at token level and typos. In this\npaper, we address this problem by considering both character-based similarity\nand token-based similarity (i.e. two-level similarity). Measuring one-level\n(e.g. character-based) similarity is computationally expensive, and measuring\ntwo-level similarity is dramatically more expensive. By exploiting the\nproperties of the two-level similarity and the weights of tokens, we develop\nnovel techniques to significantly reduce the number of sub-string candidates\nthat require computation of two-level similarity against the dictionary of\nentities. A comprehensive experimental study on real world datasets show that\nour algorithm can efficiently extract entities from documents and produce a\nhigh F1 score in the range of [0.91, 0.97].\n"]},
{"authors": ["Jiaying Feng", "Xiaowang Zhang", "Zhiyong Feng"], "title": ["MapSQ: A MapReduce-based Framework for SPARQL Queries on GPU"], "date": ["2017-02-12T03:06:25Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.03484v1"], "summary": ["  In this paper, we present a MapReduce-based framework for evaluating SPARQL\nqueries on GPU (named MapSQ) to large-scale RDF datesets efficiently by\napplying both high performance. Firstly, we develop a MapReduce-based Join\nalgorithm to handle SPARQL queries in a parallel way. Secondly, we present a\ncoprocessing strategy to manage the process of evaluating queries where CPU is\nused to assigns subqueries and GPU is used to compute the join of subqueries.\nFinally, we implement our proposed framework and evaluate our proposal by\ncomparing with two popular and latest SPARQL query engines gStore and gStoreD\non the LUBM benchmark. The experiments demonstrate that our proposal MapSQ is\nhighly efficient and effective (up to 50% speedup).\n"]},
{"authors": ["Angelika Kimmig", "Alex Memory", "Renee J. Miller", "Lise Getoor"], "title": ["A Collective, Probabilistic Approach to Schema Mapping: Appendix"], "date": ["2017-02-11T19:18:41Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.03447v1"], "summary": ["  In this appendix we provide additional supplementary material to \"A\nCollective, Probabilistic Approach to Schema Mapping.\" We include an additional\nextended example, supplementary experiment details, and proof for the\ncomplexity result stated in the main paper.\n"]},
{"authors": ["Anuradha Awasthi", "Arnab Bhattacharya", "Sanchit Gupta", "Ujjwal Kumar Singh"], "title": ["K-Dominant Skyline Join Queries: Extending the Join Paradigm to\n  K-Dominant Skylines"], "date": ["2017-02-11T06:51:21Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.03390v1"], "summary": ["  Skyline queries enable multi-criteria optimization by filtering objects that\nare worse in all the attributes of interest than another object. To handle the\nlarge answer set of skyline queries in high-dimensional datasets, the concept\nof k-dominance was proposed where an object is said to dominate another object\nif it is better (or equal) in at least k attributes. This relaxes the full\ndomination criterion of normal skyline queries and, therefore, produces lesser\nnumber of skyline objects. This is called the k-dominant skyline set. Many\npractical applications, however, require that the preferences are applied on a\njoined relation. Common examples include flights having one or multiple stops,\na combination of product price and shipping costs, etc. In this paper, we\nextend the k-dominant skyline queries to the join paradigm by enabling such\nqueries to be asked on joined relations. We call such queries KSJQ (k-dominant\nskyline join queries). The number of skyline attributes, k, that an object must\ndominate is from the combined set of skyline attributes of the joined relation.\nWe show how pre-processing the base relations helps in reducing the time of\nanswering such queries over the naive method of joining the relations first and\nthen running the k-dominant skyline computation. We also extend the query to\nhandle cases where the skyline preference is on aggregated values in the joined\nrelation (such as total cost of the multiple legs of the flight) which are\navailable only after the join is performed. In addition to these problems, we\ndevise efficient algorithms to choose the value of k based on the desired\ncardinality of the final skyline set. Experiments on both real and synthetic\ndatasets demonstrate the efficiency, scalability and practicality of our\nalgorithms.\n"]},
{"authors": ["Konstantinos Xirogiannopoulos", "Amol Deshpande"], "title": ["Extracting and Analyzing Hidden Graphs from Relational Databases"], "date": ["2017-01-25T17:25:56Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.07388v2"], "summary": ["  Analyzing interconnection structures among underlying entities or objects in\na dataset through the use of graph analytics has been shown to provide\ntremendous value in many application domains. However, graphs are not the\nprimary representation choice for storing most data today, and in order to have\naccess to these analyses, users are forced to extract data from their data\nstores, construct the requisite graphs, and then load them into some graph\nengine in order to execute their graph analysis task. Moreover, these graphs\ncan be significantly larger than the initial input stored in the database,\nmaking it infeasible to construct or analyze such graphs in memory. In this\npaper we address both of these challenges by building a system that enables\nusers to declaratively specify graph extraction tasks over a relational\ndatabase schema and then execute graph algorithms on the extracted graphs. We\npropose a declarative domain-specific language for this purpose, and pair it up\nwith a novel condensed, in-memory representation that significantly reduces the\nmemory footprint of these graphs, permitting analysis of larger-than-memory\ngraphs. We present a general algorithm for creating this condensed\nrepresentation for a large class of graph extraction queries against arbitrary\nschemas. We observe that the condensed representation suffers from a\nduplication issue, that results in inaccuracies for most graph algorithms. We\nthen present a suite of in-memory representations that handle this duplication\nin different ways and allow trading off the memory required and the\ncomputational cost for executing different graph algorithms. We introduce novel\ndeduplication algorithms for removing this duplication in the graph, which are\nof independent interest for graph compression, and provide a comprehensive\nexperimental evaluation over several real-world and synthetic datasets\nillustrating these trade-offs.\n"]},
{"authors": ["Meghyn Bienvenu", "Stanislav Kikot", "Roman Kontchakov", "Vladimir V. Podolskii", "Vladislav Ryzhikov", "Michael Zakharyaschev"], "title": ["The Complexity of Ontology-Based Data Access with OWL 2 QL and Bounded\n  Treewidth Queries"], "date": ["2017-02-11T00:10:30Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.03358v1"], "summary": ["  Our concern is the overhead of answering OWL 2 QL ontology-mediated queries\n(OMQs) in ontology-based data access compared to evaluating their underlying\ntree-shaped and bounded treewidth conjunctive queries (CQs). We show that OMQs\nwith bounded-depth ontologies have nonrecursive datalog (NDL) rewritings that\ncan be constructed and evaluated in LOGCFL for combined complexity, even in NL\nif their CQs are tree-shaped with a bounded number of leaves, and so incur no\noverhead in complexity-theoretic terms. For OMQs with arbitrary ontologies and\nbounded-leaf CQs, NDL-rewritings are constructed and evaluated in LOGCFL. We\nshow experimentally feasibility and scalability of our rewritings compared to\npreviously proposed NDL-rewritings. On the negative side, we prove that\nanswering OMQs with tree-shaped CQs is not fixed-parameter tractable if the\nontology depth or the number of leaves in the CQs is regarded as the parameter,\nand that answering OMQs with a fixed ontology (of infinite depth) is\nNP-complete for tree-shaped and LOGCFL for bounded-leaf CQs.\n"]},
{"authors": ["Vladimir Dzyuba", "Matthijs van Leeuwen"], "title": ["Learning what matters - Sampling interesting patterns"], "date": ["2017-02-07T12:01:08Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.01975v2"], "summary": ["  In the field of exploratory data mining, local structure in data can be\ndescribed by patterns and discovered by mining algorithms. Although many\nsolutions have been proposed to address the redundancy problems in pattern\nmining, most of them either provide succinct pattern sets or take the interests\nof the user into account-but not both. Consequently, the analyst has to invest\nsubstantial effort in identifying those patterns that are relevant to her\nspecific interests and goals. To address this problem, we propose a novel\napproach that combines pattern sampling with interactive data mining. In\nparticular, we introduce the LetSIP algorithm, which builds upon recent\nadvances in 1) weighted sampling in SAT and 2) learning to rank in interactive\npattern mining. Specifically, it exploits user feedback to directly learn the\nparameters of the sampling distribution that represents the user's interests.\nWe compare the performance of the proposed algorithm to the state-of-the-art in\ninteractive pattern mining by emulating the interests of a user. The resulting\nsystem allows efficient and interleaved learning and sampling, thus\nuser-specific anytime data exploration. Finally, LetSIP demonstrates favourable\ntrade-offs concerning both quality-diversity and exploitation-exploration when\ncompared to existing methods.\n"]},
{"authors": ["Yang Zhang", "Yusu Wang", "Srinivasan Parthasarathy"], "title": ["Analyzing and Visualizing Scalar Fields on Graphs"], "date": ["2017-02-10T07:47:48Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.03825v1"], "summary": ["  The value proposition of a dataset often resides in the implicit\ninterconnections or explicit relationships (patterns) among individual\nentities, and is often modeled as a graph. Effective visualization of such\ngraphs can lead to key insights uncovering such value. In this article we\npropose a visualization method to explore graphs with numerical attributes\nassociated with nodes (or edges) -- referred to as scalar graphs. Such\nnumerical attributes can represent raw content information, similarities, or\nderived information reflecting important network measures such as triangle\ndensity and centrality. The proposed visualization strategy seeks to\nsimultaneously uncover the relationship between attribute values and graph\ntopology, and relies on transforming the network to generate a terrain map. A\nkey objective here is to ensure that the terrain map reveals the overall\ndistribution of components-of-interest (e.g. dense subgraphs, k-cores) and the\nrelationships among them while being sensitive to the attribute values over the\ngraph. We also design extensions that can capture the relationship across\nmultiple numerical attributes (scalars). We demonstrate the efficacy of our\nmethod on several real-world data science tasks while scaling to large graphs\nwith millions of nodes.\n"]},
{"authors": ["Anh Dinh", "Ji Wang", "Sheng Wang", "Gang Chen", "Wei-Ngan Chin", "Qian Lin", "Beng Chin Ooi", "Pingcheng Ruan", "Kian-Lee Tan", "Zhongle Xie", "Hao Zhang", "Meihui Zhang"], "title": ["UStore: A Distributed Storage With Rich Semantics"], "date": ["2017-02-09T12:06:37Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.02799v1"], "summary": ["  Today's storage systems expose abstractions which are either too low-level\n(e.g., key-value store, raw-block store) that they require developers to\nre-invent the wheels, or too high-level (e.g., relational databases, Git) that\nthey lack generality to support many classes of applications. In this work, we\npropose and implement a general distributed data storage system, called UStore,\nwhich has rich semantics. UStore delivers three key properties, namely\nimmutability, sharing and security, which unify and add values to many classes\nof today's applications, and which also open the door for new applications. By\nkeeping the core properties within the storage, UStore helps reduce application\ndevelopment efforts while offering high performance at hand. The storage\nembraces current hardware trends as key enablers. It is built around a\ndata-structure similar to that of Git, a popular source code versioning system,\nbut it also synthesizes many designs from distributed systems and databases.\nOur current implementation of UStore has better performance than general\nin-memory key-value storage systems, especially for version scan operations. We\nport and evaluate four applications on top of UStore: a Git-like application, a\ncollaborative data science application, a transaction management application,\nand a blockchain application. We demonstrate that UStore enables faster\ndevelopment and the UStore-backed applications can have better performance than\nthe existing implementations.\n"]},
{"authors": ["Pankaj K. Agarwal", "Nirman Kumar", "Stavros Sintos", "Subhash Suri"], "title": ["Efficient Algorithms for k-Regret Minimizing Sets"], "date": ["2017-02-05T19:30:44Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.01446v2"], "summary": ["  A regret minimizing set Q is a small size representation of a much larger\ndatabase P so that user queries executed on Q return answers whose scores are\nnot much worse than those on the full dataset. In particular, a k-regret\nminimizing set has the property that the regret ratio between the score of the\ntop-1 item in Q and the score of the top-k item in P is minimized, where the\nscore of an item is the inner product of the item's attributes with a user's\nweight (preference) vector. The problem is challenging because we want to find\na single representative set Q whose regret ratio is small with respect to all\npossible user weight vectors.\n  We show that k-regret minimization is NP-Complete for all dimensions d >= 3.\nThis settles an open problem from Chester et al. [VLDB 2014], and resolves the\ncomplexity status of the problem for all d: the problem is known to have\npolynomial-time solution for d <= 2. In addition, we propose two new\napproximation schemes for regret minimization, both with provable guarantees,\none based on coresets and another based on hitting sets. We also carry out\nextensive experimental evaluation, and show that our schemes compute\nregret-minimizing sets comparable in size to the greedy algorithm proposed in\n[VLDB 14] but our schemes are significantly faster and scalable to large data\nsets.\n"]},
{"authors": ["Chathuri Gunawardhana", "Manuel Bravo", "Lu\u00eds Rodrigues"], "title": ["Unobtrusive Deferred Update Stabilization for Efficient Geo-Replication"], "date": ["2017-02-06T20:21:32Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.01786v1"], "summary": ["  In this paper we propose a novel approach to manage the throughput vs latency\ntradeoff that emerges when managing updates in geo-replicated systems. Our\napproach consists in allowing full concurrency when processing local updates\nand using a deferred local serialisation procedure before shipping updates to\nremote datacenters. This strategy allows to implement inexpensive mechanisms to\nensure system consistency requirements while avoiding intrusive effects on\nupdate operations, a major performance limitation of previous systems. We have\nimplemented our approach as a variant of Riak KV. Our extensive evaluation\nshows that we outperform sequencer-based approaches by almost an order of\nmagnitude in the maximum achievable throughput. Furthermore, unlike previous\nsequencer-free solutions, our approach reaches nearly optimal remote update\nvisibility latencies without limiting throughput.\n"]},
{"authors": ["Mahmoud Abo Khamis", "Hung Q. Ngo", "Atri Rudra"], "title": ["FAQ: Questions Asked Frequently"], "date": ["2015-04-15T20:31:00Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1504.04044v6"], "summary": ["  We define and study the Functional Aggregate Query (FAQ) problem, which\nencompasses many frequently asked questions in constraint satisfaction,\ndatabases, matrix operations, probabilistic graphical models and logic. This is\nour main conceptual contribution.\n  We then present a simple algorithm called \"InsideOut\" to solve this general\nproblem. InsideOut is a variation of the traditional dynamic programming\napproach for constraint programming based on variable elimination. Our\nvariation adds a couple of simple twists to basic variable elimination in order\nto deal with the generality of FAQ, to take full advantage of Grohe and Marx's\nfractional edge cover framework, and of the analysis of recent worst-case\noptimal relational join algorithms.\n  As is the case with constraint programming and graphical model inference, to\nmake InsideOut run efficiently we need to solve an optimization problem to\ncompute an appropriate 'variable ordering'. The main technical contribution of\nthis work is a precise characterization of when a variable ordering is\n'semantically equivalent' to the variable ordering given by the input FAQ\nexpression. Then, we design an approximation algorithm to find an equivalent\nvariable ordering that has the best 'fractional FAQ-width'. Our results imply a\nhost of known and a few new results in graphical model inference, matrix\noperations, relational joins, and logic.\n  We also briefly explain how recent algorithms on beyond worst-case analysis\nfor joins and those for solving SAT and #SAT can be viewed as variable\nelimination to solve FAQ over compactly represented input functions.\n"]},
{"authors": ["Arya Mazumdar", "Barna Saha"], "title": ["A Theoretical Analysis of First Heuristics of Crowdsourced Entity\n  Resolution"], "date": ["2017-02-03T23:56:58Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.01208v1"], "summary": ["  Entity resolution (ER) is the task of identifying all records in a database\nthat refer to the same underlying entity, and are therefore duplicates of each\nother. Due to inherent ambiguity of data representation and poor data quality,\nER is a challenging task for any automated process. As a remedy, human-powered\nER via crowdsourcing has become popular in recent years. Using crowd to answer\nqueries is costly and time consuming. Furthermore, crowd-answers can often be\nfaulty. Therefore, crowd-based ER methods aim to minimize human participation\nwithout sacrificing the quality and use a computer generated similarity matrix\nactively. While, some of these methods perform well in practice, no theoretical\nanalysis exists for them, and further their worst case performances do not\nreflect the experimental findings. This creates a disparity in the\nunderstanding of the popular heuristics for this problem. In this paper, we\nmake the first attempt to close this gap. We provide a thorough analysis of the\nprominent heuristic algorithms for crowd-based ER. We justify experimental\nobservations with our analysis and information theoretic lower bounds.\n"]},
{"authors": ["Navid Yaghmazadeh", "Yuepeng Wang", "Isil Dillig", "Thomas Dillig"], "title": ["Type- and Content-Driven Synthesis of SQL Queries from Natural Language"], "date": ["2017-02-03T21:26:43Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.01168v1"], "summary": ["  This paper presents a new technique for automatically synthesizing SQL\nqueries from natural language. Our technique is fully automated, works for any\ndatabase without requiring additional customization, and does not require users\nto know the underlying database schema. Our method achieves these goals by\ncombining natural language processing, program synthesis, and automated program\nrepair. Given the user's English description, our technique first uses semantic\nparsing to generate a query sketch, which is subsequently completed using\ntype-directed program synthesis and assigned a confidence score using database\ncontents. However, since the user's description may not accurately reflect the\nactual database schema, our approach also performs fault localization and\nrepairs the erroneous part of the sketch. This synthesize-repair loop is\nrepeated until the algorithm infers a query with a sufficiently high confidence\nscore. We have implemented the proposed technique in a tool called Sqlizer and\nevaluate it on three different databases. Our experiments show that the desired\nquery is ranked within the top 5 candidates in close to 90% of the cases.\n"]},
{"authors": ["Helge Holzmann", "Vinay Goel", "Avishek Anand"], "title": ["ArchiveSpark: Efficient Web Archive Access, Extraction and Derivation"], "date": ["2017-02-03T14:17:02Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.01015v1"], "summary": ["  Web archives are a valuable resource for researchers of various disciplines.\nHowever, to use them as a scholarly source, researchers require a tool that\nprovides efficient access to Web archive data for extraction and derivation of\nsmaller datasets. Besides efficient access we identify five other objectives\nbased on practical researcher needs such as ease of use, extensibility and\nreusability.\n  Towards these objectives we propose ArchiveSpark, a framework for efficient,\ndistributed Web archive processing that builds a research corpus by working on\nexisting and standardized data formats commonly held by Web archiving\ninstitutions. Performance optimizations in ArchiveSpark, facilitated by the use\nof a widely available metadata index, result in significant speed-ups of data\nprocessing. Our benchmarks show that ArchiveSpark is faster than alternative\napproaches without depending on any additional data stores while improving\nusability by seamlessly integrating queries and derivations with external\ntools.\n"]},
{"authors": ["Wan-Lei Zhao", "Jie Yang", "Cheng-Hao Deng"], "title": ["Scalable Nearest Neighbor Search based on kNN Graph"], "date": ["2017-01-30T03:51:28Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.08475v2"], "summary": ["  Nearest neighbor search is known as a challenging issue that has been studied\nfor several decades. Recently, this issue becomes more and more imminent in\nviewing that the big data problem arises from various fields. In this paper, a\nscalable solution based on hill-climbing strategy with the support of k-nearest\nneighbor graph (kNN) is presented. Two major issues have been considered in the\npaper. Firstly, an efficient kNN graph construction method based on two means\ntree is presented. For the nearest neighbor search, an enhanced hill-climbing\nprocedure is proposed, which sees considerable performance boost over original\nprocedure. Furthermore, with the support of inverted indexing derived from\nresidue vector quantization, our method achieves close to 100% recall with high\nspeed efficiency in two state-of-the-art evaluation benchmarks. In addition, a\ncomparative study on both the compressional and traditional nearest neighbor\nsearch methods is presented. We show that our method achieves the best\ntrade-off between search quality, efficiency and memory complexity.\n"]},
{"authors": ["Shubham Varma", "Neyshith Sameer", "C. Ravindranath Chowdary"], "title": ["ReLiC: Entity Profiling by using Random Forest and Trustworthiness of a\n  Source - Technical Report"], "date": ["2017-02-03T07:04:13Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.00921v1"], "summary": ["  The digital revolution has brought most of the world on the world wide web.\nThe data available on WWW has increased many folds in the past decade. Social\nnetworks, online clubs and organisations have come into existence. Information\nis extracted from these venues about a real world entity like a person,\norganisation, event, etc. However, this information may change over time, and\nthere is a need for the sources to be up-to-date. Therefore, it is desirable to\nhave a model to extract relevant data items from different sources and merge\nthem to build a complete profile of an entity (entity profiling). Further, this\nmodel should be able to handle incorrect or obsolete data items. In this paper,\nwe propose a novel method for completing a profile. We have developed a two\nphase method-1) The first phase (resolution phase) links records to the\nqueries. We have proposed and observed that the use of random forest for entity\nresolution increases the performance of the system as this has resulted in more\nrecords getting linked to the correct entity. Also, we used trustworthiness of\na source as a feature to the random forest. 2) The second phase selects the\nappropriate values from records to complete a profile based on our proposed\nselection criteria. We have used various metrics for measuring the performance\nof the resolution phase as well as for the overall ReLiC framework. It is\nestablished through our results that the use of biased sources has\nsignificantly improved the performance of the ReLiC framework. Experimental\nresults show that our proposed system, ReLiC outperforms the state-of-the-art.\n"]},
{"authors": ["Theodoros Rekatsinas", "Xu Chu", "Ihab F. Ilyas", "Christopher R\u00e9"], "title": ["HoloClean: Holistic Data Repairs with Probabilistic Inference"], "date": ["2017-02-02T20:25:41Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.00820v1"], "summary": ["  We introduce HoloClean, a framework for holistic data repairing driven by\nprobabilistic inference. HoloClean unifies existing qualitative data repairing\napproaches, which rely on integrity constraints or external data sources, with\nquantitative data repairing methods, which leverage statistical properties of\nthe input data. Given an inconsistent dataset as input, HoloClean automatically\ngenerates a probabilistic program that performs data repairing. Inspired by\nrecent theoretical advances in probabilistic inference, we introduce a series\nof optimizations which ensure that inference over HoloClean's probabilistic\nmodel scales to instances with millions of tuples. We show that HoloClean\nscales to instances with millions of tuples and find data repairs with an\naverage precision of ~90% and an average recall of above ~76% across a diverse\narray of datasets exhibiting different types of errors. This yields an average\nF1 improvement of more than 2x against state-of-the-art methods.\n"]},
{"authors": ["Yunfan Chen", "Lei Chen", "Chen Jason Zhang"], "title": ["CrowdFusion: A Crowdsourced Approach on Data Fusion Refinement"], "date": ["2017-02-02T07:59:25Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.00567v1"], "summary": ["  Data fusion has played an important role in data mining because high-quality\ndata is required in a lot of applications. As on-line data may be out-of-date\nand errors in the data may propagate with copying and referring between\nsources, it is hard to achieve satisfying results with merely applying existing\ndata fusion methods to fuse Web data. In this paper, we make use of the crowd\nto achieve high-quality data fusion result. We design a framework selecting a\nset of tasks to ask crowds in order to improve the confidence of data. Since\ndata are correlated and crowds may provide incorrect answers, how to select a\nproper set of tasks to ask the crowd is a very challenging problem. In this\npaper, we design an approximation solution to address this challenge since we\nprove that the problem is at NP-hard. To further improve the efficiency, we\ndesign a pruning strategy and a preprocessing method, which effectively improve\nthe performance of the proposed approximation solution. Furthermore, we find\nthat under certain scenarios, we are not interested in all the facts, but only\na specific set of facts. Thus, for these specific scenarios, we also develop\nanother approximation solution which is much faster than the general\napproximation solution. We verify the solutions with extensive experiments on a\nreal crowdsourcing platform.\n"]},
{"authors": ["Yu Cheng", "Weijie Zhao", "Florin Rusu"], "title": ["OLA-RAW: Scalable Exploration over Raw Data"], "date": ["2017-02-01T17:07:56Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.00358v1"], "summary": ["  In-situ processing has been proposed as a novel data exploration solution in\nmany domains generating massive amounts of raw data, e.g., astronomy, since it\nprovides immediate SQL querying over raw files. The performance of in-situ\nprocessing across a query workload is, however, limited by the speed of full\nscan, tokenizing, and parsing of the entire data. Online aggregation (OLA) has\nbeen introduced as an efficient method for data exploration that identifies\nuninteresting patterns faster by continuously estimating the result of a\ncomputation during the actual processing---the computation can be stopped as\nearly as the estimate is accurate enough to be deemed uninteresting. However,\nexisting OLA solutions have a high upfront cost of randomly shuffling and/or\nsampling the data. In this paper, we present OLA-RAW, a bi-level sampling\nscheme for parallel online aggregation over raw data. Sampling in OLA-RAW is\nquery-driven and performed exclusively in-situ during the runtime query\nexecution, without data reorganization. This is realized by a novel\nresource-aware bi-level sampling algorithm that processes data in random chunks\nconcurrently and determines adaptively the number of sampled tuples inside a\nchunk. In order to avoid the cost of repetitive conversion from raw data,\nOLA-RAW builds and maintains a memory-resident bi-level sample synopsis\nincrementally. We implement OLA-RAW inside a modern in-situ data processing\nsystem and evaluate its performance across several real and synthetic datasets\nand file formats. Our results show that OLA-RAW chooses the sampling plan that\nminimizes the execution time and guarantees the required accuracy for each\nquery in a given workload. The end result is a focused data exploration process\nthat avoids unnecessary work and discards uninteresting data.\n"]},
{"authors": ["Panthadeep Bhattacharjee", "Amit Awekar"], "title": ["Batch Incremental Shared Nearest Neighbor Density Based Clustering\n  Algorithm for Dynamic Datasets"], "date": ["2017-01-31T14:19:18Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.09049v1"], "summary": ["  Incremental data mining algorithms process frequent updates to dynamic\ndatasets efficiently by avoiding redundant computation. Existing incremental\nextension to shared nearest neighbor density based clustering (SNND) algorithm\ncannot handle deletions to dataset and handles insertions only one point at a\ntime. We present an incremental algorithm to overcome both these bottlenecks by\nefficiently identifying affected parts of clusters while processing updates to\ndataset in batch mode. We show effectiveness of our algorithm by performing\nexperiments on large synthetic as well as real world datasets. Our algorithm is\nup to four orders of magnitude faster than SNND and requires up to 60% extra\nmemory than SNND while providing output identical to SNND.\n"]},
{"authors": ["Serge Abiteboul", "Marcelo Arenas", "Pablo Barcel\u00f3", "Meghyn Bienvenu", "Diego Calvanese", "Claire David", "Richard Hull", "Eyke H\u00fcllermeier", "Benny Kimelfeld", "Leonid Libkin", "Wim Martens", "Tova Milo", "Filip Murlak", "Frank Neven", "Magdalena Ortiz", "Thomas Schwentick", "Julia Stoyanovich", "Jianwen Su", "Dan Suciu", "Victor Vianu", "Ke Yi"], "title": ["Research Directions for Principles of Data Management (Dagstuhl\n  Perspectives Workshop 16151)"], "date": ["2017-01-31T12:15:31Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.09007v1"], "summary": ["  In April 2016, a community of researchers working in the area of Principles\nof Data Management (PDM) joined in a workshop at the Dagstuhl Castle in\nGermany. The workshop was organized jointly by the Executive Committee of the\nACM Symposium on Principles of Database Systems (PODS) and the Council of the\nInternational Conference on Database Theory (ICDT). The mission of this\nworkshop was to identify and explore some of the most important research\ndirections that have high relevance to society and to Computer Science today,\nand where the PDM community has the potential to make significant\ncontributions. This report describes the family of research directions that the\nworkshop focused on from three perspectives: potential practical relevance,\nresults already obtained, and research questions that appear surmountable in\nthe short and medium term.\n"]},
{"authors": ["Jose-Emilio Labra-Gayo", "Eric Prud'hommeaux", "Harold Solbrig", "Iovka Boneva"], "title": ["Validating and describing linked data portals using shapes"], "date": ["2017-01-31T05:51:09Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.08924v1"], "summary": ["  Linked data portals need to be able to advertise and describe the structure\nof their content. A sufficiently expressive and intuitive schema language will\nallow portals to communicate these structures. Validation tools will aid in the\npublication and maintenance of linked data and increase their quality.\n  Two schema language proposals have recently emerged for describing the\nstructures of RDF graphs: Shape Expressions (ShEx) and Shapes Constraint\nLanguage (SHACL). In this paper we describe how these formalisms can be used in\nthe development of a linked data portal to describe and validate its contents.\nAs a use case, we specify a data model inspired by the WebIndex data model, a\nmedium size linked data portal, using both ShEx and SHACL, and we propose a\nbenchmark that can generate compliant test data structures of any size. We then\nperform some preliminary experiments showing performance of one validation\nengine based on ShEx.\n"]},
{"authors": ["Chengjie Qin", "Florin Rusu"], "title": ["Dot-Product Join: An Array-Relation Join Operator for Big Model\n  Analytics"], "date": ["2016-02-29T07:41:28Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1602.08845v2"], "summary": ["  Big Model analytics tackles the training of massive models that go beyond the\navailable memory of a single computing device, e.g., CPU or GPU. It generalizes\nBig Data analytics which is targeted at how to train memory-resident models\nover out-of-memory training data. In this paper, we propose an in-database\nsolution for Big Model analytics. We identify dot-product as the primary\noperation for training generalized linear models and introduce the first\narray-relation dot-product join database operator between a set of sparse\narrays and a dense relation. This is a constrained formulation of the\nextensively studied sparse matrix vector multiplication (SpMV) kernel. The\nparamount challenge in designing the dot-product join operator is how to\noptimally schedule access to the dense relation based on the non-contiguous\nentries in the sparse arrays. We prove that this problem is NP-hard and propose\na practical solution characterized by two technical contributions---dynamic\nbatch processing and array reordering. We devise three heuristics -- LSH,\nRadix, and K-center -- for array reordering and analyze them thoroughly. We\nexecute extensive experiments over synthetic and real data that confirm the\nminimal overhead the operator incurs when sufficient memory is available and\nthe graceful degradation it suffers as memory becomes scarce. Moreover,\ndot-product join achieves an order of magnitude reduction in execution time\nover alternative in-database solutions.\n"]},
{"authors": ["Fadila Bentayeb", "Nora Ma\u00efz", "Hadj Mahboubi", "C\u00e9cile Favre", "Sabine Loudcher", "Nouria Harbi", "Omar Boussa\u00efd", "J\u00e9r\u00f4me Darmont"], "title": ["Innovative Approaches for efficiently Warehousing Complex Data from the\n  Web"], "date": ["2017-01-30T15:17:05Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.08643v1"], "summary": ["  Research in data warehousing and OLAP has produced important technologies for\nthe design, management and use of information systems for decision support.\nWith the development of Internet, the availability of various types of data has\nincreased. Thus, users require applications to help them obtaining knowledge\nfrom the Web. One possible solution to facilitate this task is to extract\ninformation from the Web, transform and load it to a Web Warehouse, which\nprovides uniform access methods for automatic processing of the data. In this\nchapter, we present three innovative researches recently introduced to extend\nthe capabilities of decision support systems, namely (1) the use of XML as a\nlogical and physical model for complex data warehouses, (2) associating data\nmining to OLAP to allow elaborated analysis tasks for complex data and (3)\nschema evolution in complex data warehouses for personalized analyses. Our\ncontributions cover the main phases of the data warehouse design process: data\nintegration and modeling and user driven-OLAP analysis.\n"]},
{"authors": ["J\u00e9r\u00f4me Darmont"], "title": ["Data Processing Benchmarks"], "date": ["2017-01-30T15:10:07Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.08634v1"], "summary": ["  The aim of this article is to present an overview of the major families of\nstate-of-the-art data processing benchmarks, namely transaction processing\nbenchmarks and decision support benchmarks. We also address the newer trends in\ncloud benchmarking. Finally, we discuss the issues, tradeoffs and future trends\nfor data processing benchmarks.\n"]},
{"authors": ["Hadj Mahboubi", "Marouane Hachicha", "J\u00e9r\u00f4me Darmont"], "title": ["XML Warehousing and OLAP"], "date": ["2017-01-30T14:27:07Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.08612v1"], "summary": ["  The aim of this article is to present an overview of the major XML\nwarehousing approaches from the literature, as well as the existing approaches\nfor performing OLAP analyses over XML data (which is termed XML-OLAP or XOLAP;\nWang et al., 2005). We also discuss the issues and future trends in this area\nand illustrate this topic by presenting the design of a unified, XML data\nwarehouse architecture and a set of XOLAP operators expressed in an XML\nalgebra.\n"]},
{"authors": ["Jeff Heaton"], "title": ["Comparing Dataset Characteristics that Favor the Apriori, Eclat or\n  FP-Growth Frequent Itemset Mining Algorithms"], "date": ["2017-01-30T12:34:02Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.09042v1"], "summary": ["  Frequent itemset mining is a popular data mining technique. Apriori, Eclat,\nand FP-Growth are among the most common algorithms for frequent itemset mining.\nConsiderable research has been performed to compare the relative performance\nbetween these three algorithms, by evaluating the scalability of each algorithm\nas the dataset size increases. While scalability as data size increases is\nimportant, previous papers have not examined the performance impact of\nsimilarly sized datasets that contain different itemset characteristics. This\npaper explores the effects that two dataset characteristics can have on the\nperformance of these three frequent itemset algorithms. To perform this\nempirical analysis, a dataset generator is created to measure the effects of\nfrequent item density and the maximum transaction size on performance. The\ngenerated datasets contain the same number of rows. This provides some insight\ninto dataset characteristics that are conducive to each algorithm. The results\nof this paper's research demonstrate Eclat and FP-Growth both handle increases\nin maximum transaction size and frequent itemset density considerably better\nthan the Apriori algorithm.\n  This paper explores the effects that two dataset characteristics can have on\nthe performance of these three frequent itemset algorithms. To perform this\nempirical analysis, a dataset generator is created to measure the effects of\nfrequent item density and the maximum transaction size on performance. The\ngenerated datasets contain the same number of rows. This provides some insight\ninto dataset characteristics that are conducive to each algorithm. The results\nof this paper's research demonstrate Eclat and FP-Growth both handle increases\nin maximum transaction size and frequent itemset density considerably better\nthan the Apriori algorithm.\n"]},
{"authors": ["Hao Li"], "title": ["Big Data Technology Accelerate Genomics Precision Medicine"], "date": ["2017-01-29T14:21:52Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.09045v1"], "summary": ["  During genomics life science research, the data volume of whole genomics and\nlife science algorithm is going bigger and bigger, which is calculated as TB,\nPB or EB etc. The key problem will be how to store and analyze the data with\noptimized way. This paper demonstrates how Intel Big Data Technology and\nArchitecture help to facilitate and accelerate the genomics life science\nresearch in data store and utilization. Intel defines high performance\nGenomicsDB for variant call data query and Lustre filesystem with Hierarchal\nStorage Management for genomics data store. Based on these great technology,\nIntel defines genomics knowledge share and exchange architecture, which is\nlanded and validated in BGI China and Shanghai Children Hospital with very\npositive feedback. And these big data technology can definitely be scaled to\nmuch more genomics life science partners in the world.\n"]},
{"authors": ["Mohamed Anis Bach Tobji", "Mohamed Salah Gouider"], "title": ["Incremental Maintenance Of Association Rules Under Support Threshold\n  Change"], "date": ["2017-01-27T21:02:52Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.08191v1"], "summary": ["  Maintenance of association rules is an interesting problem. Several\nincremental maintenance algorithms were proposed since the work of (Cheung et\nal, 1996). The majority of these algorithms maintain rule bases assuming that\nsupport threshold doesn't change. In this paper, we present incremental\nmaintenance algorithm under support threshold change. This solution allows user\nto maintain its rule base under any support threshold.\n"]},
{"authors": ["Mohamed Anis Bach Tobji"], "title": ["Comparative Study Of Data Mining Query Languages"], "date": ["2017-01-27T21:00:19Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.08190v1"], "summary": ["  Since formulation of Inductive Database (IDB) problem, several Data Mining\n(DM) languages have been proposed, confirming that KDD process could be\nsupported via inductive queries (IQ) answering. This paper reviews the existing\nDM languages. We are presenting important primitives of the DM language and\nclassifying our languages according to primitives' satisfaction. In addition,\nwe presented languages' syntaxes and tried to apply each one to a database\nsample to test a set of KDD operations. This study allows us to highlight\nlanguages capabilities and limits, which is very useful for future work and\nperspectives.\n"]},
{"authors": ["Apratim Bhattacharyya", "Jilles Vreeken"], "title": ["Efficiently Summarising Event Sequences with Rich Interleaving Patterns"], "date": ["2017-01-27T16:02:54Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.08096v1"], "summary": ["  Discovering the key structure of a database is one of the main goals of data\nmining. In pattern set mining we do so by discovering a small set of patterns\nthat together describe the data well. The richer the class of patterns we\nconsider, and the more powerful our description language, the better we will be\nable to summarise the data. In this paper we propose \\ourmethod, a novel greedy\nMDL-based method for summarising sequential data using rich patterns that are\nallowed to interleave. Experiments show \\ourmethod is orders of magnitude\nfaster than the state of the art, results in better models, as well as\ndiscovers meaningful semantics in the form patterns that identify multiple\nchoices of values.\n"]},
{"authors": ["Hadj Mahboubi", "J\u00e9r\u00f4me Darmont"], "title": ["Query Performance Optimization in XML Data Warehouses"], "date": ["2017-01-27T15:53:04Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.08088v1"], "summary": ["  XML data warehouses form an interesting basis for decision-support\napplications that exploit complex data. However, native-XML database management\nsystems (DBMSs) currently bear limited performances and it is necessary to\nresearch for ways to optimize them. In this chapter, we present two such\ntechniques. First, we propose a join index that is specifically adapted to the\nmultidimensional architecture of XML warehouses. It eliminates join operations\nwhile preserving the information contained in the original warehouse. Second,\nwe present a strategy for selecting XML materialized views by clustering the\nquery workload. To validate these proposals, we measure the response time of a\nset of decision-support XQueries over an XML data warehouse, with and without\nusing our optimization techniques. Our experimental results demonstrate their\nefficiency, even when queries are complex and data are voluminous.\n"]},
{"authors": ["Hadj Mahboubi", "J\u00e9r\u00f4me Darmont"], "title": ["Indices in XML Databases"], "date": ["2017-01-27T14:00:53Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.08054v1"], "summary": ["  With XML becoming a standard for business information representation and\nexchange, stor-ing, indexing, and querying XML documents have rapidly become\nmajor issues in database research. In this context, query processing and\noptimization are primordial, native-XML data-bases not being mature yet. Data\nstructures such as indices, which help enhance performances substantially, are\nextensively researched, especially since XML data bear numerous specifici-ties\nwith respect to relational data. In this paper, we survey state-of-the-art XML\nindices and discuss the main issues, tradeoffs and future trends in XML\nindexing. We also present an in-dex that we specifically designed for the\nparticular architecture of XML data warehouses.\n"]},
{"authors": ["J\u00e9r\u00f4me Darmont"], "title": ["Data Warehouse Benchmarking with DWEB"], "date": ["2017-01-27T13:56:37Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.08053v1"], "summary": ["  Performance evaluation is a key issue for designers and users of Database\nManagement Systems (DBMSs). Performance is generally assessed with software\nbenchmarks that help, e.g., test architectural choices, compare different\ntechnologies or tune a system. In the particular context of data warehousing\nand On-Line Analytical Processing (OLAP), although the Transaction Processing\nPerformance Council (TPC) aims at issuing standard decision-support benchmarks,\nfew benchmarks do actually exist. We present in this chapter the Data Warehouse\nEngineering Benchmark (DWEB), which allows generating various ad-hoc synthetic\ndata warehouses and workloads. DWEB is fully parameterized to fulfill various\ndata warehouse design needs. However, two levels of parameterization keep it\nrelatively easy to tune. We also expand on our previous work on DWEB by\npresenting its new Extract, Transform, and Load (ETL) feature as well as its\nnew execution protocol. A Java implementation of DWEB is freely available\non-line, which can be interfaced with most existing relational DMBSs. To the\nbest of our knowledge, DWEB is the only easily available, up-to-date benchmark\nfor data warehouses.\n"]},
{"authors": ["J\u00e9r\u00f4me Darmont"], "title": ["Database Benchmarks"], "date": ["2017-01-27T13:42:54Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.08052v1"], "summary": ["  The aim of this article is to present an overview of the major families of\nstate-of-the-art data-base benchmarks, namely: relational benchmarks, object\nand object-relational benchmarks, XML benchmarks, and decision-support\nbenchmarks, and to discuss the issues, tradeoffs and future trends in database\nbenchmarking. We particularly focus on XML and decision-support benchmarks,\nwhich are currently the most innovative tools that are developed in this area.\n"]},
{"authors": ["Hadj Mahboubi", "Jean-Christian Ralaivao", "Sabine Loudcher", "Omar Boussa\u00efd", "Fadila Bentayeb", "J\u00e9r\u00f4me Darmont"], "title": ["X-WACoDa: An XML-based approach for Warehousing and Analyzing Complex\n  Data"], "date": ["2017-01-27T12:39:39Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.08033v1"], "summary": ["  Data warehousing and OLAP applications must nowadays handle complex data that\nare not only numerical or symbolic. The XML language is well-suited to\nlogically and physically represent complex data. However, its usage induces new\ntheoretical and practical challenges at the modeling, storage and analysis\nlevels, and a new trend toward XML warehousing has been emerging for a couple\nof years. Unfortunately, no standard XML data warehouse architecture emerges.\nIn this paper, we propose a unified XML warehouse reference model that\nsynthesizes and enhances related work, and fits into a global XML warehousing\nand analysis approach we have developed. We also present a software platform\nthat is based on this model, as well as a case study that illustrates its\nusage.\n"]},
{"authors": ["Kamel Aouiche", "J\u00e9r\u00f4me Darmont"], "title": ["Index and Materialized View Selection in Data Warehouses"], "date": ["2017-01-27T12:30:49Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.08029v1"], "summary": ["  The aim of this article is to present an overview of the major families of\nstate-of-the-art index and materialized view selection methods, and to discuss\nthe issues and future trends in data warehouse performance optimization. We\nparticularly focus on data mining-based heuristics we developed to reduce the\nselection problem complexity and target the most pertinent candidate indexes\nand materialized views.\n"]},
{"authors": ["J\u00e9r\u00f4me Darmont", "Emerson Olivier"], "title": ["Biomedical Data Warehouses"], "date": ["2017-01-27T12:30:07Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.08028v1"], "summary": ["  The aim of this article is to present an overview of the existing biomedical\ndata warehouses and to discuss the issues and future trends in this area. We\nillustrate this topic by presenting the design of an innovative, complex data\nwarehouse for personal, anticipative medicine.\n"]},
{"authors": ["Jerome Darmont"], "title": ["Object Database Benchmarks"], "date": ["2017-01-26T15:29:51Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.07739v1"], "summary": ["  The need for performance measurement tools appeared soon after the emergence\nof the first Object-Oriented Database Management Systems (OODBMSs), and proved\nimportant for both designers and users (Atkinson \\& Maier, 1990). Performance\nevaluation is useful to designers to determine elements of architecture and\nmore generally to validate or refute hypotheses regarding the actual behavior\nof an OODBMS. Thus, performance evaluation is an essential component in the\ndevelopment process of well-designed and efficient systems. Users may also\nemploy performance evaluation, either to compare the efficiency of different\ntechnologies before selecting an OODBMS or to tune a system.Performance\nevaluation by experimentation on a real system is generally referred to as\nbenchmarking. It consists in performing a series of tests on a given OODBMS to\nestimate its performance in a given setting. Benchmarks are generally used to\ncompare the global performance of OODBMSs, but they can also be exploited to\nillustrate the advantages of one system or another in a given situation, or to\ndetermine an optimal hardware configuration. Typically, a benchmark is\nconstituted of two main elements: a workload model constituted of a database\nand a set of read and write operations to apply on this database, and a set of\nperformance metrics.\n"]},
{"authors": ["Georgia Kougka", "Anastasios Gounaris", "Alkis Simitsis"], "title": ["The Many Faces of Data-centric Workflow Optimization: A Survey"], "date": ["2017-01-26T14:45:28Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.07723v1"], "summary": ["  Workflow technology is rapidly evolving and, rather than being limited to\nmodeling the control flow in business processes, is becoming a key mechanism to\nperform advanced data management, such as big data analytics. This survey\nfocuses on data-centric workflows (or workflows for data analytics or data\nflows), where a key aspect is data passing through and getting manipulated by a\nsequence of steps. The large volume and variety of data, the complexity of\noperations performed, and the long time such workflows take to compute give\nrise to the need for optimization. In general, data-centric workflow\noptimization is a technology in evolution. This survey focuses on techniques\napplicable to workflows comprising arbitrary types of data manipulation steps\nand semantic inter-dependencies between such steps. Further, it serves a\ntwofold purpose. Firstly, to present the main dimensions of the relevant\noptimization problems and the types of optimizations that occur before flow\nexecution. Secondly, to provide a concise overview of the existing approaches\nwith a view to highlighting key observations and areas deserving more attention\nfrom the community.\n"]},
{"authors": ["Foto Afrati", "Manas Joglekar", "Christopher R\u00e9", "Semih Salihoglu", "Jeffrey D. Ullman"], "title": ["GYM: A Multiround Join Algorithm In MapReduce"], "date": ["2014-10-15T18:25:22Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1410.4156v8"], "summary": ["  Multiround algorithms are now commonly used in distributed data processing\nsystems, yet the extent to which algorithms can benefit from running more\nrounds is not well understood. This paper answers this question for a spectrum\nof rounds for the problem of computing the equijoin of $n$ relations.\nSpecifically, given any query $Q$ with width $\\w$, {\\em intersection width}\n$\\iw$, input size $\\mathrm{IN}$, output size $\\mathrm{OUT}$, and a cluster of\nmachines with $M$ memory available per machine, we show that:\n  (1) $Q$ can be computed in $O(n)$ rounds with $O(n\\frac{(\\mathrm{IN}^{\\w} +\n\\mathrm{OUT})^2}{M})$ communication cost.\n  (2) $Q$ can be computed in $O(\\log(n))$ rounds with\n$O(n\\frac{(\\mathrm{IN}^{\\max(\\w, 3\\iw)} + \\mathrm{OUT})^2}{M})$ communication\ncost. \\end{itemize} Intersection width is a new notion of queries and\ngeneralized hypertree decompositions (GHDs) of queries we introduce to capture\nhow connected the adjacent cyclic components of the GHDs are.\n  We achieve our first result by introducing a distributed and generalized\nversion of Yannakakis's algorithm, called GYM. GYM takes as input any GHD of\n$Q$ with width $\\w$ and depth $d$, and computes $Q$ in $O(d + \\log(n))$ rounds\nand $O(n\\frac{(\\mathrm{IN}^{\\w} + \\mathrm{OUT})^2}{M})$ communication cost. We\nachieve our second result by showing how to construct GHDs of $Q$ with width\n$\\max(\\w, 3\\iw)$ and depth $O(\\log(n))$. We describe another technique to\nconstruct GHDs with longer widths and shorter depths, demonstrating a spectrum\nof tradeoffs one can make between communication and the number of rounds.\n"]},
{"authors": ["Jimmy Dobler", "Atri Rudra"], "title": ["Implementation of Tetris as a Model Counter"], "date": ["2017-01-25T20:23:58Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.07473v1"], "summary": ["  Solving SharpSAT problems is an important area of work. In this paper, we\ndiscuss implementing Tetris, an algorithm originally designed for handling\nnatural joins, as an exact model counter for the SharpSAT problem. Tetris uses\na simple geometric framework, yet manages to achieve the fractional\nhypertree-width bound. Its design allows it to handle complex problems\ninvolving extremely large numbers of clauses on which other state-of-the-art\nmodel counters do not perform well, yet still performs strongly on standard SAT\nbenchmarks.\n  We have achieved the following objectives. First, we have found a natural set\nof model counting benchmarks on which Tetris outperforms other model counters.\nSecond, we have constructed a data structure capable of efficiently handling\nand caching all of the data Tetris needs to work on over the course of the\nalgorithm. Third, we have modified Tetris in order to move from a theoretical,\nasymptotic-time-focused environment to one that performs well in practice. In\nparticular, we have managed to produce results keeping us within a single order\nof magnitude as compared to other solvers on most benchmarks, and outperform\nthose solvers by multiple orders of magnitude on others.\n"]},
{"authors": ["Yongjoo Park", "Michael Cafarella", "Barzan Mozafari"], "title": ["Visualization-Aware Sampling for Very Large Databases"], "date": ["2015-10-13T22:51:36Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1510.03921v2"], "summary": ["  Interactive visualizations are crucial in ad hoc data exploration and\nanalysis. However, with the growing number of massive datasets, generating\nvisualizations in interactive timescales is increasingly challenging. One\napproach for improving the speed of the visualization tool is via data\nreduction in order to reduce the computational overhead, but at a potential\ncost in visualization accuracy. Common data reduction techniques, such as\nuniform and stratified sampling, do not exploit the fact that the sampled\ntuples will be transformed into a visualization for human consumption.\n  We propose a visualization-aware sampling (VAS) that guarantees high quality\nvisualizations with a small subset of the entire dataset. We validate our\nmethod when applied to scatter and map plots for three common visualization\ngoals: regression, density estimation, and clustering. The key to our sampling\nmethod's success is in choosing tuples which minimize a visualization-inspired\nloss function. Our user study confirms that optimizing this loss function\ncorrelates strongly with user success in using the resulting visualizations. We\nalso show the NP-hardness of our optimization problem and propose an efficient\napproximation algorithm. Our experiments show that, compared to previous\nmethods, (i) using the same sample size, VAS improves user's success by up to\n35% in various visualization tasks, and (ii) VAS can achieve a required\nvisualization quality up to 400 times faster.\n"]},
{"authors": ["Alekh Jindal", "Jorge-Arnulfo Quiane-Ruiz", "Samuel Madden"], "title": ["INGESTBASE: A Declarative Data Ingestion System"], "date": ["2017-01-21T22:34:47Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.06093v1"], "summary": ["  Big data applications have fast arriving data that must be quickly ingested.\nAt the same time, they have specific needs to preprocess and transform the data\nbefore it could be put to use. The current practice is to do these preparatory\ntransformations once the data is already ingested, however, this is expensive\nto run and cumbersome to manage. As a result, there is a need to push data\npreprocessing down to the ingestion itself. In this paper, we present a\ndeclarative data ingestion system, called INGESTBASE, to allow application\ndevelopers to plan and specify their data ingestion logic in a more systematic\nmanner. We introduce the notion of ingestions plans, analogous to query plans,\nand present a declarative ingestion language to help developers easily build\nsophisticated ingestion plans. INGESTBASE provides an extensible ingestion\noptimizer to rewrite and optimize ingestion plans by applying rules such as\noperator reordering and pipelining. Finally, the INGESTBASE runtime engine runs\nthe optimized ingestion plan in a distributed and fault-tolerant manner. Later,\nat query processing time, INGESTBASE supports ingestion-aware data access and\ninterfaces with upstream query processors, such as Hadoop MapReduce and Spark,\nto post- process the ingested data. We demonstrate through a number of\nexperiments that INGESTBASE: (i) is flexible enough to express a variety of\ningestion techniques, (ii) incurs a low ingestion overhead, (iii) provides\nefficient access to the ingested data, and (iv) has much better performance, up\nto 6 times, than preparing data as an afterthought, via a query processor.\n"]},
{"authors": ["Sudhakar Singh", "Rakhi Garg", "P. K. Mishra"], "title": ["Observations on Factors Affecting Performance of MapReduce based Apriori\n  on Hadoop Cluster"], "date": ["2017-01-21T05:12:13Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.05982v1"], "summary": ["  Designing fast and scalable algorithm for mining frequent itemsets is always\nbeing a most eminent and promising problem of data mining. Apriori is one of\nthe most broadly used and popular algorithm of frequent itemset mining.\nDesigning efficient algorithms on MapReduce framework to process and analyze\nbig datasets is contemporary research nowadays. In this paper, we have focused\non the performance of MapReduce based Apriori on homogeneous as well as on\nheterogeneous Hadoop cluster. We have investigated a number of factors that\nsignificantly affects the execution time of MapReduce based Apriori running on\nhomogeneous and heterogeneous Hadoop Cluster. Factors are specific to both\nalgorithmic and non-algorithmic improvements. Considered factors specific to\nalgorithmic improvements are filtered transactions and data structures.\nExperimental results show that how an appropriate data structure and filtered\ntransactions technique drastically reduce the execution time. The\nnon-algorithmic factors include speculative execution, nodes with poor\nperformance, data locality & distribution of data blocks, and parallelism\ncontrol with input split size. We have applied strategies against these factors\nand fine tuned the relevant parameters in our particular application.\nExperimental results show that if cluster specific parameters are taken care of\nthen there is a significant reduction in execution time. Also we have discussed\nthe issues regarding MapReduce implementation of Apriori which may\nsignificantly influence the performance.\n"]},
{"authors": ["Vinh Nguyen", "Amit Sheth"], "title": ["Logical Inferences with Contexts of RDF Triples"], "date": ["2017-01-20T08:51:41Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.05724v1"], "summary": ["  Logical inference, an integral feature of the Semantic Web, is the process of\nderiving new triples by applying entailment rules on knowledge bases. The\nentailment rules are determined by the model-theoretic semantics. Incorporating\ncontext of an RDF triple (e.g., provenance, time, and location) into the\ninferencing process requires the formal semantics to be capable of describing\nthe context of RDF triples also in the form of triples, or in other words, RDF\ncontextual triples about triples. The formal semantics should also provide the\nrules that could entail new contextual triples about triples. In this paper, we\npropose the first inferencing mechanism that allows context of RDF triples,\nrepresented in the form of RDF triples about triples, to be the first-class\ncitizens in the model-theoretic semantics and in the logical rules. Our\ninference mechanism is well-formalized with all new concepts being captured in\nthe model-theoretic semantics. This formal semantics also allows us to derive a\nnew set of entailment rules that could entail new contextual triples about\ntriples. To demonstrate the feasibility and the scalability of the proposed\nmechanism, we implement a new tool in which we transform the existing knowledge\nbases to our representation of RDF triples about triples and provide the option\nfor this tool to compute the inferred triples for the proposed rules. We\nevaluate the computation of the proposed rules on a large scale using various\nreal-world knowledge bases such as Bio2RDF NCBI Genes and DBpedia. The results\nshow that the computation of the inferred triples can be highly scalable. On\naverage, one billion inferred triples adds 5-6 minutes to the overall\ntransformation process. NCBI Genes, with 20 billion triples in total, took only\n232 minutes for the transformation of 12 billion triples and added 42 minutes\nfor inferring 8 billion triples to the overall process.\n"]},
{"authors": ["Seokki Lee", "Sven Koehler", "Bertram Ludaescher", "Boris Glavic"], "title": ["Efficiently Computing Provenance Graphs for Queries with Negation"], "date": ["2017-01-20T05:46:00Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.05699v1"], "summary": ["  Explaining why an answer is in the result of a query or why it is missing\nfrom the result is important for many applications including auditing,\ndebugging data and queries, and answering hypothetical questions about data.\nBoth types of questions, i.e., why and why-not provenance, have been studied\nextensively. In this work, we present the first practical approach for\nanswering such questions for queries with negation (first-order queries). Our\napproach is based on a rewriting of Datalog rules (called firing rules) that\ncaptures successful rule derivations within the context of a Datalog query. We\nextend this rewriting to support negation and to capture failed derivations\nthat explain missing answers. Given a (why or why-not) provenance question, we\ncompute an explanation, i.e., the part of the provenance that is relevant to\nanswer the question. We introduce optimizations that prune parts of a\nprovenance graph early on if we can determine that they will not be part of the\nexplanation for a given question. We present an implementation that runs on top\nof a relational database using SQL to compute explanations. Our experiments\ndemonstrate that our approach scales to large instances and significantly\noutperforms an earlier approach which instantiates the full provenance to\ncompute explanations.\n"]},
{"authors": ["Tarun Kathuria", "S. Sudarshan"], "title": ["Efficient and Provable Multi-Query Optimization"], "date": ["2015-12-08T17:56:26Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1512.02568v2"], "summary": ["  Complex queries for massive data analysis jobs have become increasingly\ncommonplace. Many such queries contain com- mon subexpressions, either within a\nsingle query or among multiple queries submitted as a batch. Conventional query\noptimizers do not exploit these subexpressions and produce sub-optimal plans.\nThe problem of multi-query optimization (MQO) is to generate an optimal\ncombined evaluation plan by computing common subexpressions once and reusing\nthem. Exhaustive algorithms for MQO explore an O(n^n) search space. Thus, this\nproblem has primarily been tackled using various heuristic algorithms, without\nproviding any theoretical guarantees on the quality of their solution. In this\npaper, instead of the conventional cost minimization problem, we treat the\nproblem as maximizing a linear transformation of the cost function. We propose\na greedy algorithm for this transformed formulation of the problem, which under\nweak, intuitive assumptions, provides an approximation factor guarantee for\nthis formulation. We go on to show that this factor is optimal, unless P = NP.\nAnother noteworthy point about our algorithm is that it can be easily\nincorporated into existing transformation-based optimizers. We finally propose\noptimizations which can be used to improve the efficiency of our algorithm.\n"]},
{"authors": ["Xing Niu", "Boris Glavic"], "title": ["Optimizing Provenance Computations"], "date": ["2017-01-19T17:12:27Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.05513v1"], "summary": ["  Data provenance is essential for debugging query results, auditing data in\ncloud environments, and explaining outputs of Big Data analytics. A\nwell-established technique is to represent provenance as annotations on data\nand to instrument queries to propagate these annotations to produce results\nannotated with provenance. However, even sophisticated optimizers are often\nincapable of producing efficient execution plans for instrumented queries,\nbecause of their inherent complexity and unusual structure. Thus, while\ninstrumentation enables provenance support for databases without requiring any\nmodification to the DBMS, the performance of this approach is far from optimal.\nIn this work, we develop provenance specific optimizations to address this\nproblem. Specifically, we introduce algebraic equivalences targeted at\ninstrumented queries and discuss alternative, equivalent ways of instrumenting\na query for provenance capture. Furthermore, we present an extensible heuristic\nand cost-based optimization (CBO) framework that governs the application of\nthese optimizations and implement this framework in our GProM provenance\nsystem. Our CBO is agnostic to the plan space shape, uses a DBMS for cost\nestimation, and enables retrofitting of optimization choices into existing code\nby adding a few LOC. Our experiments confirm that these optimizations are\nhighly effective, often improving performance by several orders of magnitude\nfor diverse provenance tasks.\n"]},
{"authors": ["Varunya Attasena", "Nouria Harbi", "J\u00e9r\u00f4me Darmont"], "title": ["A Novel Multi-Secret Sharing Approach for Secure Data Warehousing and\n  On-Line Analysis Processing in the Cloud"], "date": ["2017-01-19T14:54:21Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.05449v1"], "summary": ["  Cloud computing helps reduce costs, increase business agility and deploy\nsolutions with a high return on investment for many types of applications,\nincluding data warehouses and on-line analytical processing. However, storing\nand transferring sensitive data into the cloud raises legitimate security\nconcerns. In this paper, we propose a new multi-secret sharing approach for\ndeploying data warehouses in the cloud and allowing on-line analysis\nprocessing, while enforcing data privacy, integrity and availability. We first\nvalidate the relevance of our approach theoretically and then experimentally\nwith both a simple random dataset and the Star Schema Benchmark. We also\ndemonstrate its superiority to related methods.\n"]},
{"authors": ["Lauren Milechin", "Alexander Chen", "Vijay Gadepally", "Dylan Hutchison", "Siddharth Samsi", "Jeremy Kepner"], "title": ["D4M 3.0"], "date": ["2017-01-19T00:37:12Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1702.03253v1"], "summary": ["  The D4M tool is used by hundreds of researchers to perform complex analytics\non unstructured data. Over the past few years, the D4M toolbox has evolved to\nsupport connectivity with a variety of database engines, graph analytics in the\nApache Accumulo database, and an implementation using the Julia programming\nlanguage. In this article, we describe some of our latest additions to the D4M\ntoolbox and our upcoming D4M 3.0 release.\n"]},
{"authors": ["Kyle OBrien", "Vijay Gadepally", "Jennie Duggan", "Adam Dziedzic", "Aaron Elmore", "Jeremy Kepner", "Samuel Madden", "Tim Mattson", "Zuohao She", "Michael Stonebraker"], "title": ["BigDAWG Polystore Release and Demonstration"], "date": ["2017-01-19T00:29:31Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.05799v1"], "summary": ["  The Intel Science and Technology Center for Big Data is developing a\nreference implementation of a Polystore database. The BigDAWG (Big Data Working\nGroup) system supports \"many sizes\" of database engines, multiple programming\nlanguages and complex analytics for a variety of workloads. Our recent efforts\ninclude application of BigDAWG to an ocean metagenomics problem and\ncontainerization of BigDAWG. We intend to release an open source BigDAWG v1.0\nin the Spring of 2017. In this article, we will demonstrate a number of\npolystore applications developed with oceanographic researchers at MIT and\ndescribe our forthcoming open source release of the BigDAWG system.\n"]},
{"authors": ["Zeinab Bahmani", "Leopoldo Bertossi", "Nikolaos Vasiloglou"], "title": ["ERBlox: Combining Matching Dependencies with Machine Learning for Entity\n  Resolution"], "date": ["2016-02-07T03:06:40Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1602.02334v3"], "summary": ["  Entity resolution (ER), an important and common data cleaning problem, is\nabout detecting data duplicate representations for the same external entities,\nand merging them into single representations. Relatively recently, declarative\nrules called \"matching dependencies\" (MDs) have been proposed for specifying\nsimilarity conditions under which attribute values in database records are\nmerged. In this work we show the process and the benefits of integrating four\ncomponents of ER: (a) Building a classifier for duplicate/non-duplicate record\npairs built using machine learning (ML) techniques; (b) Use of MDs for\nsupporting the blocking phase of ML; (c) Record merging on the basis of the\nclassifier results; and (d) The use of the declarative language \"LogiQL\" -an\nextended form of Datalog supported by the \"LogicBlox\" platform- for all\nactivities related to data processing, and the specification and enforcement of\nMDs.\n"]},
{"authors": ["Romain Perriot", "J\u00e9r\u00e9my Pfeifer", "Laurent D 'Orazio", "Bruno Bachelet", "Sandro Bimonte", "J\u00e9r\u00f4me Darmont"], "title": ["Cost Models for Selecting Materialized Views in Public Clouds"], "date": ["2017-01-18T15:25:07Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.05099v1"], "summary": ["  Data warehouse performance is usually achieved through physical data\nstructures such as indexes or materialized views. In this context, cost models\ncan help select a relevant set ofsuch performance optimization structures.\nNevertheless, selection becomes more complex in the cloud. The criterion to\noptimize is indeed at least two-dimensional, with monetary cost balancing\noverall query response time. This paper introduces new cost models that fit\ninto the pay-as-you-go paradigm of cloud computing. Based on these cost models,\nan optimization problem is defined to discover, among candidate views, those to\nbe materialized to minimize both the overall cost of using and maintaining the\ndatabase in a public cloud and the total response time ofa given query\nworkload. We experimentally show that maintaining materialized views is always\nadvantageous, both in terms of performance and cost.\n"]},
{"authors": ["Semyon Grigorev", "Anastasiya Ragozina"], "title": ["Context-Free Path Querying with Structural Representation of Result"], "date": ["2016-12-28T12:50:49Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1612.08872v2"], "summary": ["  Graph data model and graph databases are very popular in various areas such\nas bioinformatics, semantic web, and social networks. One specific problem in\nthe area is a path querying with constraints formulated in terms of formal\ngrammars. The query in this approach is written as grammar, and paths querying\nis graph parsing with respect to given grammar. There are several solutions to\nit, but how to provide structural representation of query result which is\npractical for answer processing and debugging is still an open problem. In this\npaper we propose a graph parsing technique which allows one to build such\nrepresentation with respect to given grammar in polynomial time and space for\narbitrary context-free grammar and graph. Proposed algorithm is based on\ngeneralized LL parsing algorithm, while previous solutions are based mostly on\nCYK or Earley algorithms, which reduces time complexity in some cases.\n"]},
{"authors": ["Marouane Hachicha", "J\u00e9r\u00f4me Darmont"], "title": ["A Survey of XML Tree Patterns"], "date": ["2017-01-17T12:50:56Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.04652v1"], "summary": ["  With XML becoming an ubiquitous language for data interoperability purposes\nin various domains, efficiently querying XML data is a critical issue. This has\nlead to the design of algebraic frameworks based on tree-shaped patterns akin\nto the tree-structured data model of XML. Tree patterns are graphic\nrepresentations of queries over data trees. They are actually matched against\nan input data tree to answer a query. Since the turn of the twenty-first\ncentury, an astounding research effort has been focusing on tree pattern models\nand matching optimization (a primordial issue). This paper is a comprehensive\nsurvey of these topics, in which we outline and compare the various features of\ntree patterns. We also review and discuss the two main families of approaches\nfor optimizing tree pattern matching, namely pattern tree minimization and\nholistic matching. We finally present actual tree pattern-based developments,\nto provide a global overview of this significant research topic.\n"]},
{"authors": ["Vivek Shah"], "title": ["Transactional Partitioning: A New Abstraction for Main-Memory Databases"], "date": ["2017-01-16T15:56:47Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.04339v1"], "summary": ["  The growth in variety and volume of OLTP (Online Transaction Processing)\napplications poses a challenge to OLTP systems to meet performance and cost\ndemands in the existing hardware landscape. These applications are highly\ninteractive (latency sensitive) and require update consistency. They target\ncommodity hardware for deployment and demand scalability in throughput with\nincreasing clients and data. Currently, OLTP systems used by these applications\nprovide trade-offs in performance and ease of development over a variety of\napplications. In order to bridge the gap between performance and ease of\ndevelopment, we propose an intuitive, high-level programming model which allows\nOLTP applications to be modeled as a cluster of application logic units. By\nextending transactions guaranteeing full ACID semantics to provide the proposed\nmodel, we maintain ease of application development. The model allows the\napplication developer to reason about program performance, and to influence it\nwithout the involvement of OLTP system designers (database designers) and/or\nDBAs. As a result, the database designer is free to focus on efficient running\nof programs to ensure optimal cluster resource utilization.\n"]},
{"authors": ["Xiaowang Zhang", "Jiahui Zhang", "Zhiyong Feng"], "title": ["hMDAP: A Hybrid Framework for Multi-paradigm Data Analytical Processing\n  on Spark"], "date": ["2017-01-16T06:22:24Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.04182v1"], "summary": ["  We propose hMDAP, a hybrid framework for large-scale data analytical\nprocessing on Spark, to support multi-paradigm process (incl. OLAP, machine\nlearning, and graph analysis etc.) in distributed environments. The framework\nfeatures a three-layer data process module and a business process module which\ncontrols the former. We will demonstrate the strength of hMDAP by using traffic\nscenarios in a real world.\n"]},
{"authors": ["Antoine Amarilli", "Pierre Bourhis", "Mika\u00ebl Monet", "Pierre Senellart"], "title": ["Combined Tractability of Query Evaluation via Tree Automata and Cycluits\n  (Extended Version)"], "date": ["2016-12-13T14:33:48Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1612.04203v2"], "summary": ["  We investigate parameterizations of both database instances and queries that\nmake query evaluation fixed-parameter tractable in combined complexity. We\nintroduce a new Datalog fragment with stratified negation,\nintensional-clique-guarded Datalog (ICG-Datalog), with linear-time evaluation\non structures of bounded treewidth for programs of bounded rule size. Such\nprograms capture in particular conjunctive queries with simplicial\ndecompositions of bounded width, guarded negation fragment queries of bounded\nCQ-rank, or two-way regular path queries. Our result proceeds via compilation\nto alternating two-way automata, whose semantics is defined via cyclic\nprovenance circuits (cycluits) that can be tractably evaluated. Last, we prove\nthat probabilistic query evaluation remains intractable in combined complexity\nunder this parameterization.\n"]},
{"authors": ["Qiong Li", "Xiaowang Zhang", "Zhiyong Feng"], "title": ["PRSP: A Plugin-based Framework for RDF Stream Processing"], "date": ["2017-01-14T00:17:53Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.03854v1"], "summary": ["  In this paper, we propose a plugin-based framework for RDF stream processing\nnamed PRSP. Within this framework, we can employ SPARQL query engines to\nprocess C-SPARQL queries with maintaining the high performance of those engines\nin a simple way. Taking advantage of PRSP, we can process large-scale RDF\nstreams in a distributed context via distributed SPARQL engines. Besides, we\ncan evaluate the performance and correctness of existing SPARQL query engines\nin handling RDF streams in a united way, which amends the evaluation of them\nranging from static RDF (i.e., RDF graph) to dynamic RDF (i.e., RDF stream).\nFinally, within PRSP, we experimently evaluate the correctness and the\nperformance on YABench. The experiments show that PRSP can still maintain the\nhigh performance of those engines in RDF stream processing although there are\nsome slight differences among them.\n"]},
{"authors": ["Besat Kassaie"], "title": ["SPARQL over GraphX"], "date": ["2017-01-11T18:38:16Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.03091v1"], "summary": ["  The ability of the RDF data model to link data from heterogeneous domains has\nled to an explosive growth of RDF data. So, evaluating SPARQL queries over\nlarge RDF data has been crucial for the semantic web community. However, due to\nthe graph nature of RDF data, evaluating SPARQL queries in relational databases\nand common data-parallel systems needs a lot of joins and is inefficient. On\nthe other hand, the enormity of datasets that are graph in nature such as\nsocial network data, has led the database community to develop graph-parallel\nprocessing systems to support iterative graph computations efficiently. In this\nwork we take advantage of the graph representation of RDF data and exploit\nGraphX, a new graph processing system based on Spark. We propose a subgraph\nmatching algorithm, compatible with the GraphX programming model to evaluate\nSPARQL queries. Some experiments are performed to show the system scalability\nto handle large datasets.\n"]},
{"authors": ["Antoine Amarilli", "Yael Amsterdamer", "Tova Milo", "Pierre Senellart"], "title": ["Top-k Querying of Unknown Values under Order Constraints (Extended\n  Version)"], "date": ["2017-01-10T15:21:11Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.02634v1"], "summary": ["  Many practical scenarios make it necessary to evaluate top-k queries over\ndata items with partially unknown values. This paper considers a setting where\nthe values are taken from a numerical domain, and where some partial order\nconstraints are given over known and unknown values: under these constraints,\nwe assume that all possible worlds are equally likely. Our work is the first to\npropose a principled scheme to derive the value distributions and expected\nvalues of unknown items in this setting, with the goal of computing estimated\ntop-k results by interpolating the unknown values from the known ones. We study\nthe complexity of this general task, and show tight complexity bounds, proving\nthat the problem is intractable, but can be tractably approximated. We then\nconsider the case of tree-shaped partial orders, where we show a constructive\nPTIME solution. We also compare our problem setting to other top-k definitions\non uncertain data.\n"]},
{"authors": ["Thomas Schwentick", "Nils Vortmeier", "Thomas Zeume"], "title": ["Dynamic Complexity under Definable Changes"], "date": ["2017-01-10T09:43:10Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.02494v1"], "summary": ["  This paper studies dynamic complexity under definable change operations in\nthe DynFO framework by Patnaik and Immerman. It is shown that for changes\ndefinable by parameter-free first-order formulas, all (uniform) $AC^1$ queries\ncan be maintained by first-order dynamic programs. Furthermore, many\nmaintenance results for single-tuple changes are extended to more powerful\nchange operations: (1) The reachability query for undirected graphs is\nfirst-order maintainable under single tuple changes and first-order defined\ninsertions, likewise the reachability query for directed acyclic graphs under\nquantifier-free insertions. (2) Context-free languages are first-order\nmaintainable under $\\Sigma_1$-defined changes. These results are complemented\nby several inexpressibility results, for example, that the reachability query\ncannot be maintained by quantifier-free programs under definable,\nquantifier-free deletions.\n"]},
{"authors": ["Andriy V. Miranskyy", "Zainab Al-zanbouri", "David Godwin", "Ayse Basar Bener"], "title": ["Database Engines: Evolution of Greenness"], "date": ["2017-01-09T20:47:21Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.02344v1"], "summary": ["  Context: Information Technology consumes up to 10\\% of the world's\nelectricity generation, contributing to CO2 emissions and high energy costs.\nData centers, particularly databases, use up to 23% of this energy. Therefore,\nbuilding an energy-efficient (green) database engine could reduce energy\nconsumption and CO2 emissions.\n  Goal: To understand the factors driving databases' energy consumption and\nexecution time throughout their evolution.\n  Method: We conducted an empirical case study of energy consumption by two\nMySQL database engines, InnoDB and MyISAM, across 40 releases. We examined the\nrelationships of four software metrics to energy consumption and execution time\nto determine which metrics reflect the greenness and performance of a database.\n  Results: Our analysis shows that database engines' energy consumption and\nexecution time increase as databases evolve. Moreover, the Lines of Code metric\nis correlated moderately to strongly with energy consumption and execution time\nin 88% of cases.\n  Conclusions: Our findings provide insights to both practitioners and\nresearchers. Database administrators may use them to select a fast, green\nrelease of the MySQL database engine. MySQL database-engine developers may use\nthe software metric to assess products' greenness and performance. Researchers\nmay use our findings to further develop new hypotheses or build models to\npredict greenness and performance of databases.\n"]},
{"authors": ["Pierre Bourhis", "Juan L. Reutter", "Fernando Su\u00e1rez", "Domagoj Vrgo\u010d"], "title": ["JSON: data model, query languages and schema specification"], "date": ["2017-01-09T15:53:41Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.02221v1"], "summary": ["  Despite the fact that JSON is currently one of the most popular formats for\nexchanging data on the Web, there are very few studies on this topic and there\nare no agreement upon theoretical framework for dealing with JSON. There- fore\nin this paper we propose a formal data model for JSON documents and, based on\nthe common features present in available systems using JSON, we define a\nlightweight query language allowing us to navigate through JSON documents. We\nalso introduce a logic capturing the schema proposal for JSON and study the\ncomplexity of basic computational tasks associated with these two formalisms.\n"]},
{"authors": ["Daniel Lemire", "Christoph Rupp"], "title": ["Upscaledb: Efficient Integer-Key Compression in a Key-Value Store using\n  SIMD Instructions"], "date": ["2016-11-16T20:17:07Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1611.05428v2"], "summary": ["  Compression can sometimes improve performance by making more of the data\navailable to the processors faster. We consider the compression of integer keys\nin a B+-tree index. For this purpose, systems such as IBM DB2 use variable-byte\ncompression over differentially coded keys. We revisit this problem with\nvarious compression alternatives such as Google's VarIntGB, Binary Packing and\nFrame-of-Reference. In all cases, we describe algorithms that can operate\ndirectly on compressed data. Many of our alternatives exploit the\nsingle-instruction-multiple-data (SIMD) instructions supported by modern CPUs.\nWe evaluate our techniques in a database environment provided by Upscaledb, a\nproduction-quality key-value database. Our best techniques are SIMD\naccelerated: they simultaneously reduce memory usage while improving\nsingle-threaded speeds. In particular, a differentially coded SIMD\nbinary-packing techniques (BP128) can offer a superior query speed (e.g., 40%\nbetter than an uncompressed database) while providing the best compression\n(e.g., by a factor of ten). For analytic workloads, our fast compression\ntechniques offer compelling benefits. Our software is available as open source.\n"]},
{"authors": ["Alfredo Cuzzocrea", "J\u00e9r\u00f4me Darmont", "Hadj Mahboubi"], "title": ["Fragmenting very large XML data warehouses via K-means clustering\n  algorithm"], "date": ["2017-01-09T14:26:35Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.02190v1"], "summary": ["  XML data sources are more and more gaining popularity in the context of a\nwide family of Business Intelligence (BI) and On-Line Analytical Processing\n(OLAP) applications, due to the amenities of XML in representing and managing\nsemi-structured and complex multidimensional data. As a consequence, many XML\ndata warehouse models have been proposed during past years in order to handle\nhetero-geneity and complexity of multidimensional data in a way traditional\nrelational data warehouse approaches fail to achieve. However, XML-native\ndatabase systems currently suffer from limited performance, both in terms of\nvolumes of manageable data and query response time. Therefore , recent research\nefforts are focusing the attention on fragmentation techniques, which are able\nto overcome the limitations above. Derived horizontal fragmentation is already\nused in relational data warehouses, and can definitely be adapted to the XML\ncontext. However, classical fragmentation algorithms are not suitable to\ncontrol the number of originated fragments, which instead plays a critical role\nin data warehouses, and, with more emphasis, distributed data warehouse\narchitectures. Inspired by this research challenge, in this paper we propose\nthe use of K-means clustering algorithm for effectively and efficiently\nsupporting the fragmentation of very large XML data warehouses, and, at the\nsame time, completely controlling and determining the number of originated\nfragments via adequately setting the parameter K. We complete our analytical\ncontribution by means of a comprehensive experimental assessment where we\ncompare the efficiency of our proposed XML data warehouse fragmentation\ntechnique against those of classical derived horizontal fragmentation\nalgorithms adapted to XML data warehouses.\n"]},
{"authors": ["Christopher R. Aberger", "Susan Tu", "Kunle Olukotun", "Christopher R\u00e9"], "title": ["EmptyHeaded: A Relational Engine for Graph Processing"], "date": ["2015-03-09T04:02:36Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1503.02368v7"], "summary": ["  There are two types of high-performance graph processing engines: low- and\nhigh-level engines. Low-level engines (Galois, PowerGraph, Snap) provide\noptimized data structures and computation models but require users to write\nlow-level imperative code, hence ensuring that efficiency is the burden of the\nuser. In high-level engines, users write in query languages like datalog\n(SociaLite) or SQL (Grail). High-level engines are easier to use but are orders\nof magnitude slower than the low-level graph engines. We present EmptyHeaded, a\nhigh-level engine that supports a rich datalog-like query language and achieves\nperformance comparable to that of low-level engines. At the core of\nEmptyHeaded's design is a new class of join algorithms that satisfy strong\ntheoretical guarantees but have thus far not achieved performance comparable to\nthat of specialized graph processing engines. To achieve high performance,\nEmptyHeaded introduces a new join engine architecture, including a novel query\noptimizer and data layouts that leverage single-instruction multiple data\n(SIMD) parallelism. With this architecture, EmptyHeaded outperforms high-level\napproaches by up to three orders of magnitude on graph pattern queries,\nPageRank, and Single-Source Shortest Paths (SSSP) and is an order of magnitude\nfaster than many low-level baselines. We validate that EmptyHeaded competes\nwith the best-of-breed low-level engine (Galois), achieving comparable\nperformance on PageRank and at most 3x worse performance on SSSP.\n"]},
{"authors": ["Dinusha Vatsalan", "Peter Christen", "Erhard Rahm"], "title": ["Scalable Multi-Database Privacy-Preserving Record Linkage using Counting\n  Bloom Filters"], "date": ["2017-01-05T07:57:55Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.01232v1"], "summary": ["  Privacy-preserving record linkage (PPRL) aims at integrating sensitive\ninformation from multiple disparate databases of different organizations. PPRL\napproaches are increasingly required in real-world application areas such as\nhealthcare, national security, and business. Previous approaches have mostly\nfocused on linking only two databases as well as the use of a dedicated linkage\nunit. Scaling PPRL to more databases (multi-party PPRL) is an open challenge\nsince privacy threats as well as the computation and communication costs for\nrecord linkage increase significantly with the number of databases. We thus\npropose the use of a new encoding method of sensitive data based on Counting\nBloom Filters (CBF) to improve privacy for multi-party PPRL. We also\ninvestigate optimizations to reduce communication and computation costs for\nCBF-based multi-party PPRL with and without the use of a dedicated linkage\nunit. Empirical evaluations conducted with real datasets show the viability of\nthe proposed approaches and demonstrate their scalability, linkage quality, and\nprivacy protection.\n"]},
{"authors": ["Karamjit Singh", "Garima Gupta", "Gautam Shroff", "Puneet Agarwal"], "title": ["Minimally-Supervised Attribute Fusion for Data Lakes"], "date": ["2017-01-04T18:19:19Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.01094v1"], "summary": ["  Aggregate analysis, such as comparing country-wise sales versus global market\nshare across product categories, is often complicated by the unavailability of\ncommon join attributes, e.g., category, across diverse datasets from different\ngeographies or retail chains, even after disparate data is technically ingested\ninto a common data lake. Sometimes this is a missing data issue, while in other\ncases it may be inherent, e.g., the records in different geographical databases\nmay actually describe different product 'SKUs', or follow different norms for\ncategorization. Record linkage techniques can be used to automatically map\nproducts in different data sources to a common set of global attributes,\nthereby enabling federated aggregation joins to be performed. Traditional\nrecord-linkage techniques are typically unsupervised, relying textual\nsimilarity features across attributes to estimate matches. In this paper, we\npresent an ensemble model combining minimal supervision using Bayesian network\nmodels together with unsupervised textual matching for automating such\n'attribute fusion'. We present results of our approach on a large volume of\nreal-life data from a market-research scenario and compare with a standard\nrecord matching algorithm. Finally we illustrate how attribute fusion using\nmachine learning could be included as a data-lake management feature,\nespecially as our approach also provides confidence values for matches,\nenabling human intervention, if required.\n"]},
{"authors": ["Cristina Feier", "Antti Kuusisto", "Carsten Lutz"], "title": ["Rewritability in Monadic Disjunctive Datalog, MMSNP, and Expressive\n  Description Logics"], "date": ["2017-01-03T18:17:15Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.02231v1"], "summary": ["  We study rewritability of monadic disjunctive Datalog programs, (the\ncomplements of) MMSNP sentences, and ontology-mediated queries (OMQs) based on\nexpressive description logics of the ALC family and on conjunctive queries. We\nshow that rewritability into FO and into monadic Datalog (MDLog) are decidable,\nand that rewritability into Datalog is decidable when the original query\nsatisfies a certain condition related to equality. We establish\n2NExpTime-completeness for all studied problems except rewritability into MDLog\nfor which there remains a gap between 2NExpTime and 3ExpTime. We also analyze\nthe shape of rewritings, which in the MMSNP case correspond to obstructions,\nand give a new construction of canonical Datalog programs that is more\nelementary than existing ones and also applies to formulas with free variables.\n"]},
{"authors": ["Michael Hanus", "Julia Krone"], "title": ["A Typeful Integration of SQL into Curry"], "date": ["2017-01-03T10:32:32Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.00631v1"], "summary": ["  We present an extension of the declarative programming language Curry to\nsupport the access to data stored in relational databases via SQL. Since Curry\nis statically typed, our emphasis on this SQL integration is on type safety.\nOur extension respects the type system of Curry so that run-time errors due to\nill-typed data are avoided. This is obtained by preprocessing SQL statements at\ncompile time and translating them into type-safe database access operations. As\na consequence, the type checker of the Curry system can spot type errors in SQL\nstatements at compile time. To generate appropriately typed access operations,\nthe preprocessor uses an entity-relationship (ER) model describing the\nstructure of the relational data. In addition to standard SQL, SQL statements\nembedded in Curry can include program expressions and also relationships\nspecified in the ER model. The latter feature is useful to avoid the\nerror-prone use of foreign keys. As a result, our SQL integration supports a\nhigh-level and type-safe access to databases in Curry programs.\n"]},
{"authors": ["Stefan Brass", "Heike Stephan"], "title": ["Experiences with Some Benchmarks for Deductive Databases and\n  Implementations of Bottom-Up Evaluation"], "date": ["2017-01-03T10:31:41Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.00627v1"], "summary": ["  OpenRuleBench is a large benchmark suite for rule engines, which includes\ndeductive databases. We previously proposed a translation of Datalog to C++\nbased on a method that \"pushes\" derived tuples immediately to places where they\nare used. In this paper, we report performance results of various\nimplementation variants of this method compared to XSB, YAP and DLV. We study\nonly a fraction of the OpenRuleBench problems, but we give a quite detailed\nanalysis of each such task and the factors which influence performance. The\nresults not only show the potential of our method and implementation approach,\nbut could be valuable for anybody implementing systems which should be able to\nexecute tasks of the discussed types.\n"]},
{"authors": ["Falco Nogatz", "Dietmar Seipel"], "title": ["Implementing GraphQL as a Query Language for Deductive Databases in\n  SWI-Prolog Using DCGs, Quasi Quotations, and Dicts"], "date": ["2017-01-03T10:31:26Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.00626v1"], "summary": ["  The methods to access large relational databases in a distributed system are\nwell established: the relational query language SQL often serves as a language\nfor data access and manipulation, and in addition public interfaces are exposed\nusing communication protocols like REST. Similarly to REST, GraphQL is the\nquery protocol of an application layer developed by Facebook. It provides a\nunified interface between the client and the server for data fetching and\nmanipulation. Using GraphQL's type system, it is possible to specify data\nhandling of various sources and to combine, e.g., relational with NoSQL\ndatabases. In contrast to REST, GraphQL provides a single API endpoint and\nsupports flexible queries over linked data.\n  GraphQL can also be used as an interface for deductive databases. In this\npaper, we give an introduction of GraphQL and a comparison to REST. Using\nlanguage features recently added to SWI-Prolog 7, we have developed the Prolog\nlibrary GraphQL.pl, which implements the GraphQL type system and query syntax\nas a domain-specific language with the help of definite clause grammars (DCG),\nquasi quotations, and dicts. Using our library, the type system created for a\ndeductive database can be validated, while the query system provides a unified\ninterface for data access and introspection.\n"]},
{"authors": ["Stefan Brass", "Heike Stephan"], "title": ["Bottom-Up Evaluation of Datalog: Preliminary Report"], "date": ["2017-01-03T10:30:51Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.00623v1"], "summary": ["  Bottom-up evaluation of Datalog has been studied for a long time, and is\nstandard material in textbooks. However, if one actually wants to develop a\ndeductive database system, it turns out that there are many implementation\noptions. For instance, the sequence in which rule instances are applied is not\ngiven. In this paper, we study a method that immediately uses a derived tuple\nto derive more tuples (called the Push method). In this way, storage space for\nintermediate results can be reduced. The main contribution of our method is the\nway in which we minimize the copying of values at runtime, and do much work\nalready at compile-time.\n"]},
{"authors": ["Dietmar Seipel"], "title": ["Knowledge Engineering for Hybrid Deductive Databases"], "date": ["2017-01-03T10:30:37Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.00622v1"], "summary": ["  Modern knowledge base systems frequently need to combine a collection of\ndatabases in different formats: e.g., relational databases, XML databases, rule\nbases, ontologies, etc. In the deductive database system DDBASE, we can manage\nthese different formats of knowledge and reason about them. Even the file\nsystems on different computers can be part of the knowledge base. Often, it is\nnecessary to handle different versions of a knowledge base. E.g., we might want\nto find out common parts or differences of two versions of a relational\ndatabase.\n  We will examine the use of abstractions of rule bases by predicate dependency\nand rule predicate graphs. Also the proof trees of derived atoms can help to\ncompare different versions of a rule base. Moreover, it might be possible to\nhave derivations joining rules with other formalisms of knowledge\nrepresentation.\n  Ontologies have shown their benefits in many applications of intelligent\nsystems, and there have been many proposals for rule languages compatible with\nthe semantic web stack, e.g., SWRL, the semantic web rule language. Recently,\nontologies are used in hybrid systems for specifying the provenance of the\ndifferent components.\n"]},
{"authors": ["Zhen He", "J\u00e9r\u00f4me Darmont"], "title": ["Evaluating the Dynamic Behavior of Database Applications"], "date": ["2017-01-02T14:20:12Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.00400v1"], "summary": ["  This paper explores the effect that changing access patterns has on the\nperformance of database management systems. Changes in access patterns play an\nimportant role in determining the efficiency of key performance optimization\ntechniques, such as dynamic clustering, prefetching, and buffer replacement.\nHowever, all existing benchmarks or evaluation frameworks produce static access\npatterns in which objects are always accessed in the same order repeatedly.\nHence, we have proposed the Dynamic Evaluation Framework (DEF) that simulates\naccess pattern changes using configurable styles of change. DEF has been\ndesigned to be open and fully extensible (e.g., new access pattern change\nmodels can be added easily). In this paper, we instantiate DEF into the Dynamic\nobject Evaluation Framework (DoEF) which is designed for object databases,\ni.e., object-oriented or object-relational databases such as multi-media\ndatabases or most XML databases.The capabilities of DoEF have been evaluated by\nsimulating the execution of four different dynamic clustering algorithms. The\nresults confirm our analysis that flexible conservative re-clustering is the\nkey in determining a clustering algorithm's ability to adapt to changes in\naccess pattern. These results show the effectiveness of DoEF at determining the\nadaptability of each dynamic clustering algorithm to changes in access pattern\nin a simulation environment. In a second set of experiments, we have used DoEF\nto compare the performance of two real-life object stores : Platypus and SHORE.\nDoEF has helped to reveal the poor swapping performance of Platypus.\n"]},
{"authors": ["J\u00e9r\u00f4me Darmont", "Fadila Bentayeb", "Omar Boussa\u00efd"], "title": ["Benchmarking data warehouses"], "date": ["2017-01-02T14:19:24Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.00399v1"], "summary": ["  Data warehouse architectural choices and optimization techniques are critical\nto decision support query performance. To facilitate these choices, the\nperformance of the designed data warehouse must be assessed, usually with\nbenchmarks. These tools can either help system users comparing the performances\nof different systems, or help system engineers testing the effect of various\ndesign choices. While the Transaction Processing Performance Council's standard\nbenchmarks address the first point, they are not tunable enough to address the\nsecond one and fail to model different data warehouse schemas. By contrast, our\nData Warehouse Engineering Benchmark (DWEB) allows generating various ad-hoc\nsynthetic data warehouses and workloads. DWEB is implemented as a Java free\nsoftware that can be interfaced with most existing relational database\nmanagement systems. The full specifications of DWEB, as well as experiments we\nperformed to illustrate how our benchmark may be used, are provided in this\npaper.\n"]},
{"authors": ["Omar Boussaid", "Jerome Darmont", "Fadila Bentayeb", "Sabine Loudcher"], "title": ["Warehousing complex data from the Web"], "date": ["2017-01-02T14:18:01Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.00398v1"], "summary": ["  The data warehousing and OLAP technologies are now moving onto handling\ncomplex data that mostly originate from the Web. However, intagrating such data\ninto a decision-support process requires their representation under a form\nprocessable by OLAP and/or data mining techniques. We present in this paper a\ncomplex data warehousing methodology that exploits XML as a pivot language. Our\napproach includes the integration of complex data in an ODS, under the form of\nXML documents; their dimensional modeling and storage in an XML data warehouse;\nand their analysis with combined OLAP and data mining techniques. We also\naddress the crucial issue of performance in XML warehouses.\n"]},
{"authors": ["Jeevan Joishi", "Ashish Sureka"], "title": ["Graph or Relational Databases: A Speed Comparison for Process Mining\n  Algorithm"], "date": ["2016-12-31T08:00:07Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1701.00072v1"], "summary": ["  Process-Aware Information System (PAIS) are IT systems that manages, supports\nbusiness processes and generate large event logs from execution of business\nprocesses. An event log is represented as a tuple of the form CaseID,\nTimeStamp, Activity and Actor. Process Mining is an emerging area of research\nthat deals with the study and analysis of business processes based on event\nlogs. Process Mining aims at analyzing event logs and discover business process\nmodels, enhance them or check for conformance with an a priori model. The large\nvolume of event logs generated are stored in databases. Relational databases\nperform well for certain class of applications. However, there are certain\nclass of applications for which relational databases are not able to scale. A\nnumber of NoSQL databases have emerged to encounter the challenges of\nscalability. Discovering social network from event logs is one of the most\nchallenging and important Process Mining task. Similar-Task and Sub-Contract\nalgorithms are some of the most widely used Organizational Mining techniques.\nOur objective is to investigate which of the databases (Relational or Graph)\nperform better for Organizational Mining under Process Mining. An intersection\nof Process Mining and Graph Databases can be accomplished by modelling these\nOrganizational Mining metrics with graph databases. We implement Similar-Task\nand Sub-Contract algorithms on relational and NoSQL (graph-oriented) databases\nusing only query language constructs. We conduct empirical analysis on a large\nreal world data set to compare the performance of row-oriented database and\nNoSQL graph-oriented database. We benchmark performance factors like query\nexecution time, CPU usage and disk/memory space usage for NoSQL graph-oriented\ndatabase against row-oriented database.\n"]},
{"authors": ["Eugenia Ternovska"], "title": ["Lifted Relational Algebra with Recursion and Connections to Modal Logic"], "date": ["2016-12-29T19:17:31Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1612.09251v1"], "summary": ["  We propose a new formalism for specifying and reasoning about problems that\ninvolve heterogeneous \"pieces of information\" -- large collections of data,\ndecision procedures of any kind and complexity and connections between them.\nThe essence of our proposal is to lift Codd's relational algebra from\noperations on relational tables to operations on classes of structures (with\nrecursion), and to add a direction of information propagation. We observe the\npresence of information propagation in several formalisms for efficient\nreasoning and use it to express unary negation and operations used in graph\ndatabases. We carefully analyze several reasoning tasks and establish a precise\nconnection between a generalized query evaluation and temporal logic model\nchecking. Our development allows us to reveal a general correspondence between\nclassical and modal logics and may shed a new light on the good computational\nproperties of modal logics and related formalisms.\n"]},
{"authors": ["Xiaoyang Chen", "Hongwei Huo", "Jun Huan", "Jeffrey Scott Vitter"], "title": ["MSQ-Index: A Succinct Index for Fast Graph Similarity Search"], "date": ["2016-12-29T14:23:46Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1612.09155v1"], "summary": ["  Graph similarity search has received considerable attention in many\napplications, such as bioinformatics, data mining, pattern recognition, and\nsocial networks. Existing methods for this problem have limited scalability\nbecause of the huge amount of memory they consume when handling very large\ngraph databases with millions or billions of graphs.\n  In this paper, we study the problem of graph similarity search under the\ngraph edit distance constraint. We present a space-efficient index structure\nbased upon the q-gram tree that incorporates succinct data structures and\nhybrid encoding to achieve improved query time performance with minimal space\nusage. Specifically, the space usage of our index requires only 5%-15% of the\nprevious state-of-the-art indexing size on the tested data while at the same\ntime achieving 2-3 times acceleration in query time with small data sets. We\nalso boost the query performance by augmenting the global filter with range\nsearch, which allows us to perform a query in a reduced region. In addition, we\npropose two effective filters that combine degree structures and label\nstructures. Extensive experiments demonstrate that our proposed approach is\nsuperior in space and competitive in filtering to the state-of-the-art\napproaches. To the best of our knowledge, our index is the first in-memory\nindex for this problem that successfully scales to cope with the large dataset\nof 25 million chemical structure graphs from the PubChem dataset.\n"]},
{"authors": ["Dinusha Vatsalan", "Peter Christen"], "title": ["Multi-Party Privacy-Preserving Record Linkage using Bloom Filters"], "date": ["2016-12-28T09:30:00Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1612.08835v1"], "summary": ["  Privacy-preserving record linkage (PPRL), the problem of identifying records\nthat correspond to the same real-world entity across several data sources held\nby different parties without revealing any sensitive information about these\nrecords, is increasingly being required in many real-world application areas.\nExamples range from public health surveillance to crime and fraud detection,\nand national security. Various techniques have been developed to tackle the\nproblem of PPRL, with the majority of them considering linking data from only\ntwo sources. However, in many real-world applications data from more than two\nsources need to be linked. In this paper we propose a viable solution for\nmulti-party PPRL using two efficient privacy techniques: Bloom filter encoding\nand distributed secure summation. Our proposed protocol efficiently identifies\nmatching sets of records held by all data sources that have a similarity above\na certain minimum threshold. While being efficient, our protocol is also secure\nunder the semi-honest adversary model in that no party can learn any sensitive\ninformation about any other parties' data, but all parties learn which of their\nrecords have a high similarity with records held by the other parties. We\nevaluate our protocol on a large real voter registration database showing the\nscalability, linkage quality, and privacy of our approach.\n"]},
{"authors": ["Wolfgang Gatterbauer"], "title": ["Semi-Supervised Learning with Heterophily"], "date": ["2014-12-09T20:58:45Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1412.3100v2"], "summary": ["  We derive a family of linear inference algorithms that generalize existing\ngraph-based label propagation algorithms by allowing them to propagate\ngeneralized assumptions about \"attraction\" or \"compatibility\" between classes\nof neighboring nodes (in particular those that involve heterophily between\nnodes where \"opposites attract\"). We thus call this formulation Semi-Supervised\nLearning with Heterophily (SSLH) and show how it generalizes and improves upon\na recently proposed approach called Linearized Belief Propagation (LinBP).\nImportantly, our framework allows us to reduce the problem of estimating the\nrelative compatibility between nodes from partially labeled graph to a simple\noptimization problem. The result is a very fast algorithm that -- despite its\nsimplicity -- is surprisingly effective: we can classify unlabeled nodes within\nthe same graph in the same time as LinBP but with a superior accuracy and\ndespite our algorithm not knowing the compatibilities.\n"]},
{"authors": ["Amir Hossein Akhavan Rahnama"], "title": ["Distributed Real-Time Sentiment Analysis for Big Data Social Streams"], "date": ["2016-12-27T09:10:18Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1612.08543v1"], "summary": ["  Big data trend has enforced the data-centric systems to have continuous fast\ndata streams. In recent years, real-time analytics on stream data has formed\ninto a new research field, which aims to answer queries about\nwhat-is-happening-now with a negligible delay. The real challenge with\nreal-time stream data processing is that it is impossible to store instances of\ndata, and therefore online analytical algorithms are utilized. To perform\nreal-time analytics, pre-processing of data should be performed in a way that\nonly a short summary of stream is stored in main memory. In addition, due to\nhigh speed of arrival, average processing time for each instance of data should\nbe in such a way that incoming instances are not lost without being captured.\nLastly, the learner needs to provide high analytical accuracy measures.\nSentinel is a distributed system written in Java that aims to solve this\nchallenge by enforcing both the processing and learning process to be done in\ndistributed form. Sentinel is built on top of Apache Storm, a distributed\ncomputing platform. Sentinels learner, Vertical Hoeffding Tree, is a parallel\ndecision tree-learning algorithm based on the VFDT, with ability of enabling\nparallel classification in distributed environments. Sentinel also uses\nSpaceSaving to keep a summary of the data stream and stores its summary in a\nsynopsis data structure. Application of Sentinel on Twitter Public Stream API\nis shown and the results are discussed.\n"]},
{"authors": ["Niranjan Kamat", "Arnab Nandi"], "title": ["A Closer Look at Variance Implementations in Modern Database Systems"], "date": ["2015-09-14T23:00:28Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1509.04349v2"], "summary": ["  Variance is a popular and often necessary component of sampled aggregation\nqueries. It is typically used as a secondary measure to ascertain statistical\nproperties of the result such as its error. Yet, it is more expensive to\ncompute than simple, primary measures such as \\texttt{SUM}, \\texttt{MEAN}, and\n\\texttt{COUNT}.\n  There exist numerous techniques to compute variance. While the definition of\nvariance is considered to require multiple passes on the data, other\nmathematical representations can compute the value in a single pass. Some\nsingle-pass representations, however, can suffer from severe precision loss,\nespecially for large number of data points.\n  In this paper, we study variance implementations in various real-world\nsystems and find that major database systems such as PostgreSQL 9.4 and most\nlikely System X, a major commercially used closed-source database, use a\nrepresentation that is efficient, but suffers from floating point precision\nloss resulting from catastrophic cancellation. We note deficiencies in another\npopular representation, used by databases such as MySQL and Impala, that\nsuffers from not being distributive and therefore cannot take advantage of\nmodern parallel computational resources. We review literature over the past\nfive decades on variance calculation in both the statistics and database\ncommunities, and summarize recommendations on implementing variance functions\nin various settings, such as approximate query processing and large-scale\ndistributed aggregation.\n"]},
{"authors": ["Mahmoud Abo Khamis", "Hung Q. Ngo", "Christopher R\u00e9", "Atri Rudra"], "title": ["Joins via Geometric Resolutions: Worst-case and Beyond"], "date": ["2014-04-02T20:50:22Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1404.0703v7"], "summary": ["  We present a simple geometric framework for the relational join. Using this\nframework, we design an algorithm that achieves the fractional hypertree-width\nbound, which generalizes classical and recent worst-case algorithmic results on\ncomputing joins. In addition, we use our framework and the same algorithm to\nshow a series of what are colloquially known as beyond worst-case results. The\nframework allows us to prove results for data stored in Btrees,\nmultidimensional data structures, and even multiple indices per table. A key\nidea in our framework is formalizing the inference one does with an index as a\ntype of geometric resolution; transforming the algorithmic problem of computing\njoins to a geometric problem. Our notion of geometric resolution can be viewed\nas a geometric analog of logical resolution. In addition to the geometry and\nlogic connections, our algorithm can also be thought of as backtracking search\nwith memoization.\n"]},
{"authors": ["Jiaheng Lu", "Zhen Hua Liu", "Pengfei Xu", "Chao Zhang"], "title": ["UDBMS: Road to Unification for Multi-model Data Management"], "date": ["2016-12-23T17:47:15Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1612.08050v1"], "summary": ["  A traditional database systems is organized around a single data model that\ndetermines how data can be organized, stored and manipulated. But the vision of\nthis paper is to develop new principles and techniques to manage multiple data\nmodels against a single, integrated backend. For example, semi-structured,\ngraph and relational models are examples of data models that may be supported\nby a new system. Having a single data platform for managing both\nwell-structured data and NoSQL data is beneficial to users; this approach\nsignificantly reduces integration, migration, development, maintenance and\noperational issues. The problem is challenging: the existing database\nprinciples mainly work for a single model and the research on multi-model data\nmanagement is still at an early stage. In this paper, we envision a UDBMS\n(Unified Database Management System) for multi-model data management in one\nplatform. UDBMS will provide several new features such as unified data model\nand flexible schema, unified query processing, unified index structure and\ncross-model transaction guarantees. We discuss our vision as well as present\nmultiple research challenges that we need to address.\n"]},
{"authors": ["Jiongqian Liang", "Srinivasan Parthasarathy"], "title": ["Robust Contextual Outlier Detection: Where Context Meets Sparsity"], "date": ["2016-07-28T06:40:30Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1607.08329v3"], "summary": ["  Outlier detection is a fundamental data science task with applications\nranging from data cleaning to network security. Given the fundamental nature of\nthe task, this has been the subject of much research. Recently, a new class of\noutlier detection algorithms has emerged, called {\\it contextual outlier\ndetection}, and has shown improved performance when studying anomalous behavior\nin a specific context. However, as we point out in this article, such\napproaches have limited applicability in situations where the context is sparse\n(i.e. lacking a suitable frame of reference). Moreover, approaches developed to\ndate do not scale to large datasets. To address these problems, here we propose\na novel and robust approach alternative to the state-of-the-art called RObust\nContextual Outlier Detection (ROCOD). We utilize a local and global behavioral\nmodel based on the relevant contexts, which is then integrated in a natural and\nrobust fashion. We also present several optimizations to improve the\nscalability of the approach. We run ROCOD on both synthetic and real-world\ndatasets and demonstrate that it outperforms other competitive baselines on the\naxes of efficacy and efficiency (40X speedup compared to modern contextual\noutlier detection methods). We also drill down and perform a fine-grained\nanalysis to shed light on the rationale for the performance gains of ROCOD and\nreveal its effectiveness when handling objects with sparse contexts.\n"]},
{"authors": ["Gaetan de Rassenfosse", "Martin Kracker", "Gianluca Tarasconi"], "title": ["Getting Started with PATSTAT Register"], "date": ["2016-12-22T10:05:45Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1612.07514v1"], "summary": ["  This paper provides a technical introduction to the PATSTAT Register\ndatabase, which contains bibliographical, procedural and legal status data on\npatent applications handled by the European Patent Office. It presents eight\nMySQL queries that cover some of the most relevant aspects of the database for\nresearch purposes. It targets academic researchers and practitioners who are\nfamiliar with the PATSTAT database and the MySQL language.\n"]},
{"authors": ["Arijit Khan"], "title": ["Vertex-Centric Graph Processing: The Good, the Bad, and the Ugly"], "date": ["2016-12-22T00:53:41Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1612.07404v1"], "summary": ["  We study distributed graph algorithms that adopt an iterative vertex-centric\nframework for graph processing, popularized by the Google's Pregel system.\nSince then, there are several attempts to implement many graph algorithms in a\nvertex-centric framework, as well as efforts to design optimization techniques\nfor improving the efficiency. However, to the best of our knowledge, there has\nnot been any systematic study to compare these vertex-centric implementations\nwith their sequential counterparts. Our paper addresses this gap in two ways.\n(1) We analyze the computational complexity of such implementations with the\nnotion of time-processor product, and benchmark several vertex-centric graph\nalgorithms whether they perform more work with respect to their best-known\nsequential solutions. (2) Employing the concept of balanced practical Pregel\nalgorithms, we study if these implementations suffer from imbalanced workload\nand large number of iterations. Our findings illustrate that with the exception\nof Euler tour tree algorithm, all other algorithms either perform more work\nthan their best-known sequential approach, or suffer from imbalanced workload/\nlarge number of iterations, or even both. We also emphasize on graph algorithms\nthat are fundamentally difficult to be expressed in vertex-centric frameworks,\nand conclude by discussing the road ahead for distributed graph processing.\n"]},
{"authors": ["Sairam Gurajada", "Martin Theobald"], "title": ["Distributed Processing of Generalized Graph-Pattern Queries in SPARQL\n  1.1"], "date": ["2016-09-17T08:06:28Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1609.05293v2"], "summary": ["  We propose an efficient and scalable architecture for processing generalized\ngraph-pattern queries as they are specified by the current W3C recommendation\nof the SPARQL 1.1 \"Query Language\" component. Specifically, the class of\nqueries we consider consists of sets of SPARQL triple patterns with labeled\nproperty paths. From a relational perspective, this class resolves to\nconjunctive queries of relational joins with additional graph-reachability\npredicates. For the scalable, i.e., distributed, processing of this kind of\nqueries over very large RDF collections, we develop a suitable partitioning and\nindexing scheme, which allows us to shard the RDF triples over an entire\ncluster of compute nodes and to process an incoming SPARQL query over all of\nthe relevant graph partitions (and thus compute nodes) in parallel. Unlike most\nprior works in this field, we specifically aim at the unified optimization and\ndistributed processing of queries consisting of both relational joins and\ngraph-reachability predicates. All communication among the compute nodes is\nestablished via a proprietary, asynchronous communication protocol based on the\nMessage Passing Interface.\n"]},
{"authors": ["Matej Mihel\u010di\u0107", "Sa\u0161o D\u017eeroski", "Nada Lavra\u010d", "Tomislav \u0160muc"], "title": ["A framework for redescription set construction"], "date": ["2016-06-13T13:15:41Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1606.03935v2"], "summary": ["  Redescription mining is a field of knowledge discovery that aims at finding\ndifferent descriptions of similar subsets of instances in the data. These\ndescriptions are represented as rules inferred from one or more disjoint sets\nof attributes, called views. As such, they support knowledge discovery process\nand help domain experts in formulating new hypotheses or constructing new\nknowledge bases and decision support systems. In contrast to previous\napproaches that typically create one smaller set of redescriptions satisfying a\npre-defined set of constraints, we introduce a framework that creates large and\nheterogeneous redescription set from which user/expert can extract compact sets\nof differing properties, according to its own preferences. Construction of\nlarge and heterogeneous redescription set relies on CLUS-RM algorithm and a\nnovel, conjunctive refinement procedure that facilitates generation of larger\nand more accurate redescription sets. The work also introduces the variability\nof redescription accuracy when missing values are present in the data, which\nsignificantly extends applicability of the method. Crucial part of the\nframework is the redescription set extraction based on heuristic\nmulti-objective optimization procedure that allows user to define importance\nlevels towards one or more redescription quality criteria. We provide both\ntheoretical and empirical comparison of the novel framework against current\nstate of the art redescription mining algorithms and show that it represents\nmore efficient and versatile approach for mining redescriptions from data.\n"]},
{"authors": ["Ciprian-Octavian Truic\u0103", "J\u00e9r\u00f4me Darmont", "Julien Velcin"], "title": ["A Scalable Document-based Architecture for Text Analysis"], "date": ["2016-12-19T14:24:23Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1612.06195v1"], "summary": ["  Analyzing textual data is a very challenging task because of the huge volume\nof data generated daily. Fundamental issues in text analysis include the lack\nof structure in document datasets, the need for various preprocessing steps\n%(e.g., stem or lemma extraction, part-of-speech tagging, named entities\nrecognition...), and performance and scaling issues. Existing text analysis\narchitectures partly solve these issues, providing restrictive data schemas,\naddressing only one aspect of text preprocessing and focusing on one single\ntask when dealing with performance optimization. %As a result, no definite\nsolution is currently available. Thus, we propose in this paper a new generic\ntext analysis architecture, where document structure is flexible, many\npreprocessing techniques are integrated and textual datasets are indexed for\nefficient access. We implement our conceptual architecture using both a\nrelational and a document-oriented database. Our experiments demonstrate the\nfeasibility of our approach and the superiority of the document-oriented\nlogical and physical implementation.\n"]},
{"authors": ["Manas Joglekar", "Hector Garcia-Molina", "Aditya Parameswaran"], "title": ["Interactive Data Exploration with Smart Drill-Down"], "date": ["2014-12-01T07:09:14Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1412.0364v3"], "summary": ["  We present {\\em smart drill-down}, an operator for interactively exploring a\nrelational table to discover and summarize \"interesting\" groups of tuples. Each\ngroup of tuples is described by a {\\em rule}. For instance, the rule $(a, b,\n\\star, 1000)$ tells us that there are a thousand tuples with value $a$ in the\nfirst column and $b$ in the second column (and any value in the third column).\nSmart drill-down presents an analyst with a list of rules that together\ndescribe interesting aspects of the table. The analyst can tailor the\ndefinition of interesting, and can interactively apply smart drill-down on an\nexisting rule to explore that part of the table. We demonstrate that the\nunderlying optimization problems are {\\sc NP-Hard}, and describe an algorithm\nfor finding the approximately optimal list of rules to display when the user\nuses a smart drill-down, and a dynamic sampling scheme for efficiently\ninteracting with large tables. Finally, we perform experiments on real datasets\non our experimental prototype to demonstrate the usefulness of smart drill-down\nand study the performance of our algorithms.\n"]},
{"authors": ["Afsin Akdogan"], "title": ["Partitioning, Indexing and Querying Spatial Data on Cloud"], "date": ["2016-12-18T06:24:06Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1612.05858v1"], "summary": ["  The number of mobile devices (e.g., smartphones, wearable technologies) is\nrapidly growing. In line with this trend, a massive amount of spatial data is\nbeing collected since these devices allow users to geo-tag user-generated\ncontent. Clearly, a scalable computing infrastructure is needed to manage such\nlarge datasets. Meanwhile, Cloud Computing service providers (e.g., Amazon,\nGoogle, and Microsoft) allow users to lease computing resources. However, most\nof the existing spatial indexing techniques are designed for the centralized\nparadigm which is limited to the capabilities of a single sever. To address the\nscalability shortcomings of existing approaches, we provide a study that focus\non generating a distributed spatial index structure that not only scales out to\nmultiple servers but also scales up since it fully exploits the multi-core CPUs\navailable on each server using Voronoi diagram as the partitioning and indexing\ntechnique which we also use to process spatial queries effectively. More\nspecifically, since the data objects continuously move and issue position\nupdates to the index structure, we collect the latest positions of objects and\nperiodically generate a read-only index to eliminate costly distributed\nupdates. Our approach scales near-linearly in index construction and query\nprocessing, and can efficiently construct an index for millions of objects\nwithin a few seconds. In addition to scalability and efficiency, we also aim to\nmaximize the server utilization that can support the same workload with less\nnumber of servers. Server utilization is a crucial point while using Cloud\nComputing because users are charged based on the total amount of time they\nreserve each server, with no consideration of utilization.\n"]},
{"authors": ["Luis Gal\u00e1rraga", "Simon Razniewski", "Antoine Amarilli", "Fabian M. Suchanek"], "title": ["Predicting Completeness in Knowledge Bases"], "date": ["2016-12-17T16:08:04Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1612.05786v1"], "summary": ["  Knowledge bases such as Wikidata, DBpedia, or YAGO contain millions of\nentities and facts. In some knowledge bases, the correctness of these facts has\nbeen evaluated. However, much less is known about their completeness, i.e., the\nproportion of real facts that the knowledge bases cover. In this work, we\ninvestigate different signals to identify the areas where a knowledge base is\ncomplete. We show that we can combine these signals in a rule mining approach,\nwhich allows us to predict where facts may be missing. We also show that\ncompleteness predictions can help other applications such as fact prediction.\n"]},
{"authors": ["Yodsawalai Chodpathumwan", "Amirhossein Aleyasin", "Arash Termehchy", "Yizhou Sun"], "title": ["Representation Independent Proximity and Similarity Search"], "date": ["2015-08-15T19:07:47Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1508.03763v3"], "summary": ["  Finding similar or strongly related entities in a graph database is a\nfundamental problem in data management and analytics with applications in\nsimilarity query processing, entity resolution, and pattern matching.\nSimilarity search algorithms usually leverage the structural properties of the\ndata graph to quantify the degree of similarity or relevance between entities.\nNevertheless, the same information can be represented in many different\nstructures and the structural properties observed over particular\nrepresentations do not necessarily hold for alternative structures. Thus, these\nalgorithms are effective on some representations and ineffective on others. We\npostulate that a similarity search algorithm should return essentially the same\nanswers over different databases that represent the same information. We\nformally define the property of representation independence for similarity\nsearch algorithms as their robustness against transformations that modify the\nstructure of databases and preserve their information content. We formalize two\nwidespread groups of such transformations called {\\it relationship\nreorganizing} and {\\it entity rearranging} transformations. We show that\ncurrent similarity search algorithms are not representation independent under\nthese transformations and propose an algorithm called {\\bf R-PathSim}, which is\nprovably robust under these transformations. We perform an extensive empirical\nstudy on the representation independence of current similarity search\nalgorithms under relationship reorganizing and entity rearranging\ntransformations. Our empirical results suggest that current similarity search\nalgorithms except for R-PathSim are highly sensitive to the data\nrepresentation. These results also indicate that R-PathSim is as effective or\nmore effective than other similarity search algorithms.\n"]},
{"authors": ["Amir Shaikhha", "Yannis Klonatos", "Christoph Koch"], "title": ["Building Efficient Query Engines in a High-Level Language"], "date": ["2016-12-16T17:32:40Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1612.05566v1"], "summary": ["  Abstraction without regret refers to the vision of using high-level\nprogramming languages for systems development without experiencing a negative\nimpact on performance. A database system designed according to this vision\noffers both increased productivity and high performance, instead of sacrificing\nthe former for the latter as is the case with existing, monolithic\nimplementations that are hard to maintain and extend. In this article, we\nrealize this vision in the domain of analytical query processing. We present\nLegoBase, a query engine written in the high-level language Scala. The key\ntechnique to regain efficiency is to apply generative programming: LegoBase\nperforms source-to-source compilation and optimizes the entire query engine by\nconverting the high-level Scala code to specialized, low-level C code. We show\nhow generative programming allows to easily implement a wide spectrum of\noptimizations, such as introducing data partitioning or switching from a row to\na column data layout, which are difficult to achieve with existing low-level\nquery compilers that handle only queries. We demonstrate that sufficiently\npowerful abstractions are essential for dealing with the complexity of the\noptimization effort, shielding developers from compiler internals and\ndecoupling individual optimizations from each other. We evaluate our approach\nwith the TPC-H benchmark and show that: (a) With all optimizations enabled,\nLegoBase significantly outperforms a commercial database and an existing query\ncompiler. (b) Programmers need to provide just a few hundred lines of\nhigh-level code for implementing the optimizations, instead of complicated\nlow-level code that is required by existing query compilation approaches. (c)\nThe compilation overhead is low compared to the overall execution time, thus\nmaking our approach usable in practice for compiling query engines.\n"]},
{"authors": ["Ilya Kolchinsky", "Assaf Schuster", "Danny Keren"], "title": ["Efficient Detection of Complex Event Patterns Using Lazy Chain Automata"], "date": ["2016-12-15T15:27:37Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1612.05110v1"], "summary": ["  Complex Event Processing (CEP) is an emerging field with important\napplications in many areas. CEP systems collect events arriving from input data\nstreams and use them to infer more complex events according to predefined\npatterns. The Non-deterministic Finite Automaton (NFA) is one of the most\npopular mechanisms on which such systems are based. During the event detection\nprocess, NFAs incrementally extend previously observed partial matches until a\nfull match for the query is found. As a result, each arriving event needs to be\nprocessed to determine whether a new partial match is to be initiated or an\nexisting one extended. This method may be highly inefficient when many of the\nevents do not result in output matches. We present a lazy evaluation mechanism\nthat defers processing of frequent event types and stores them internally upon\narrival. Events are then matched in ascending order of frequency, thus\nminimizing potentially redundant computations. We introduce a Lazy Chain NFA,\nwhich utilizes the above principle, and does not depend on the underlying\npattern structure. An algorithm for constructing a Lazy Chain NFA for common\npattern types is presented, including conjunction, negation and iteration.\nFinally, we experimentally evaluate our mechanism on real-world stock trading\ndata. The results demonstrate a performance gain of two orders of magnitude\nover traditional NFA-based approaches, with significantly reduced memory\nresource requirements.\n"]},
{"authors": ["Peter Christen"], "title": ["Application of Advanced Record Linkage Techniques for Complex Population\n  Reconstruction"], "date": ["2016-12-13T17:10:11Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1612.04286v1"], "summary": ["  Record linkage is the process of identifying records that refer to the same\nentities from several databases. This process is challenging because commonly\nno unique entity identifiers are available. Linkage therefore has to rely on\npartially identifying attributes, such as names and addresses of people. Recent\nyears have seen the development of novel techniques for linking data from\ndiverse application areas, where a major focus has been on linking complex data\nthat contain records about different types of entities. Advanced approaches\nthat exploit both the similarities between record attributes as well as the\nrelationships between entities to identify clusters of matching records have\nbeen developed.\n  In this application paper we study the novel problem where rather than\ndifferent types of entities we have databases where the same entity can have\ndifferent roles, and where these roles change over time. We specifically\ndevelop novel techniques for linking historical birth, death, marriage and\ncensus records with the aim to reconstruct the population covered by these\nrecords over a period of several decades. Our experimental evaluation on real\nScottish data shows that even with advanced linkage techniques that consider\ngroup, relationship, and temporal aspects it is challenging to achieve high\nquality linkage from such complex data.\n"]},
{"authors": ["Nieves R. Brisaboa", "Ana Cerdeira-Pena", "Narciso L\u00f3pez-L\u00f3pez", "Gonzalo Navarro", "Miguel R. Penabad", "Fernando Silva-Coira"], "title": ["Efficient Representation of Multidimensional Data over Hierarchical\n  Domains"], "date": ["2016-12-13T10:52:03Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1612.04094v1"], "summary": ["  We consider the problem of representing multidimensional data where the\ndomain of each dimension is organized hierarchically, and the queries require\nsummary information at a different node in the hierarchy of each dimension.\nThis is the typical case of OLAP databases. A basic approach is to represent\neach hierarchy as a one-dimensional line and recast the queries as\nmultidimensional range queries. This approach can be implemented compactly by\ngeneralizing to more dimensions the $k^2$-treap, a compact representation of\ntwo-dimensional points that allows for efficient summarization queries along\ngeneric ranges. Instead, we propose a more flexible generalization, which\ninstead of a generic quadtree-like partition of the space, follows the domain\nhierarchies across each dimension to organize the partitioning. The resulting\nstructure is much more efficient than a generic multidimensional structure,\nsince queries are resolved by aggregating much fewer nodes of the tree.\n"]},
{"authors": ["Junhua Fang", "Rong Zhang", "Tom Z. J. Fu", "Zhenjie Zhang", "Aoying Zhou", "Junhua Zhu"], "title": ["Parallel Stream Processing Against Workload Skewness and Variance"], "date": ["2016-10-17T14:03:41Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1610.05121v2"], "summary": ["  Key-based workload partitioning is a common strategy used in parallel stream\nprocessing engines, enabling effective key-value tuple distribution over worker\nthreads in a logical operator. While randomized hashing on the keys is capable\nof balancing the workload for key-based partitioning when the keys generally\nfollow a static distribution, it is likely to generate poor balancing\nperformance when workload variance occurs on the incoming data stream. This\npaper presents a new key-based workload partitioning framework, with practical\nalgorithms to support dynamic workload assignment for stateful operators. The\nframework combines hash-based and explicit key-based routing strategies for\nworkload distribution, which specifies the destination worker threads for a\nhandful of keys and assigns the other keys with the hashing function. When\nshort-term distribution fluctuations occur to the incoming data stream, the\nsystem adaptively updates the routing table containing the chosen keys, in\norder to rebalance the workload with minimal migration overhead within the\nstateful operator. We formulate the rebalance operation as an optimization\nproblem, with multiple objectives on minimizing state migration costs,\ncontrolling the size of the routing table and breaking workload imbalance among\nworker threads. Despite of the NP-hardness nature behind the optimization\nformulation, we carefully investigate and justify the heuristics behind key\n(re)routing and state migration, to facilitate fast response to workload\nvariance with ignorable cost to the normal processing in the distributed\nsystem. Empirical studies on synthetic data and real-world stream applications\nvalidate the usefulness of our proposals and prove the huge advantage of our\napproaches over state-of-the-art solutions in the literature.\n"]},
{"authors": ["Vera Zaychik Moffitt", "Julia Stoyanovich"], "title": ["Querying Evolving Graphs with Portal"], "date": ["2016-02-02T03:10:45Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1602.00773v2"], "summary": ["  Graphs are used to represent a plethora of phenomena, from the Web and social\nnetworks, to biological pathways, to semantic knowledge bases. Arguably the\nmost interesting and important questions one can ask about graphs have to do\nwith their evolution. Which Web pages are showing an increasing popularity\ntrend? How does influence propagate in social networks? How does knowledge\nevolve?\n  This paper proposes a logical model of an evolving graph called a TGraph,\nwhich captures evolution of graph topology and of its vertex and edge\nattributes. We present a compositional temporal graph algebra TGA, and show a\nreduction of TGA to temporal relational algebra with graph-specific primitives.\nWe formally study the properties of TGA, and also show that it is sufficient to\nconcisely express a wide range of common use cases. We describe an\nimplementation of our model and algebra in Portal, built on top of Apache Spark\n/ GraphX. We conduct extensive experiments on real datasets, and show that\nPortal scales.\n"]},
{"authors": ["Andrei Sorin Sabau"], "title": ["Stream Clustering using Probabilistic Data Structures"], "date": ["2016-12-08T15:43:54Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1612.02701v1"], "summary": ["  Most density based stream clustering algorithms separate the clustering\nprocess into an online and offline component. Exact summarized statistics are\nbeing employed for defining micro-clusters or grid cells during the online\nstage followed by macro-clustering during the offline stage. This paper\nproposes a novel alternative to the traditional two phase stream clustering\nscheme, introducing sketch-based data structures for assessing both stream\ndensity and cluster membership with probabilistic accuracy guarantees. A\ncount-min sketch using a damped window model estimates stream density. Bloom\nfilters employing a variation of active-active buffering estimate cluster\nmembership. Instances of both types of sketches share the same set of hash\nfunctions. The resulting stream clustering algorithm is capable of detecting\narbitrarily shaped clusters while correctly handling outliers and making no\nassumption on the total number of clusters. Experimental results over a number\nof real and synthetic datasets illustrate the proposed algorithm quality and\nefficiency.\n"]},
{"authors": ["Parmita Mehta", "Sven Dorkenwald", "Dongfang Zhao", "Tomer Kaftan", "Alvin Cheung", "Magdalena Balazinska", "Ariel Rokem", "Andrew Connolly", "Jacob Vanderplas", "Yusra AlSayyad"], "title": ["Comparative Evaluation of Big-Data Systems on Scientific Image Analytics\n  Workloads"], "date": ["2016-12-07T23:29:43Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1612.02485v1"], "summary": ["  Scientific discoveries are increasingly driven by analyzing large volumes of\nimage data. Many new libraries and specialized database management systems\n(DBMSs) have emerged to support such tasks. It is unclear, however, how well\nthese systems support real-world image analysis use cases, and how performant\nare the image analytics tasks implemented on top of such systems. In this\npaper, we present the first comprehensive evaluation of large-scale image\nanalysis systems using two real-world scientific image data processing use\ncases. We evaluate five representative systems (SciDB, Myria, Spark, Dask, and\nTensorFlow) and find that each of them has shortcomings that complicate\nimplementation or hurt performance. Such shortcomings lead to new research\nopportunities in making large-scale image analysis both efficient and easy to\nuse.\n"]},
{"authors": ["Guillaume Bagan", "Angela Bonifati", "Radu Ciucanu", "George H. L. Fletcher", "Aur\u00e9lien Lemay", "Nicky Advokaat"], "title": ["gMark: Schema-Driven Generation of Graphs and Queries"], "date": ["2015-11-26T13:36:25Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1511.08386v6"], "summary": ["  Massive graph data sets are pervasive in contemporary application domains.\nHence, graph database systems are becoming increasingly important. In the\nexperimental study of these systems, it is vital that the research community\nhas shared solutions for the generation of database instances and query\nworkloads having predictable and controllable properties. In this paper, we\npresent the design and engineering principles of gMark, a domain- and query\nlanguage-independent graph instance and query workload generator. A core\ncontribution of gMark is its ability to target and control the diversity of\nproperties of both the generated instances and the generated workloads coupled\nto these instances. Further novelties include support for regular path queries,\na fundamental graph query paradigm, and schema-driven selectivity estimation of\nqueries, a key feature in controlling workload chokepoints. We illustrate the\nflexibility and practical usability of gMark by showcasing the framework's\ncapabilities in generating high quality graphs and workloads, and its ability\nto encode user-defined schemas across a variety of application domains.\n"]},
{"authors": ["Thomas Beyhl", "Holger Giese"], "title": ["Incremental View Maintenance for Deductive Graph Databases Using\n  Generalized Discrimination Networks"], "date": ["2016-12-06T02:36:47Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1612.01641v1"], "summary": ["  Nowadays, graph databases are employed when relationships between entities\nare in the scope of database queries to avoid performance-critical join\noperations of relational databases. Graph queries are used to query and modify\ngraphs stored in graph databases. Graph queries employ graph pattern matching\nthat is NP-complete for subgraph isomorphism. Graph database views can be\nemployed that keep ready answers in terms of precalculated graph pattern\nmatches for often stated and complex graph queries to increase query\nperformance. However, such graph database views must be kept consistent with\nthe graphs stored in the graph database.\n  In this paper, we describe how to use incremental graph pattern matching as\ntechnique for maintaining graph database views. We present an incremental\nmaintenance algorithm for graph database views, which works for imperatively\nand declaratively specified graph queries. The evaluation shows that our\nmaintenance algorithm scales when the number of nodes and edges stored in the\ngraph database increases. Furthermore, our evaluation shows that our approach\ncan outperform existing approaches for the incremental maintenance of graph\nquery results.\n"]},
{"authors": ["Vachik S. Dave", "Mohammad Al Hasan"], "title": ["TopCom: Index for Shortest Distance Query in Directed Graph"], "date": ["2016-02-04T02:02:05Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1602.01537v2"], "summary": ["  Finding shortest distance between two vertices in a graph is an important\nproblem due to its numerous applications in diverse domains, including\ngeo-spatial databases, social network analysis, and information retrieval.\nClassical algorithms (such as, Dijkstra) solve this problem in polynomial time,\nbut these algorithms cannot provide real-time response for a large number of\nbursty queries on a large graph. So, indexing based solutions that pre-process\nthe graph for efficiently answering (exactly or approximately) a large number\nof distance queries in real-time is becoming increasingly popular. Existing\nsolutions have varying performance in terms of index size, index building time,\nquery time, and accuracy. In this work, we propose T OP C OM , a novel\nindexing-based solution for exactly answering distance queries. Our experiments\nwith two of the existing state-of-the-art methods (IS-Label and TreeMap) show\nthe superiority of T OP C OM over these two methods considering scalability and\nquery time. Besides, indexing of T OP C OM exploits the DAG (directed acyclic\ngraph) structure in the graph, which makes it significantly faster than the\nexisting methods if the SCCs (strongly connected component) of the input graph\nare relatively small.\n"]},
{"authors": ["Eid Aldikanji", "Khalil Ajami"], "title": ["Studying Academic Indicators within Virtual Learning Environment Using\n  Educational Data Mining"], "date": ["2016-12-04T09:38:20Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1612.01090v1"], "summary": ["  Our main goal is to discover the main factors influencing students' academic\ntrajectory and students' academic evolution within such environment. Our\nresults indicate strong correlation in this virtual learning environment\nbetween student average and some factors like: student's English level (despite\nthe fact that Arabic language is the teaching language), student's age,\nstudent's gender, student's over-stay and student's place of residence (inside\nor outside Syria). Our results indicate also a need to modify the academic\ntrajectory of students by changing the prerequisites of few courses delivered\nas a part of BIT diploma like Advanced DBA II, Data Security. In this research,\nthe results also highlight the effect of the Syrian Crisis on students.\nFinally, we've suggested some future recommendations based on our observations\nand results to develop the current information system in SVU in order to help\nus to deduce some indicators more easily.\n"]},
{"authors": ["Wan-Lei Zhao", "Cheng-Hao Deng", "Chong-Wah Ngo"], "title": ["Boost K-Means"], "date": ["2016-10-08T04:36:42Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1610.02483v2"], "summary": ["  Due to its simplicity and versatility, k-means remains popular since it was\nproposed three decades ago. The performance of k-means has been enhanced from\ndifferent perspectives over the years. Unfortunately, a good trade-off between\nquality and efficiency is hardly reached. In this paper, a novel k-means\nvariant is presented. Different from most of k-means variants, the clustering\nprocedure is driven by an explicit objective function, which is feasible for\nthe whole l2-space. The classic egg-chicken loop in k-means has been simplified\nto a pure stochastic optimization procedure. The procedure of k-means becomes\nsimpler and converges to a considerably better local optima. The effectiveness\nof this new variant has been studied extensively in different contexts, such as\ndocument clustering, nearest neighbor search and image clustering. Superior\nperformance is observed across different scenarios.\n"]},
{"authors": ["Zheguang Zhao", "Lorenzo De Stefani", "Emanuel Zgraggen", "Carsten Binnig", "Eli Upfal", "Tim Kraska"], "title": ["Controlling False Discoveries During Interactive Data Exploration"], "date": ["2016-12-04T00:24:17Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1612.01040v1"], "summary": ["  Recent tools for interactive data exploration significantly increase the\nchance that users make false discoveries. The crux is that these tools\nimplicitly allow the user to test a large body of different hypotheses with\njust a few clicks thus incurring in the issue commonly known in statistics as\nthe multiple hypothesis testing error. In this paper, we propose solutions to\nintegrate multiple hypothesis testing control into interactive data exploration\ntools. A key insight is that existing methods for controlling the false\ndiscovery rate (such as FDR) are not directly applicable for interactive data\nexploration. We therefore discuss a set of new control procedures that are\nbetter suited and integrated them in our system called Aware. By means of\nextensive experiments using both real-world and synthetic data sets we\ndemonstrate how Aware can help experts and novice users alike to efficiently\ncontrol false discoveries.\n"]},
{"authors": ["Singh Vijendra", "Priyanka Trikha"], "title": ["Density Based Algorithm With Automatic Parameters Generation"], "date": ["2016-12-02T10:38:49Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1612.00623v1"], "summary": ["  The traditional algorithms do not meet the latest multiple requirements\nsimultaneously for objects. Density-based method is one of the methodologies,\nwhich can detect arbitrary shaped clusters where clusters are defined as dense\nregions separated by low density regions. In this paper, we present a new\nclustering algorithm to enhance the density-based algorithm DBSCAN. This\nenables an automatic parameter generation strategy to create clusters with\ndifferent densities and enables noises recognition, and generates arbitrary\nshaped clusters. The kdtree is used for increasing the memory efficiency.\nExperimental result shows that proposed algorithm is capable of handling\ncomplex objects with good memory efficiency and accuracy.\n"]},
{"authors": ["Vinay Deolalikar", "Hernan Laffitte"], "title": ["Extensive Large-Scale Study of Error in Samping-Based Distinct Value\n  Estimators for Databases"], "date": ["2016-12-01T21:33:25Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1612.00476v1"], "summary": ["  The problem of distinct value estimation has many applications. Being a\ncritical component of query optimizers in databases, it also has high\ncommercial impact. Many distinct value estimators have been proposed, using\nvarious statistical approaches. However, characterizing the errors incurred by\nthese estimators is an open problem: existing analytical approaches are not\npowerful enough, and extensive empirical studies at large scale do not exist.\nWe conduct an extensive large-scale empirical study of 11 distinct value\nestimators from four different approaches to the problem over families of\nZipfian distributions whose parameters model real-world applications. Our study\nis the first that \\emph{scales to the size of a billion-rows} that today's\nlarge commercial databases have to operate in. This allows us to characterize\nthe error that is encountered in real-world applications of distinct value\nestimation. By mining the generated data, we show that estimator error depends\non a key latent parameter --- the average uniform class size --- that has not\nbeen studied previously. This parameter also allows us to unearth error\npatterns that were previously unknown. Importantly, ours is the first approach\nthat provides a framework for \\emph{visualizing the error patterns} in distinct\nvalue estimation, facilitating discussion of this problem in enterprise\nsettings. Our characterization of errors can be used for several problems in\ndistinct value estimation, such as the design of hybrid estimators. This work\naims at the practitioner and the researcher alike, and addresses questions\nfrequently asked by both audiences.\n"]},
{"authors": ["Singh Vijendra", "Hemjyotsana Parashar", "Nisha Vasudeva"], "title": ["A New Method for Classification of Datasets for Data Mining"], "date": ["2016-12-01T05:24:36Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1612.00151v1"], "summary": ["  Decision tree is an important method for both induction research and data\nmining, which is mainly used for model classification and prediction. ID3\nalgorithm is the most widely used algorithm in the decision tree so far. In\nthis paper, the shortcoming of ID3's inclining to choose attributes with many\nvalues is discussed, and then a new decision tree algorithm which is improved\nversion of ID3. In our proposed algorithm attributes are divided into groups\nand then we apply the selection measure 5 for these groups. If information gain\nis not good then again divide attributes values into groups. These steps are\ndone until we get good classification/misclassification ratio. The proposed\nalgorithms classify the data sets more accurately and efficiently.\n"]},
{"authors": ["Shichao Zhang"], "title": ["Data Partitioning View of Mining Big Data"], "date": ["2016-11-29T16:05:56Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1611.09691v1"], "summary": ["  There are two main approximations of mining big data in memory. One is to\npartition a big dataset to several subsets, so as to mine each subset in\nmemory. By this way, global patterns can be obtained by synthesizing all local\npatterns discovered from these subsets. Another is the statistical sampling\nmethod. This indicates that data partitioning should be an important strategy\nfor mining big data. This paper recalls our work on mining big data with a data\npartitioning and shows some interesting findings among the local patterns\ndiscovered from subsets of a dataset.\n"]},
{"authors": ["J\u00e9r\u00f4me Darmont", "Le Gruenwald"], "title": ["A comparison study of object-oriented database clustering techniques"], "date": ["2016-11-28T15:19:52Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1611.09177v1"], "summary": ["  It is widely acknowledged that a good object clustering is critical to the\nperformance of OODBs. Clustering means storing related objects close together\non secondary storage so that when one object is accessed from disk, all its\nrelated objects are also brought into memory. Then access to these related\nobjects is a main memory access that is much faster than a disk access. The aim\nof this paper is to compare the performance of three clustering algorithms:\nCactis, CK and ORION. Simulation experiments we performed showed that the\nCactis algorithm is better than the ORION algorithm and that the CK algorithm\ntotally out-performs both other algorithms in terms of response time and\nclustering overhead.\n"]},
{"authors": ["J\u00e9r\u00f4me Darmont", "Amar Attoui", "Michel Gourgand"], "title": ["Simulation of clustering algorithms in OODBs in order to evaluate their\n  performances"], "date": ["2016-11-28T15:19:14Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1611.09176v1"], "summary": ["  A good object clustering is critical to the performance of object-oriented\ndatabases. However, it always involves some kind of overhead for the system.\nThe aim of this paper is to propose a modelling methodology in order to\nevaluate the performances of different clustering policies. This methodology\nhas been used to compare the performances of three clustering algorithms found\nin the literature (Cactis, CK and ORION) that we considered representative of\nthe current research in the field of object clustering. The actual performance\nevaluation was performed using simulation. Simulation experiments showed that\nthe Cactis algorithm is better than the ORION algorithm and that the CK\nalgorithm totally outperforms both other algorithms in terms of response time\nand clustering overhead.\n"]},
{"authors": ["J\u00e9r\u00f4me Darmont", "Michel Schneider"], "title": ["Benchmarking OODBs with a Generic Tool"], "date": ["2016-11-28T15:17:18Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1611.09172v1"], "summary": ["  We present in this paper a generic object-oriented benchmark (OCB: the Object\nClustering Benchmark) that has been designed to evaluate the performances of\nObject-Oriented Data-bases (OODBs), and more specifically the performances of\nclustering policies within OODBs. OCB is generic because its sample database\nmay be customized to fit any of the databases in-troduced by the main existing\nbenchmarks, e.g., OO1 (Object Operation 1) or OO7. The first version of OCB was\npurposely clustering-oriented due to a clustering-oriented workload, but OCB\nhas been thoroughly extended to be able to suit other purposes. Eventually,\nOCB's code is compact and easily portable. OCB has been validated through two\nimplementations: one within the O2 OODB and another one within the Texas\npersistent object store. The perfor-mances of a specific clustering policy\ncalled DSTC (Dynamic, Statistical, Tunable Clustering) have also been evaluated\nwith OCB.\n"]},
{"authors": ["J\u00e9r\u00f4me Darmont"], "title": ["DESP-C++: A Discrete-Event Simulation Package for C++"], "date": ["2016-11-28T15:16:42Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1611.09170v1"], "summary": ["  DESP-C++ is a C++ discrete-event random simulation engine that has been\ndesigned to be fast, very easy to use and expand, and valid. DESP-C++ is based\non the resource view. Its complete architecture is presented in detail, as well\nas a short \" user manual \". The validity of DESP-C++ is demonstrated by the\nsimulation of three significant models. In each case, the simulation results\nobtained with DESP-C++ match those obtained with a validated simulation\nsoftware: QNAP2. The versatility of DESP-C++ is also illustrated this way,\nsince the modelled systems are very different from each other: a simple\nproduction system, the dining philosopher classical deadlock problem, and a\ncomplex object-oriented database management system.\n"]},
{"authors": ["Dario Garcia-Gasulla", "Eduard Ayguad\u00e9", "Jes\u00fas Labarta", "Ulises Cort\u00e9s"], "title": ["Limitations and Alternatives for the Evaluation of Large-scale Link\n  Prediction"], "date": ["2016-11-02T11:07:51Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1611.00547v2"], "summary": ["  Link prediction, the problem of identifying missing links among a set of\ninter-related data entities, is a popular field of research due to its\napplication to graph-like domains. Producing consistent evaluations of the\nperformance of the many link prediction algorithms being proposed can be\nchallenging due to variable graph properties, such as size and density. In this\npaper we first discuss traditional data mining solutions which are applicable\nto link prediction evaluation, arguing about their capacity for producing\nfaithful and useful evaluations. We also introduce an innovative modification\nto a traditional evaluation methodology with the goal of adapting it to the\nproblem of evaluating link prediction algorithms when applied to large graphs,\nby tackling the problem of class imbalance. We empirically evaluate the\nproposed methodology and, building on these findings, make a case for its\nimportance on the evaluation of large-scale graph processing.\n"]},
{"authors": ["Xiangnan Ren", "Houda Khrouf", "Zakia Kazi-Aoul", "Yousra Chabchoub", "Olivier Cur\u00e9"], "title": ["On measuring performances of C-SPARQL and CQELS"], "date": ["2016-11-24T17:43:48Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1611.08269v1"], "summary": ["  To cope with the massive growth of semantic data streams, several RDF Stream\nProcessing (RSP) engines have been implemented. The efficiency of their\nthroughput, latency and memory consumption can be evaluated using available\nbenchmarks such as LSBench and City- Bench. Nevertheless, these benchmarks lack\nan in-depth performance evaluation as some measurement metrics have not been\nconsidered. The main goal of this paper is to analyze the performance of two\npopular RSP engines, namely C-SPARQL and CQELS, when varying a set of\nperformance metrics. More precisely, we evaluate the impact of stream rate,\nnumber of streams and window size on execution time as well as on memory\nconsumption.\n"]},
{"authors": ["Maaz Bin Safeer Ahmad", "Alvin Cheung"], "title": ["Leveraging Parallel Data Processing Frameworks with Verified Lifting"], "date": ["2016-11-23T03:16:38Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1611.07623v1"], "summary": ["  Many parallel data frameworks have been proposed in recent years that let\nsequential programs access parallel processing. To capitalize on the benefits\nof such frameworks, existing code must often be rewritten to the\ndomain-specific languages that each framework supports. This rewriting-tedious\nand error-prone-also requires developers to choose the framework that best\noptimizes performance given a specific workload.\n  This paper describes Casper, a novel compiler that automatically retargets\nsequential Java code for execution on Hadoop, a parallel data processing\nframework that implements the MapReduce paradigm. Given a sequential code\nfragment, Casper uses verified lifting to infer a high-level summary expressed\nin our program specification language that is then compiled for execution on\nHadoop. We demonstrate that Casper automatically translates Java benchmarks\ninto Hadoop. The translated results execute on average 3.3x faster than the\nsequential implementations and scale better, as well, to larger datasets.\n"]},
{"authors": ["Pengfei Xu", "Jiaheng Lu"], "title": ["Top-k String Auto-Completion with Synonyms"], "date": ["2016-11-11T15:40:06Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1611.03751v3"], "summary": ["  Auto-completion is one of the most prominent features of modern information\nsystems. The existing solutions of auto-completion provide the suggestions\nbased on the beginning of the currently input character sequence (i.e. prefix).\nHowever, in many real applications, one entity often has synonyms or\nabbreviations. For example, \"DBMS\" is an abbreviation of \"Database Management\nSystems\". In this paper, we study a novel type of auto-completion by using\nsynonyms and abbreviations. We propose three trie-based algorithms to solve the\ntop-k auto-completion with synonyms; each one with different space and time\ncomplexity trade-offs. Experiments on large-scale datasets show that it is\npossible to support effective and efficient synonym-based retrieval of\ncompletions of a million strings with thousands of synonyms rules at about a\nmicrosecond per-completion, while taking small space overhead (i.e. 160-200\nbytes per string). The source code of our experiments can be download at:\nhttp://udbms.cs.helsinki.fi/?projects/autocompletion/download .\n"]},
{"authors": ["Cong Yan", "Alvin Cheung", "Shan Lu"], "title": ["Database-Backed Web Applications in the Wild: How Well Do They Work?"], "date": ["2016-07-09T02:40:37Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1607.02561v3"], "summary": ["  Most modern database-backed web applications are built upon Object Relational\nMapping (ORM) frameworks. While ORM frameworks ease application development by\nabstracting persistent data as objects, such convenience often comes with a\nperformance cost. In this paper, we present CADO, a tool that analyzes the\napplication logic and its interaction with databases using the Ruby on Rails\nORM framework. CADO includes a static program analyzer, a profiler and a\nsynthetic data generator to extract and understand application's performance\ncharacteristics. We used CADO to analyze the performance problems of 27\nreal-world open-source Rails applications, covering domains such as online\nforums, e-commerce, project management, blogs, etc. Based on the results, we\nuncovered a number of issues that lead to sub-optimal application performance,\nranging from issuing queries, how result sets are used, and physical design. We\nsuggest possible remedies for each issue, and highlight new research\nopportunities that arise from them.\n"]},
{"authors": ["Erfan Zamanian", "Carsten Binnig", "Tim Kraska", "Tim Harris"], "title": ["The End of a Myth: Distributed Transactions Can Scale"], "date": ["2016-07-03T16:44:48Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1607.00655v2"], "summary": ["  The common wisdom is that distributed transactions do not scale. But what if\ndistributed transactions could be made scalable using the next generation of\nnetworks and a redesign of distributed databases? There would be no need for\ndevelopers anymore to worry about co-partitioning schemes to achieve decent\nperformance. Application development would become easier as data placement\nwould no longer determine how scalable an application is. Hardware provisioning\nwould be simplified as the system administrator can expect a linear scale-out\nwhen adding more machines rather than some complex sub-linear function, which\nis highly application specific.\n  In this paper, we present the design of our novel scalable database system\nNAM-DB and show that distributed transactions with the very common Snapshot\nIsolation guarantee can indeed scale using the next generation of RDMA-enabled\nnetwork technology without any inherent bottlenecks. Our experiments with the\nTPC-C benchmark show that our system scales linearly to over 6.5 million\nnew-order (14.5 million total) distributed transactions per second on 56\nmachines.\n"]},
{"authors": ["Saad Bin Suhaim", "Weimo Liu", "Nan Zhang"], "title": ["Discover Aggregates Exceptions over Hidden Web Databases"], "date": ["2016-11-19T19:37:41Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1611.06417v1"], "summary": ["  Nowadays, many web databases \"hidden\" behind their restrictive search\ninterfaces (e.g., Amazon, eBay) contain rich and valuable information that is\nof significant interests to various third parties. Recent studies have\ndemonstrated the possibility of estimating/tracking certain aggregate queries\nover dynamic hidden web databases. Nonetheless, tracking all possible aggregate\nquery answers to report interesting findings (i.e., exceptions), while still\nadhering to the stringent query-count limitations enforced by many hidden web\ndatabases providers, is very challenging. In this paper, we develop a novel\ntechnique for tracking and discovering exceptions (in terms of sudden changes\nof aggregates) over dynamic hidden web databases. Extensive real-world\nexperiments demonstrate the superiority of our proposed algorithms over\nbaseline solutions.\n"]},
{"authors": ["Andrea Detti", "Nicola Blefari Melazzi", "Michele Orru", "Riccardo Paolillo", "Giulio Rossi"], "title": ["OpenGeoBase: Information Centric Networking meets Spatial Database\n  applications - Extended Version"], "date": ["2016-07-04T08:36:07Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1607.00771v3"], "summary": ["  This paper explores methodologies, advantages and challenges related to the\nuse of Information Centric Networking (ICN) for realizing distributed spatial\ndatabases. Our findings show that the ICN functionality perfectly fits database\nrequirements: routing-by-name can be used to dispatch queries and insertions,\nin-network caching to accelerate queries, and data-centric security to\nimplement secure multi-tenancy. We present an ICN-based distributed spatial\ndatabase, named OpenGeoBase, and describe its design choices. Thanks to ICN,\nOpenGeoBase can quickly and efficiently provide information to database users;\neasily operate in a distributed way, deploying and using many database engines\nin parallel; secure every piece of content; naturally slice resources, so that\nseveral tenants and users can concurrently and independently use the database.\nWe also show how OpenGeoBase can support a real world Intelligent Transport\nSystem application, by enabling discovery of geo-referenced public\ntransportation information.\n"]},
{"authors": ["Hui Miao", "Ang Li", "Larry S. Davis", "Amol Deshpande"], "title": ["ModelHub: Towards Unified Data and Lifecycle Management for Deep\n  Learning"], "date": ["2016-11-18T20:59:25Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1611.06224v1"], "summary": ["  Deep learning has improved state-of-the-art results in many important fields,\nand has been the subject of much research in recent years, leading to the\ndevelopment of several systems for facilitating deep learning. Current systems,\nhowever, mainly focus on model building and training phases, while the issues\nof data management, model sharing, and lifecycle management are largely\nignored. Deep learning modeling lifecycle generates a rich set of data\nartifacts, such as learned parameters and training logs, and comprises of\nseveral frequently conducted tasks, e.g., to understand the model behaviors and\nto try out new models. Dealing with such artifacts and tasks is cumbersome and\nlargely left to the users. This paper describes our vision and implementation\nof a data and lifecycle management system for deep learning. First, we\ngeneralize model exploration and model enumeration queries from commonly\nconducted tasks by deep learning modelers, and propose a high-level domain\nspecific language (DSL), inspired by SQL, to raise the abstraction level and\naccelerate the modeling process. To manage the data artifacts, especially the\nlarge amount of checkpointed float parameters, we design a novel model\nversioning system (dlv), and a read-optimized parameter archival storage system\n(PAS) that minimizes storage footprint and accelerates query workloads without\nlosing accuracy. PAS archives versioned models using deltas in a\nmulti-resolution fashion by separately storing the less significant bits, and\nfeatures a novel progressive query (inference) evaluation algorithm. Third, we\nshow that archiving versioned models using deltas poses a new dataset\nversioning problem and we develop efficient algorithms for solving it. We\nconduct extensive experiments over several real datasets from computer vision\ndomain to show the efficiency of the proposed techniques.\n"]},
{"authors": ["Mohamed Ahmed Sherif", "Kevin Dre\u00dfler", "Panayiotis Smeros", "Axel-Cyrille Ngonga Ngomo"], "title": ["Annex: Radon - Rapid Discovery of Topological Relations"], "date": ["2016-11-18T15:42:22Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1611.06128v1"], "summary": ["  Datasets containing geo-spatial resources are increasingly being represented\naccording to the Linked Data principles. Several time-efficient approaches for\ndiscovering links between RDF resources have been developed over the last\nyears. However, the time-efficient discovery of topological relations between\ngeospatial resources has been paid little attention to. We address this\nresearch gap by presenting Radon, a novel approach for the rapid computation of\ntopological relations between geo-spatial resources. Our approach uses a sparse\ntiling index in combination with minimum bounding boxes to reduce the\ncomputation time of topological relations. Our evaluation of Radon's runtime on\n45 datasets and in more than 800 experiments shows that it outperforms the\nstate of the art by up to 3 orders of magnitude while maintaining an F-measure\nof 100%. Moreover, our experiments suggest that Radon scales up well when\nimplemented in parallel.\n"]},
{"authors": ["Yangli-ao Geng", "Qingyong Li", "Rong Zheng", "Fuzhen Zhuangz", "Ruisi He"], "title": ["RECOME: a New Density-Based Clustering Algorithm Using Relative KNN\n  Kernel Density"], "date": ["2016-11-02T07:10:40Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1611.00484v3"], "summary": ["  Discovering clusters from a dataset with different shapes, density, and\nscales is a known challenging problem in data clustering. In this paper, we\npropose the RElative COre MErge (RECOME) clustering algorithm. The core of\nRECOME is a novel density measure, i.e., Relative $K$ nearest Neighbor Kernel\nDensity (RNKD). RECOME identifies core objects with unit RNKD, and partitions\nnon-core objects into atom clusters by successively following higher-density\nneighbor relations toward core objects. Core objects and their corresponding\natom clusters are then merged through $\\alpha$-reachable paths on a KNN graph.\nFurthermore, we discover that the number of clusters computed by RECOME is a\nstep function of the $\\alpha$ parameter with jump discontinuity on a small\ncollection of values. A jump discontinuity discovery (JDD) method is proposed\nusing a variant of the Dijkstra's algorithm. RECOME is evaluated on three\nsynthetic datasets and six real datasets. Experimental results indicate that\nRECOME is able to discover clusters with different shapes, density and scales.\nIt achieves better clustering results than established density-based clustering\nmethods on real datasets. Moreover, JDD is shown to be effective to extract the\njump discontinuity set of parameter $\\alpha$ for all tested dataset, which can\nease the task of data exploration and parameter tuning.\n"]},
{"authors": ["Weidong Xiong", "Feng Yu", "Mohammed Hamdi", "Wen-Chi Hou"], "title": ["A Prudent-Precedence Concurrency Control Protocol for High Data\n  Contention Database Enviornments"], "date": ["2016-11-17T04:02:54Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1611.05557v1"], "summary": ["  In this paper, we propose a concurrency control protocol, called the\nPrudent-Precedence Concurrency Control (PPCC) protocol, for high data\ncontention database environments. PPCC is prudently more aggressive in\npermitting more serializable schedules than two-phase locking. It maintains a\nrestricted precedence among conflicting transactions and commits the\ntransactions according to the serialization order established in the\nexecutions. A detailed simulation model has been constructed and extensive\nexperiments have been conducted to evaluate the performance of the proposed\napproach. The results demonstrate that the proposed algorithm outperforms the\ntwo-phase locking and optimistic concurrency control in all ranges of system\nworkload.\n"]},
{"authors": ["Madhav Nimishakavi", "Uday Singh Saini", "Partha Talukdar"], "title": ["Relation Schema Induction using Tensor Factorization with Side\n  Information"], "date": ["2016-05-12T19:44:04Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1605.04227v3"], "summary": ["  Given a set of documents from a specific domain (e.g., medical research\njournals), how do we automatically build a Knowledge Graph (KG) for that\ndomain? Automatic identification of relations and their schemas, i.e., type\nsignature of arguments of relations (e.g., undergo(Patient, Surgery)), is an\nimportant first step towards this goal. We refer to this problem as Relation\nSchema Induction (RSI). In this paper, we propose Schema Induction using\nCoupled Tensor Factorization (SICTF), a novel tensor factorization method for\nrelation schema induction. SICTF factorizes Open Information Extraction\n(OpenIE) triples extracted from a domain corpus along with additional side\ninformation in a principled way to induce relation schemas. To the best of our\nknowledge, this is the first application of tensor factorization for the RSI\nproblem. Through extensive experiments on multiple real-world datasets, we find\nthat SICTF is not only more accurate than state-of-the-art baselines, but also\nsignificantly faster (about 14x faster).\n"]},
{"authors": ["Patrick Schultz", "David I. Spivak", "Christina Vasilakopoulou", "Ryan Wisnesky"], "title": ["Algebraic Databases"], "date": ["2016-02-10T20:22:40Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1602.03501v2"], "summary": ["  Databases have been studied category-theoretically for decades. The database\nschema---whose purpose is to arrange high-level conceptual entities---is\ngenerally modeled as a category or sketch. The data itself, often called an\ninstance, is generally modeled as a set-valued functor, assigning to each\nconceptual entity a set of examples. While mathematically elegant, these\ncategorical models have typically struggled with representing concrete data\nsuch as integers or strings.\n  In the present work, we propose an extension of the set-valued functor model,\nmaking use of multisorted algebraic theories (a.k.a. Lawvere theories) to\nincorporate concrete data in a principled way. This also allows constraints and\nqueries to make use of operations on data, such as multiplication or comparison\nof numbers, helping to bridge the gap between traditional databases and\nprogramming languages.\n  We also show how all of the components of our model---including schemas,\ninstances, change-of-schema functors, and queries - fit into a single double\ncategorical structure called a proarrow equipment (a.k.a. framed bicategory).\n"]},
{"authors": ["Owen Kaser", "Daniel Lemire"], "title": ["Threshold and Symmetric Functions over Bitmaps"], "date": ["2014-02-17T17:22:05Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1402.4073v2"], "summary": ["  Bitmap indexes are routinely used to speed up simple aggregate queries in\ndatabases. Set operations such as intersections, unions and complements can be\nrepresented as logical operations (AND, OR, NOT). However, less is known about\nthe application of bitmap indexes to more advanced queries. We want to extend\nthe applicability of bitmap indexes. As a starting point, we consider symmetric\nBoolean queries (e.g., threshold functions). For example, we might consider\nstores as sets of products, and ask for products that are on sale in 2 to 10\nstores. Such symmetric Boolean queries generalize intersection, union, and\nT-occurrence queries.\n  It may not be immediately obvious to an engineer how to use bitmap indexes\nfor symmetric Boolean queries. Yet, maybe surprisingly, we find that the best\nof our bitmap-based algorithms are competitive with the state-of-the-art\nalgorithms for important special cases (e.g., MergeOpt, MergeSkip, DivideSkip,\nScanCount). Moreover, unlike the competing algorithms, the result of our\ncomputation is again a bitmap which can be further processed within a bitmap\nindex.\n  We review algorithmic design issues such as the aggregation of many\ncompressed bitmaps. We conclude with a discussion on other advanced queries\nthat bitmap indexes might be able to support efficiently.\n"]},
{"authors": ["Yiwen Tang", "Hongzhi Wang", "Shiwei Zhang", "Huijun Zhang", "Ruoxi Shi"], "title": ["Efficient Web-based Data Imputation with Graph Model"], "date": ["2016-11-14T09:08:19Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1611.04288v1"], "summary": ["  A challenge for data imputation is the lack of knowledge. In this paper, we\nattempt to address this challenge by involving extra knowledge from web. To\nachieve high-performance web-based imputation, we use the dependency, i.e.FDs\nand CFDs, to impute as many as possible values automatically and fill in the\nother missing values with the minimal access of web, whose cost is relatively\nlarge. To make sufficient use of dependencies, We model the dependency set on\nthe data as a graph and perform automatical imputation and keywords generation\nfor web-based imputation based on such graph model. With the generated\nkeywords, we design two algorithms to extract values for imputation from the\nsearch results. Extensive experimental results based on real-world data\ncollections show that the proposed approach could impute missing values\nefficiently and effectively compared to existing approach.\n"]},
{"authors": ["Manas Joglekar", "Theodoros Rekatsinas", "Hector Garcia-Molina", "Aditya Parameswaran", "Christopher R\u00e9"], "title": ["SLiMFast: Guaranteed Results for Data Fusion and Source Reliability"], "date": ["2015-12-21T02:28:17Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1512.06474v3"], "summary": ["  We focus on data fusion, i.e., the problem of unifying conflicting data from\ndata sources into a single representation by estimating the source accuracies.\nWe propose SLiMFast, a framework that expresses data fusion as a statistical\nlearning problem over discriminative probabilistic models, which in many cases\ncorrespond to logistic regression. In contrast to previous approaches that use\ncomplex generative models, discriminative models make fewer distributional\nassumptions over data sources and allow us to obtain rigorous theoretical\nguarantees. Furthermore, we show how SLiMFast enables incorporating domain\nknowledge into data fusion, yielding accuracy improvements of up to 50\\% over\nstate-of-the-art baselines. Building upon our theoretical results, we design an\noptimizer that obviates the need for users to manually select an algorithm for\nlearning SLiMFast's parameters. We validate our optimizer on multiple\nreal-world datasets and show that it can accurately predict the learning\nalgorithm that yields the best data fusion results.\n"]},
{"authors": ["Marco Montali", "Andrey Rivkin"], "title": ["DB-Nets: on The Marriage of Colored Petri Nets and Relational Databases"], "date": ["2016-11-11T12:39:41Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1611.03680v1"], "summary": ["  The integrated management of business processes and mas- ter data is being\nincreasingly considered as a fundamental problem, by both the academia and the\nindustry. In this position paper, we focus on the foundations of the problem,\narguing that contemporary approaches struggle to find a suitable equilibrium\nbetween data- and process-related aspects. We then propose db-nets, a new\nformal model that balances such two pillars through the marriage of colored\nPetri nets and relational databases. We invite the research community to build\non this model, discussing its potential in modeling, formal verification, and\nsimulation.\n"]},
{"authors": ["Jaroslav Fowkes", "Charles Sutton"], "title": ["A Bayesian Network Model for Interesting Itemsets"], "date": ["2015-10-14T14:55:17Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1510.04130v2"], "summary": ["  Mining itemsets that are the most interesting under a statistical model of\nthe underlying data is a commonly used and well-studied technique for\nexploratory data analysis, with the most recent interestingness models\nexhibiting state of the art performance. Continuing this highly promising line\nof work, we propose the first, to the best of our knowledge, generative model\nover itemsets, in the form of a Bayesian network, and an associated novel\nmeasure of interestingness. Our model is able to efficiently infer interesting\nitemsets directly from the transaction database using structural EM, in which\nthe E-step employs the greedy approximation to weighted set cover. Our approach\nis theoretically simple, straightforward to implement, trivially parallelizable\nand retrieves itemsets whose quality is comparable to, if not better than,\nexisting state of the art algorithms as we demonstrate on several real-world\ndatasets.\n"]},
{"authors": ["Bernardo Gon\u00e7alves"], "title": ["Show me the material evidence: Initial experiments on evaluating\n  hypotheses from user-generated multimedia data"], "date": ["2016-11-11T10:46:58Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1611.03652v1"], "summary": ["  Subjective questions such as `does neymar dive', or `is clinton lying', or\n`is trump a fascist', are popular queries to web search engines, as can be seen\nby autocompletion suggestions on Google, Yahoo and Bing. In the era of\ncognitive computing, beyond search, they could be handled as hypotheses issued\nfor evaluation. Our vision is to leverage on unstructured data and metadata of\nthe rich user-generated multimedia that is often shared as material evidence in\nfavor or against hypotheses in social media platforms. In this paper we present\ntwo preliminary experiments along those lines and discuss challenges for a\ncognitive computing system that collects material evidence from user-generated\nmultimedia towards aggregating it into some form of collective decision on the\nhypothesis.\n"]},
{"authors": ["Sang-Woo Jun", "Huy T. Nguyen", "Vijay N. Gadepally", " Arvind"], "title": ["In-Storage Embedded Accelerator for Sparse Pattern Processing"], "date": ["2016-11-10T16:21:51Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1611.03380v1"], "summary": ["  We present a novel architecture for sparse pattern processing, using flash\nstorage with embedded accelerators. Sparse pattern processing on large data\nsets is the essence of applications such as document search, natural language\nprocessing, bioinformatics, subgraph matching, machine learning, and graph\nprocessing. One slice of our prototype accelerator is capable of handling up to\n1TB of data, and experiments show that it can outperform C/C++ software\nsolutions on a 16-core system at a fraction of the power and cost; an optimized\nversion of the accelerator can match the performance of a 48-core server.\n"]},
{"authors": ["Mar\u00eda Carmen Calvo Yanguas", "Carmen Elvira Don\u00e1zar", "Raquel Trillo Lado"], "title": ["A Formal Definition for Configuration"], "date": ["2016-11-10T13:04:49Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1611.03736v1"], "summary": ["  There exists a wide set of techniques to perform keyword-based search over\nrelational databases but all of them match the keywords in the users' queries\nto elements of the databases to be queried as first step. The matching process\nis a time-consuming and complex task. So, improving the performance of this\ntask is a key issue to improve the keyword based search on relational data\nsources.In this work, we show how to model the matching process on\nkeyword-based search on relational databases by means of the symmetric group.\nBesides, how this approach reduces the search space is explained in detail.\n"]},
{"authors": ["Fan Zhang", "Ying Zhang", "Lu Qin", "Wenjie Zhang", "Xuemin Lin"], "title": ["When Engagement Meets Similarity: Efficient (k,r)-Core Computation on\n  Social Networks"], "date": ["2016-11-10T11:02:31Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1611.03254v1"], "summary": ["  In this paper, we investigate the problem of (k,r)-core which intends to find\ncohesive subgraphs on social networks considering both user engagement and\nsimilarity perspectives. In particular, we adopt the popular concept of k-core\nto guarantee the engagement of the users (vertices) in a group (subgraph) where\neach vertex in a (k,r)-core connects to at least k other vertices. Meanwhile,\nwe also consider the pairwise similarity between users based on their profiles.\nFor a given similarity metric and a similarity threshold r, the similarity\nbetween any two vertices in a (k,r)-core is ensured not less than r. Efficient\nalgorithms are proposed to enumerate all maximal (k,r)-cores and find the\nmaximum (k,r)-core, where both problems are shown to be NP-hard. Effective\npruning techniques significantly reduce the search space of two algorithms and\na novel (k,k')-core based (k,r)-core size upper bound enhances performance of\nthe maximum (k,r)-core computation. We also devise effective search orders to\naccommodate the different nature of two mining algorithms. Comprehensive\nexperiments on real-life data demonstrate that the maximal/maximum (k,r)-cores\nenable us to find interesting cohesive subgraphs, and performance of two mining\nalgorithms is significantly improved by proposed techniques.\n"]},
{"authors": ["Xiang Wang", "Ying Zhang", "Wenjie Zhang", "Xuemin Lin", "Zengfeng Huang"], "title": ["Top-k Spatial-keyword Publish/Subscribe Over Sliding Window"], "date": ["2016-11-10T07:29:07Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1611.03204v1"], "summary": ["  With the prevalence of social media and GPS-enabled devices, a massive amount\nof geo-textual data has been generated in a stream fashion, leading to a\nvariety of applications such as location-based recommendation and information\ndissemination. In this paper, we investigate a novel real-time top-k monitoring\nproblem over sliding window of streaming data; that is, we continuously\nmaintain the top-k most relevant geo-textual messages (e.g., geo-tagged tweets)\nfor a large number of spatial-keyword subscriptions (e.g., registered users\ninterested in local events) simultaneously. To provide the most recent\ninformation under controllable memory cost, sliding window model is employed on\nthe streaming geo-textual data. To the best of our knowledge, this is the first\nwork to study top-k spatial-keyword publish/subscribe over sliding window. A\nnovel centralized system, called Skype (Topk Spatial-keyword\nPublish/Subscribe), is proposed in this paper. In Skype, to continuously\nmaintain top-k results for massive subscriptions, we devise a novel indexing\nstructure upon subscriptions such that each incoming message can be immediately\ndelivered on its arrival. To reduce the expensive top-k re-evaluation cost\ntriggered by message expiration, we develop a novel cost-based k-skyband\ntechnique to reduce the number of re-evaluations in a cost-effective way.\nExtensive experiments verify the great efficiency and effectiveness of our\nproposed techniques. Furthermore, to support better scalability and higher\nthroughput, we propose a distributed version of Skype, namely, DSkype, on top\nof Storm, which is a popular distributed stream processing system. With the\nhelp of fine-tuned subscription/message distribution mechanisms, DSkype can\nachieve orders of magnitude speed-up than its centralized version.\n"]},
{"authors": ["Kgotatso Desmond Mogotlane", "Jean Vincent Fonou-Dombeu"], "title": ["Automatic Conversion of Relational Databases into Ontologies: A\n  Comparative Analysis of Prot\u00e9g\u00e9 Plug-ins Performances"], "date": ["2016-11-08T09:00:53Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1611.02816v1"], "summary": ["  Constructing ontologies from relational databases is an active research topic\nin the Semantic Web domain. While conceptual mapping rules/principles of\nrelational databases and ontology structures are being proposed, several\nsoftware modules or plug-ins are being developed to enable the automatic\nconversion of relational databases into ontologies. However, the correlation\nbetween the resulting ontologies built automatically with plug-ins from\nrelational databases and the database-to-ontology mapping principles has been\ngiven little attention. This study reviews and applies two Prot\\'eg\\'e\nplug-ins, namely, DataMaster and OntoBase to automatically construct ontologies\nfrom a relational database. The resulting ontologies are further analysed to\nmatch their structures against the database-to-ontology mapping principles. A\ncomparative analysis of the matching results reveals that OntoBase outperforms\nDataMaster in applying the database-to-ontology mapping principles for\nautomatically converting relational databases into ontologies.\n"]},
{"authors": ["Sam Fletcher", "Md Zahidul Islam"], "title": ["Decision Tree Classification with Differential Privacy: A Survey"], "date": ["2016-11-07T07:13:27Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1611.01919v1"], "summary": ["  Data mining information about people is becoming increasingly important in\nthe data-driven society of the 21st century. Unfortunately, sometimes there are\nreal-world considerations that conflict with the goals of data mining;\nsometimes the privacy of the people being data mined needs to be considered.\nThis necessitates that the output of data mining algorithms be modified to\npreserve privacy while simultaneously not ruining the predictive power of the\noutputted model. Differential privacy is a strong, enforceable definition of\nprivacy that can be used in data mining algorithms, guaranteeing that nothing\nwill be learned about the people in the data that could not already be\ndiscovered without their participation. In this survey, we focus on one\nparticular data mining algorithm -- decision trees -- and how differential\nprivacy interacts with each of the components that constitute decision tree\nalgorithms. We analyze both greedy and random decision trees, and the conflicts\nthat arise when trying to balance privacy requirements with the accuracy of the\nmodel.\n"]},
{"authors": ["Luyang Li", "Bing Qin", "Wenjing Ren", "Ting Liu"], "title": ["Truth Discovery with Memory Network"], "date": ["2016-11-07T01:08:11Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1611.01868v1"], "summary": ["  Truth discovery is to resolve conflicts and find the truth from\nmultiple-source statements. Conventional methods mostly research based on the\nmutual effect between the reliability of sources and the credibility of\nstatements, however, pay no attention to the mutual effect among the\ncredibility of statements about the same object. We propose memory network\nbased models to incorporate these two ideas to do the truth discovery. We use\nfeedforward memory network and feedback memory network to learn the\nrepresentation of the credibility of statements which are about the same\nobject. Specially, we adopt memory mechanism to learn source reliability and\nuse it through truth prediction. During learning models, we use multiple types\nof data (categorical data and continuous data) by assigning different weights\nautomatically in the loss function based on their own effect on truth discovery\nprediction. The experiment results show that the memory network based models\nmuch outperform the state-of-the-art method and other baseline methods.\n"]},
{"authors": ["Reuven Cohen", "Liran Katzir", "Aviv Yehezkel"], "title": ["MTS Sketch for Accurate Estimation of Set-Expression Cardinalities from\n  Small Samples"], "date": ["2016-11-06T22:22:40Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1611.01853v1"], "summary": ["  Sketch-based streaming algorithms allow efficient processing of big data.\nThese algorithms use small fixed-size storage to store a summary (\"sketch\") of\nthe input data, and use probabilistic algorithms to estimate the desired\nquantity. However, in many real-world applications it is impractical to collect\nand process the entire data stream, the common practice is thus to sample and\nprocess only a small part of it. While sampling is crucial for handling massive\ndata sets, it may reduce accuracy. In this paper we present a new framework\nthat can accurately estimate the cardinality of any set expression between any\nnumber of streams using only a small sample of each stream. The proposed\nframework consists of a new sketch, called Maximal-Term with Subsample (MTS),\nand a family of algorithms that use this sketch. An example of a possible query\nthat can be efficiently answered using the proposed sketch is, How many\ndistinct tuples appear in tables $T_1$ and $T_2$, but not in $T_3$? The\nalgorithms presented in this paper answer such queries accurately, processing\nonly a small sample of the tuples in each table and using a constant amount of\nmemory. Such estimations are useful for the optimization of queries over very\nlarge database systems. We show that all our algorithms are unbiased, and we\nanalyze their asymptotic variance.\n"]},
{"authors": ["Konstantin Golenberg", "Yehoshua Sagiv"], "title": ["Constructing Data Graphs for Keyword Search"], "date": ["2016-05-25T12:59:37Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1605.07865v2"], "summary": ["  A data graph is a convenient paradigm for supporting keyword search that\ntakes into account available semantic structure and not just textual relevance.\nHowever, the problem of constructing data graphs that facilitate both\nefficiency and effectiveness of the underlying system has hardly been\naddressed. A conceptual model for this task is proposed. Principles for\nconstructing good data graphs are explained. Transformations for generating\ndata graphs from RDB and XML are developed. The results obtained from these\ntransformations are analyzed. It is shown that XML is a better starting point\nfor getting a good data graph.\n"]},
{"authors": ["Abhishek Santra", "Sanjukta Bhowmick", "Sharma Chakravarthy"], "title": ["Scalable Holistic Analysis of Multi-Source, Data-Intensive Problems\n  Using Multilayered Networks"], "date": ["2016-11-04T21:32:18Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1611.01546v1"], "summary": ["  Holistic analysis of many real-world problems are based on data collected\nfrom multiple sources contributing to some aspect of that problem. The word\nfusion has also been used in the literature for such problems involving\ndisparate data types. Holistically understanding traffic patterns, causes of\naccidents, bombings, terrorist planning and many natural phenomenon such as\nstorms, earthquakes fall into this category. Some may have real-time\nrequirements and some may need to be analyzed after the fact (post-mortem or\nforensic analysis.) What is common for all these problems is that the amount\nand types of data associated with the event. Data may also be incomplete and\ntrustworthiness of sources may also vary. Currently, manual and ad-hoc\napproaches are used in aggregating data in different ways for analyzing and\nunderstanding these problems.\n  In this paper, we approach this problem in a novel way using multilayered\nnetworks. We identify features of a central event and propose a network layer\nfor each feature. This approach allows us to study the effect of each feature\nindependently and its impact on the event. We also establish that the proposed\napproach allows us to compose these features in arbitrary ways (without loss of\ninformation) to analyze their combined effect. Additionally, formulation of\nrelationships (e.g., distance measure for a single feature instead of several\nat the same time) is simpler. Further, computations can be done once on each\nlayer in this approach and reused for mixing and matching the features for\naggregate impacts and \"what if\" scenarios to understand the problem\nholistically. This has been demonstrated by recreating the communities for the\nAND-Composed network by using the communities of the individual layers.\n  We believe that techniques proposed here make an important contribution to\nthe nascent yet fast growing area of data fusion.\n"]},
{"authors": ["Hubert Naacke", "Olivier Cur\u00e9", "Bernd Amann"], "title": ["SPARQL query processing with Apache Spark"], "date": ["2016-04-29T16:40:56Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1604.08903v2"], "summary": ["  The number of linked data sources and the size of the linked open data graph\nkeep growing every day. As a consequence, semantic RDF services are more and\nmore confronted to various \"big data\" problems. Query processing is one of them\nand needs to be efficiently addressed with executions over scalable, highly\navailable and fault tolerant frameworks. Data management systems requiring\nthese properties are rarely built from scratch but are rather designed on top\nof an existing cluster computing engine. In this work, we consider the\nprocessing of SPARQL queries with Apache Spark. We propose and compare five\ndifferent query processing approaches based on different join execution models\nand Spark components. A detailed experimentation, on real-world and synthetic\ndata sets, emphasizes that two approaches tailored for the RDF data model\noutperform the other ones on all major query shapes, i.e., star, snowflake,\nchain and hybrid.\n"]},
{"authors": ["Qunzhi Zhou", "Yogesh Simmhan", "Viktor Prasanna"], "title": ["Knowledge-infused and Consistent Complex Event Processing over Real-time\n  and Persistent Streams"], "date": ["2016-11-02T16:39:18Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1611.00676v1"], "summary": ["  Emerging applications in Internet of Things (IoT) and Cyber-Physical Systems\n(CPS) present novel challenges to Big Data platforms for performing online\nanalytics. Ubiquitous sensors from IoT deployments are able to generate data\nstreams at high velocity, that include information from a variety of domains,\nand accumulate to large volumes on disk. Complex Event Processing (CEP) is\nrecognized as an important real-time computing paradigm for analyzing\ncontinuous data streams. However, existing work on CEP is largely limited to\nrelational query processing, exposing two distinctive gaps for query\nspecification and execution: (1) infusing the relational query model with\nhigher level knowledge semantics, and (2) seamless query evaluation across\ntemporal spaces that span past, present and future events. These allow\naccessible analytics over data streams having properties from different\ndisciplines, and help span the velocity (real-time) and volume (persistent)\ndimensions. In this article, we introduce a Knowledge-infused CEP (X-CEP)\nframework that provides domain-aware knowledge query constructs along with\ntemporal operators that allow end-to-end queries to span across real-time and\npersistent streams. We translate this query model to efficient query execution\nover online and offline data streams, proposing several optimizations to\nmitigate the overheads introduced by evaluating semantic predicates and in\naccessing high-volume historic data streams. The proposed X-CEP query model and\nexecution approaches are implemented in our prototype semantic CEP engine,\nSCEPter. We validate our query model using domain-aware CEP queries from a\nreal-world Smart Power Grid application, and experimentally analyze the\nbenefits of our optimizations for executing these queries, using event streams\nfrom a campus-microgrid IoT deployment.\n"]},
{"authors": ["Haoyu Zhang", "Qin Zhang"], "title": ["Computing Skylines on Distributed Data"], "date": ["2016-11-01T23:41:03Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1611.00423v1"], "summary": ["  In this paper we study skyline queries in the distributed computational\nmodel, where we have $s$ remote sites and a central coordinator (the query\nnode); each site holds a piece of data, and the coordinator wants to compute\nthe skyline of the union of the $s$ datasets. The computation is in terms of\nrounds, and the goal is to minimize both the total communication cost and the\nround cost.\n  Viewing data objects as points in the Euclidean space, we consider both the\nhorizontal data partition case where each site holds a subset of points, and\nthe vertical data partition case where each site holds one coordinate of all\nthe points. We give a set of algorithms that have provable theoretical\nguarantees, and complement them with information theoretical lower bounds. We\nalso demonstrate the superiority of our algorithms over existing heuristics by\nan extensive set of experiments on both synthetic and real world datasets.\n"]},
{"authors": ["Bilegsaikhan Naidan", "Leonid Boytsov", "Eric Nyberg"], "title": ["Permutation Search Methods are Efficient, Yet Faster Search is Possible"], "date": ["2015-06-10T04:50:29Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1506.03163v4"], "summary": ["  We survey permutation-based methods for approximate k-nearest neighbor\nsearch. In these methods, every data point is represented by a ranked list of\npivots sorted by the distance to this point. Such ranked lists are called\npermutations. The underpinning assumption is that, for both metric and\nnon-metric spaces, the distance between permutations is a good proxy for the\ndistance between original points. Thus, it should be possible to efficiently\nretrieve most true nearest neighbors by examining only a tiny subset of data\npoints whose permutations are similar to the permutation of a query. We further\ntest this assumption by carrying out an extensive experimental evaluation where\npermutation methods are pitted against state-of-the art benchmarks (the\nmulti-probe LSH, the VP-tree, and proximity-graph based retrieval) on a variety\nof realistically large data set from the image and textual domain. The focus is\non the high-accuracy retrieval methods for generic spaces. Additionally, we\nassume that both data and indices are stored in main memory. We find\npermutation methods to be reasonably efficient and describe a setup where these\nmethods are most useful. To ease reproducibility, we make our software and data\nsets publicly available.\n"]},
{"authors": ["Yiming Lin", "Hongzhi Wang", "Jianzhong Li", "Hong Gao"], "title": ["Data Source Selection for Information Integration in Big Data Era"], "date": ["2016-10-29T13:17:50Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1610.09506v1"], "summary": ["  In Big data era, information integration often requires abundant data\nextracted from massive data sources. Due to a large number of data sources,\ndata source selection plays a crucial role in information integration, since it\nis costly and even impossible to access all data sources. Data Source selection\nshould consider both efficiency and effectiveness issues. For efficiency, the\napproach should achieve high performance and be scalability to fit large data\nsource amount. From effectiveness aspect, data quality and overlapping of\nsources are to be considered, since data quality varies much from data sources,\nwith significant differences in the accuracy and coverage of the data provided,\nand the overlapping of sources can even lower the quality of data integrated\nfrom selected data sources.\n  In this paper, we study source selection problem in \\textit{Big Data Era} and\npropose methods which can scale to datasets with up to millions of data sources\nand guarantee the quality of results. Motivated by this, we propose a new\nobject function taking the expected number of true values a source can provide\nas a criteria to evaluate the contribution of a data source. Based on our\nproposed index we present a scalable algorithm and two pruning strategies to\nimprove the efficiency without sacrificing precision. Experimental results on\nboth real world and synthetic data sets show that our methods can select\nsources providing a large proportion of true values efficiently and can scale\nto massive data sources.\n"]},
{"authors": ["Yiming Lin", "Hongzhi Wang", "Jianzhong Li", "Hong Gao"], "title": ["Efficient Entity Resolution on Heterogeneous Records"], "date": ["2016-10-29T12:51:52Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1610.09500v1"], "summary": ["  Entity resolution (ER) is the problem of identifying and merging records that\nrefer to the same real-world entity. In many scenarios, raw records are stored\nunder heterogeneous environment. Specifically, the schemas of records may\ndiffer from each other. To leverage such records better, most existing work\nassume that schema matching and data exchange have been done to convert records\nunder different schemas to those under a predefined schema. However, we observe\nthat schema matching would lose information in some cases, which could be\nuseful or even crucial to ER.\n  To leverage sufficient information from heterogeneous sources, in this paper,\nwe address several challenges of ER on heterogeneous records and show that none\nof existing similarity metrics or their transformations could be applied to\nfind similar records under heterogeneous settings. Motivated by this, we design\nthe similarity function and propose a novel framework to iteratively find\nrecords which refer to the same entity. Regarding efficiency, we build an index\nto generate candidates and accelerate similarity computation. Evaluations on\nreal-world datasets show the effectiveness and efficiency of our methods.\n"]},
{"authors": ["Amir Shaikhha", "Mohammad Dashti", "Christoph Koch"], "title": ["Push vs. Pull-Based Loop Fusion in Query Engines"], "date": ["2016-10-28T11:00:02Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1610.09166v1"], "summary": ["  Database query engines use pull-based or push-based approaches to avoid the\nmaterialization of data across query operators. In this paper, we study these\ntwo types of query engines in depth and present the limitations and advantages\nof each engine. Similarly, the programming languages community has developed\nloop fusion techniques to remove intermediate collections in the context of\ncollection programming. We draw parallels between the DB and PL communities by\ndemonstrating the connection between pipelined query engines and loop fusion\ntechniques. Based on this connection, we propose a new type of pull-based\nengine, inspired by a loop fusion technique, which combines the benefits of\nboth approaches. Then we experimentally evaluate the various engines, in the\ncontext of query compilation, for the first time in a fair environment,\neliminating the biasing impact of ancillary optimizations that have\ntraditionally only been used with one of the approaches. We show that for\nrealistic analytical workloads, there is no considerable advantage for either\nform of pipelined query engine, as opposed to what recent research suggests.\nAlso, by using microbenchmarks we show that our proposed engine dominates the\nexisting engines by combining the benefits of both.\n"]},
{"authors": ["Ke Yang", "Julia Stoyanovich"], "title": ["Measuring Fairness in Ranked Outputs"], "date": ["2016-10-26T22:02:39Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1610.08559v1"], "summary": ["  Ranking and scoring are ubiquitous. We consider the setting in which an\ninstitution, called a ranker, evaluates a set of individuals based on\ndemographic, behavioral or other characteristics. The final output is a ranking\nthat represents the relative quality of the individuals. While automatic and\ntherefore seemingly objective, rankers can, and often do, discriminate against\nindividuals and systematically disadvantage members of protected groups. This\nwarrants a careful study of the fairness of a ranking scheme.\n  In this paper we propose fairness measures for ranked outputs. We develop a\ndata generation procedure that allows us to systematically control the degree\nof unfairness in the output, and study the behavior of our measures on these\ndatasets. We then apply our proposed measures to several real datasets, and\ndemonstrate cases of unfairness. Finally, we show preliminary results of\nincorporating our ranked fairness measures into an optimization framework, and\nshow potential for improving fairness of ranked outputs while maintaining\naccuracy.\n"]},
{"authors": ["Upal Mahbub", "Sayantan Sarkar", "Vishal M. Patel", "Rama Chellappa"], "title": ["Active User Authentication for Smartphones: A Challenge Data Set and\n  Benchmark Results"], "date": ["2016-10-25T15:56:07Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1610.07930v1"], "summary": ["  In this paper, automated user verification techniques for smartphones are\ninvestigated. A unique non-commercial dataset, the University of Maryland\nActive Authentication Dataset 02 (UMDAA-02) for multi-modal user authentication\nresearch is introduced. This paper focuses on three sensors - front camera,\ntouch sensor and location service while providing a general description for\nother modalities. Benchmark results for face detection, face verification,\ntouch-based user identification and location-based next-place prediction are\npresented, which indicate that more robust methods fine-tuned to the mobile\nplatform are needed to achieve satisfactory verification accuracy. The dataset\nwill be made available to the research community for promoting additional\nresearch.\n"]},
{"authors": ["Zhefeng Wang", "Yu Yang", "Jian Pei", "Enhong Chen"], "title": ["Activity Maximization by Effective Information Diffusion in Social\n  Networks"], "date": ["2016-10-25T07:24:37Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1610.07754v1"], "summary": ["  In a social network, even about the same information the excitements between\ndifferent pairs of users are different. If you want to spread a piece of new\ninformation and maximize the expected total amount of excitements, which seed\nusers should you choose? This problem indeed is substantially different from\nthe renowned influence maximization problem and cannot be tackled using the\nexisting approaches. In this paper, motivated by the demand in a few\ninteresting applications, we model the novel problem of activity maximization.\nWe tackle the problem systematically. We first analyze the complexity and the\napproximability of the problem. We develop an upper bound and a lower bound\nthat are submodular so that the Sandwich framework can be applied. We then\ndevise a polling-based randomized algorithm that guarantees a data dependent\napproximation factor. Our experiments on three real data sets clearly verify\nthe effectiveness and scalability of our method, as well as the advantage of\nour method against the other heuristic methods.\n"]},
{"authors": ["Anja Gruenheid", "Donald Kossmann", "Divesh Srivastava"], "title": ["Online Event Integration with StoryPivot"], "date": ["2016-10-25T05:10:18Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1610.07732v1"], "summary": ["  Modern data integration systems need to process large amounts of data from a\nvariety of data sources and with real-time integration constraints. They are\nnot only employed in enterprises for managing internal data but are also used\nfor a variety of web services that use techniques such as entity resolution or\ndata cleaning in live systems. In this work, we discuss a new generation of\ndata integration systems that operate on (un-)structured data in an online\nsetting, i.e., systems which process continuously modified datasets upon which\nthe integration task is based. We use as an example of such a system an online\nevent integration system called StoryPivot. It observes events extracted from\nnews articles in data sources such as the 'Guardian' or the 'Washington Post'\nwhich are integrated to show users the evolution of real-world stories over\ntime. The design decisions for StoryPivot are influenced by the trade-off\nbetween maintaining high quality integration results while at the same time\nbuilding a system that processes and integrates events in near real-time. We\nevaluate our design decisions with experiments on two real-world datasets and\ngeneralize our findings to other data integration tasks that have a similar\nsystem setup.\n"]},
{"authors": ["Xiaowang Zhang", "Jiahui Zhang", "Muhammad Qasim Yasin", "Wenrui Wu", "Zhiyong Feng"], "title": ["Path discovery by Querying the federation of Relational Database and RDF\n  Graph"], "date": ["2016-10-25T02:12:45Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1610.07707v1"], "summary": ["  The class of queries for detecting path is an important as those can extract\nimplicit binary relations over the nodes of input graphs. Most of the path\nquerying languages used by the RDF community, like property paths in W3C SPARQL\n1.1 and nested regular expressions in nSPARQL are based on the regular\nexpressions. Federated queries allow for combining graph patterns and\nrelational database that enables the evaluations over several heterogeneous\ndata resources within a single query. Federated queries in W3C SPARQL 1.1\ncurrently evaluated over different SPARQL endpoints. In this paper, we present\na federated path querying language as an extension of regular path querying\nlanguage for supporting RDF graph integration with relational database. The\nfederated path querying language is absolutely more expressive than nested\nregular expressions and negation-free property paths. Its additional\nexpressivity can be used for capturing the conjunction and federation of nested\nregular path queries. Despite the increase in expressivity, we also show that\nfederated path queries are still enjoy a low computational complexity and can\nbe evaluated efficiently.\n"]},
{"authors": ["Qiong Hu", "Tomasz Imielinski"], "title": ["ALPINE: Anytime Mining with Definite Guarantees"], "date": ["2016-10-24T20:52:57Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1610.07649v1"], "summary": ["  ALPINE is to our knowledge the first anytime algorithm to mine frequent\nitemsets and closed frequent itemsets. It guarantees that all itemsets with\nsupport exceeding the current checkpoint's support have been found before it\nproceeds further. Thus, it is very attractive for extremely long mining tasks\nwith very high dimensional data (for example in genetics) because it can offer\nintermediate meaningful and complete results. This ANYTIME feature is the most\nimportant contribution of ALPINE, which is also fast but not necessarily the\nfastest algorithm around. Another critical advantage of ALPINE is that it does\nnot require the apriori decided minimum support value.\n"]},
{"authors": ["Nicola Wadeson", "Mark Basham"], "title": ["Savu: A Python-based, MPI Framework for Simultaneous Processing of\n  Multiple, N-dimensional, Large Tomography Datasets"], "date": ["2016-10-24T13:22:09Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1610.08015v1"], "summary": ["  Diamond Light Source (DLS), the UK synchrotron facility, attracts scientists\nfrom across the world to perform ground-breaking x-ray experiments. With over\n3000 scientific users per year, vast amounts of data are collected across the\nexperimental beamlines, with the highest volume of data collected during\ntomographic imaging experiments. A growing interest in tomography as an imaging\ntechnique, has led to an expansion in the range of experiments performed, in\naddition to a growth in the size of the data per experiment.\n  Savu is a portable, flexible, scientific processing pipeline capable of\nprocessing multiple, n-dimensional datasets in serial on a PC, or in parallel\nacross a cluster. Developed at DLS, and successfully deployed across the\nbeamlines, it uses a modular plugin format to enable experiment-specific\nprocessing and utilises parallel HDF5 to remove RAM restrictions. The Savu\ndesign, described throughout this paper, focuses on easy integration of\nexisting and new functionality, flexibility and ease of use for users and\ndevelopers alike.\n"]},
{"authors": ["Leopoldo Bertossi", "Babak Salimi"], "title": ["From Causes for Database Queries to Repairs and Model-Based Diagnosis\n  and Back"], "date": ["2015-07-01T15:20:29Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1507.00257v3"], "summary": ["  In this work we establish and investigate connections between causes for\nquery answers in databases, database repairs wrt. denial constraints, and\nconsistency-based diagnosis. The first two are relatively new research areas in\ndatabases, and the third one is an established subject in knowledge\nrepresentation. We show how to obtain database repairs from causes, and the\nother way around. Causality problems are formulated as diagnosis problems, and\nthe diagnoses provide causes and their responsibilities. The vast body of\nresearch on database repairs can be applied to the newer problems of computing\nactual causes for query answers and their responsibilities. These connections,\nwhich are interesting per se, allow us, after a transition -inspired by\nconsistency-based diagnosis- to computational problems on hitting sets and\nvertex covers in hypergraphs, to obtain several new algorithmic and complexity\nresults for database causality.\n"]},
{"authors": ["Fernando Chirigati", "Harish Doraiswamy", "Theodoros Damoulas", "Juliana Freire"], "title": ["Data Polygamy: The Many-Many Relationships among Urban Spatio-Temporal\n  Data Sets"], "date": ["2016-10-21T23:59:30Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1610.06978v1"], "summary": ["  The increasing ability to collect data from urban environments, coupled with\na push towards openness by governments, has resulted in the availability of\nnumerous spatio-temporal data sets covering diverse aspects of a city.\nDiscovering relationships between these data sets can produce new insights by\nenabling domain experts to not only test but also generate hypotheses. However,\ndiscovering these relationships is difficult. First, a relationship between two\ndata sets may occur only at certain locations and/or time periods. Second, the\nsheer number and size of the data sets, coupled with the diverse spatial and\ntemporal scales at which the data is available, presents computational\nchallenges on all fronts, from indexing and querying to analyzing them.\nFinally, it is non-trivial to differentiate between meaningful and spurious\nrelationships. To address these challenges, we propose Data Polygamy, a\nscalable topology-based framework that allows users to query for statistically\nsignificant relationships between spatio-temporal data sets. We have performed\nan experimental evaluation using over 300 spatial-temporal urban data sets\nwhich shows that our approach is scalable and effective at identifying\ninteresting relationships.\n"]},
{"authors": ["Farhana M. Choudhury", "Zhifeng Bao", "J. Shane Culpepper", "Timos Sellis"], "title": ["Monitoring the Top-m Aggregation in a Sliding Window of Spatial Queries"], "date": ["2016-10-12T02:12:18Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1610.03579v2"], "summary": ["  In this paper, we propose and study the problem of top-m rank aggregation of\nspatial objects in streaming queries, where, given a set of objects O, a stream\nof spatial queries (kNN or range), the goal is to report m objects with the\nhighest aggregate rank. The rank of an object w.r.t. an individual query is\ncomputed based on its distance from the query location, and the aggregate rank\nis computed from all of the individual rank orderings. Solutions to this\nfundamental problem can be used to monitor the popularity of spatial objects,\nwhich in turn can provide new analytical tools for spatial data. Our work draws\ninspiration from three different domains: rank aggregation, continuous queries\nand spatial databases. To the best of our knowledge, there is no prior work\nthat considers all three problem domains in a single context. Our problem is\ndifferent from the classical rank aggregation problem in the way that the rank\nof spatial objects are dependent on streaming queries whose locations are not\nknown a priori, and is different from the problem of continuous spatial queries\nbecause new query locations can arrive in any region, but do not move. In order\nto solve this problem, we show how to upper and lower bound the rank of an\nobject for any unseen query. Then we propose an approximation solution to\ncontinuously monitor the top-m objects efficiently, for which we design an\nInverted Rank File (IRF) index to guarantee the error bound of the solution. In\nparticular, we propose the notion of safe ranking to determine whether the\ncurrent result is still valid or not when new queries arrive, and propose the\nnotion of validation objects to limit the number of objects to update in the\ntop-m results. We also propose an exact solution for applications where an\napproximate solution is not sufficient. Last, we conduct extensive experiments\nto verify the efficiency and effectiveness of our solutions.\n"]},
{"authors": ["Foto Afrati", "Shlomi Dolev", "Ephraim Korach", "Shantanu Sharma", "Jeffrey D. Ullman"], "title": ["Assignment Problems of Different-Sized Inputs in MapReduce"], "date": ["2015-07-16T06:46:19Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1507.04461v2"], "summary": ["  A MapReduce algorithm can be described by a mapping schema, which assigns\ninputs to a set of reducers, such that for each required output there exists a\nreducer that receives all the inputs that participate in the computation of\nthis output. Reducers have a capacity, which limits the sets of inputs that\nthey can be assigned. However, individual inputs may vary in terms of size. We\nconsider, for the first time, mapping schemas where input sizes are part of the\nconsiderations and restrictions. One of the significant parameters to optimize\nin any MapReduce job is communication cost between the map and reduce phases.\nThe communication cost can be optimized by minimizing the number of copies of\ninputs sent to the reducers. The communication cost is closely related to the\nnumber of reducers of constrained capacity that are used to accommodate\nappropriately the inputs, so that the requirement of how the inputs must meet\nin a reducer is satisfied. In this work, we consider a family of problems where\nit is required that each input meets with each other input in at least one\nreducer. We also consider a slightly different family of problems in which,\neach input of a list, X, is required to meet each input of another list, Y, in\nat least one reducer. We prove that finding an optimal mapping schema for these\nfamilies of problems is NP-hard, and present a bin-packing-based approximation\nalgorithm for finding a near optimal mapping schema.\n"]},
{"authors": ["Nelly Vouzoukidou", "Bernd Amann", "Vassilis Christophides"], "title": ["Continuous Top-k Queries over Real-Time Web Streams"], "date": ["2016-10-20T17:02:20Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1610.06500v1"], "summary": ["  The Web has become a large-scale real-time information system forcing us to\nrevise both how to effectively assess relevance of information for a user and\nhow to efficiently implement information retrieval and dissemination\nfunctionality. To increase information relevance, Real-time Web applications\nsuch as Twitter and Facebook, extend content and social-graph relevance scores\nwith \"real-time\" user generated events (e.g. re-tweets, replies, likes). To\naccommodate high arrival rates of information items and user events we explore\na publish/subscribe paradigm in which we index queries and update on the fly\ntheir results each time a new item and relevant events arrive. In this setting,\nwe need to process continuous top-k text queries combining both static and\ndynamic scores. To the best of our knowledge, this is the first work addressing\nhow non-predictable, dynamic scores can be handled in a continuous top-k query\nsetting.\n"]},
{"authors": ["Andreas M. Wahl", "Gregor Endler", "Peter K. Schwab", "Sebastian Herbst", "Richard Lenz"], "title": ["Anfrage-getriebener Wissenstransfer zur Unterstuetzung von\n  Datenanalysten"], "date": ["2016-10-20T12:41:31Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1610.06382v1"], "summary": ["  In larger organizations, multiple teams of data scientists have to integrate\ndata from heterogeneous data sources as preparation for data analysis tasks.\nWriting effective analytical queries requires data scientists to have in-depth\nknowledge of the existence, semantics, and usage context of data sources. Once\ngathered, such knowledge is informally shared within a specific team of data\nscientists, but usually is neither formalized nor shared with other teams.\nPotential synergies remain unused. We therefore introduce a novel approach\nwhich extends data management systems with additional knowledge-sharing\ncapabilities to facilitate user collaboration without altering established data\nanalysis workflows. Relevant collective knowledge from the query log is\nextracted to support data source discovery and incremental data integration.\nExtracted knowledge is formalized and provided at query time.\n"]},
{"authors": ["Md Tamzeed Islam", "Bashima Islam", "Mohammed Eunus Ali"], "title": ["A System for Identifying and Visualizing Influential Communities"], "date": ["2016-10-20T06:23:15Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1610.06298v1"], "summary": ["  In this paper, we introduce the concept of influential communities in a\nco-author network. We term a community as the most influential if the community\nhas the highest influence among all other communities in the entire network.\nInfluence of a community depends on the impact of the contents (e.g., citations\nof papers) generated by the members of that community. We propose an algorithm\nto identify the top K influential communities of an online social network. As a\nworking prototype, we develop a visualization system that allows a user to find\nthe top K influential communities from a co-author network. A user can search\ntop K influential communities of particular research fields and our system\nprovides him/her with a visualization of these communities. A user can explore\nthe details of a community, such as authors, citations, and collaborations with\nother communities.\n"]},
{"authors": ["Kien Do", "Truyen Tran", "Svetha Venkatesh"], "title": ["Multilevel Anomaly Detection for Mixed Data"], "date": ["2016-10-20T00:04:55Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1610.06249v1"], "summary": ["  Anomalies are those deviating from the norm. Unsupervised anomaly detection\noften translates to identifying low density regions. Major problems arise when\ndata is high-dimensional and mixed of discrete and continuous attributes. We\npropose MIXMAD, which stands for MIXed data Multilevel Anomaly Detection, an\nensemble method that estimates the sparse regions across multiple levels of\nabstraction of mixed data. The hypothesis is for domains where multiple data\nabstractions exist, a data point may be anomalous with respect to the raw\nrepresentation or more abstract representations. To this end, our method\nsequentially constructs an ensemble of Deep Belief Nets (DBNs) with varying\ndepths. Each DBN is an energy-based detector at a predefined abstraction level.\nAt the bottom level of each DBN, there is a Mixed-variate Restricted Boltzmann\nMachine that models the density of mixed data. Predictions across the ensemble\nare finally combined via rank aggregation. The proposed MIXMAD is evaluated on\nhigh-dimensional realworld datasets of different characteristics. The results\ndemonstrate that for anomaly detection, (a) multilevel abstraction of\nhigh-dimensional and mixed data is a sensible strategy, and (b) empirically,\nMIXMAD is superior to popular unsupervised detection methods for both\nhomogeneous and mixed data.\n"]},
{"authors": ["Suresh K. Damodaran", "Pedro A. Colon-Hernandez"], "title": ["Portable Ontological Expressions in NoSQL Queries"], "date": ["2016-10-19T16:07:30Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1610.06084v1"], "summary": ["  A significant barrier to the portability of queries across di- verse physical\nimplementations of large data stores, espe- cially NoSQL data stores, is that\nthe queries reference the physical storage attributes, such as the table and\ncolumn names. In this paper, we describe a technique for embed- ding\nontological expressions called Address Expressions, or A-Expressions, in NoSQL\nqueries to improve their portability across diverse physical implementations.\nWe discuss an implementation of such queries over a MongoDB data store of the\nEnron email corpus with examples, and conduct a preliminary performance\nassessment.\n"]},
{"authors": ["Koray Mancuhan", "Chris Clifton"], "title": ["K-Nearest Neighbor Classification Using Anatomized Data"], "date": ["2016-10-19T15:00:59Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1610.06048v1"], "summary": ["  This paper analyzes k nearest neighbor classification with training data\nanonymized using anatomy. Anatomy preserves all data values, but introduces\nuncertainty in the mapping between identifying and sensitive values. We first\nstudy the theoretical effect of the anatomized training data on the k nearest\nneighbor error rate bounds, nearest neighbor convergence rate, and Bayesian\nerror. We then validate the derived bounds empirically. We show that 1)\nLearning from anatomized data approaches the limits of learning through the\nunprotected data (although requiring larger training data), and 2) nearest\nneighbor using anatomized data outperforms nearest neighbor on\ngeneralization-based anonymization.\n"]},
{"authors": ["Karl Czajkowski", "Carl Kesselman", "Robert Schuler", "Hongsuda Tangmunarunkit"], "title": ["ERMrest: an entity-relationship data storage service for web-based,\n  data-oriented collaboration"], "date": ["2016-10-19T14:52:15Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1610.06044v1"], "summary": ["  Scientific discovery is increasingly dependent on a scientist's ability to\nacquire, curate, integrate, analyze, and share large and diverse collections of\ndata. While the details vary from domain to domain, these data often consist of\ndiverse digital assets (e.g. image files, sequence data, or simulation outputs)\nthat are organized with complex relationships and context which may evolve over\nthe course of an investigation. In addition, discovery is often collaborative,\nsuch that sharing of the data and its organizational context is highly\ndesirable. Common systems for managing file or asset metadata hide their\ninherent relational structures, while traditional relational database systems\ndo not extend to the distributed collaborative environment often seen in\nscientific investigations. To address these issues, we introduce ERMrest, a\ncollaborative data management service which allows general entity-relationship\nmodeling of metadata manipulated by RESTful access methods. We present the\ndesign criteria, architecture, and service implementation, as well as describe\nan ecosystem of tools and services that we have created to integrate metadata\ninto an end-to-end scientific data life cycle. ERMrest has been deployed to\nhundreds of users across multiple scientific research communities and projects.\nWe present two representative use cases: an international consortium and an\nearly-phase, multidisciplinary research project.\n"]},
{"authors": ["Koray Mancuhan", "Chris Clifton"], "title": ["Statistical Learning Theory Approach for Data Classification with\n  l-diversity"], "date": ["2016-10-18T22:14:27Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1610.05815v1"], "summary": ["  Corporations are retaining ever-larger corpuses of personal data; the\nfrequency or breaches and corresponding privacy impact have been rising\naccordingly. One way to mitigate this risk is through use of anonymized data,\nlimiting the exposure of individual data to only where it is absolutely needed.\nThis would seem particularly appropriate for data mining, where the goal is\ngeneralizable knowledge rather than data on specific individuals. In practice,\ncorporate data miners often insist on original data, for fear that they might\n\"miss something\" with anonymized or differentially private approaches. This\npaper provides a theoretical justification for the use of anonymized data.\nSpecifically, we show that a support vector classifier trained on anatomized\ndata satisfying l-diversity should be expected to do as well as on the original\ndata. Anatomy preserves all data values, but introduces uncertainty in the\nmapping between identifying and sensitive values, thus satisfying l-diversity.\nThe theoretical effectiveness of the proposed approach is validated using\nseveral publicly available datasets, showing that we outperform the state of\nthe art for support vector classification using training data protected by\nk-anonymity, and are comparable to learning on the original data.\n"]},
{"authors": ["Koray Mancuhan", "Chris Clifton"], "title": ["Decision Tree Classification on Outsourced Data"], "date": ["2016-10-18T20:49:21Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1610.05796v1"], "summary": ["  This paper proposes a client-server decision tree learning method for\noutsourced private data. The privacy model is anatomization/fragmentation: the\nserver sees data values, but the link between sensitive and identifying\ninformation is encrypted with a key known only to clients. Clients have limited\nprocessing and storage capability. Both sensitive and identifying information\nthus are stored on the server. The approach presented also retains most\nprocessing at the server, and client-side processing is amortized over\npredictions made by the clients. Experiments on various datasets show that the\nmethod produces decision trees approaching the accuracy of a non-private\ndecision tree, while substantially reducing the client's computing resource\nrequirements.\n"]},
{"authors": ["Yikai Zhang", "Jeffrey Xu Yu", "Ying Zhang", "Lu Qin"], "title": ["A Fast Order-Based Approach for Core Maintenance"], "date": ["2016-06-01T09:59:17Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1606.00200v2"], "summary": ["  Graphs have been widely used in many applications such as social networks,\ncollaboration networks, and biological networks. One important graph analytics\nis to explore cohesive subgraphs in a large graph. Among several cohesive\nsubgraphs studied, k-core is one that can be computed in linear time for a\nstatic graph. Since graphs are evolving in real applications, in this paper, we\nstudy core maintenance which is to reduce the computational cost to compute\nk-cores for a graph when graphs are updated from time to time dynamically. We\nidentify drawbacks of the existing efficient algorithm, which needs a large\nsearch space to find the vertices that need to be updated, and has high\noverhead to maintain the index built, when a graph is updated. We propose a new\norder-based approach to maintain an order, called k-order, among vertices,\nwhile a graph is updated. Our new algorithm can significantly outperform the\nstate-of-the-art algorithm up to 3 orders of magnitude for the 11 large real\ngraphs tested. We report our findings in this paper.\n"]},
{"authors": ["Hui Miao", "Amit Chavan", "Amol Deshpande"], "title": ["ProvDB: A System for Lifecycle Management of Collaborative Analysis\n  Workflows"], "date": ["2016-10-17T03:22:58Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1610.04963v1"], "summary": ["  As data-driven methods are becoming pervasive in a wide variety of\ndisciplines, there is an urgent need to develop scalable and sustainable tools\nto simplify the process of data science, to make it easier to keep track of the\nanalyses being performed and datasets being generated, and to enable\nintrospection of the workflows. In this paper, we describe our vision of a\nunified provenance and metadata management system to support lifecycle\nmanagement of complex collaborative data science workflows. We argue that a\nlarge amount of information about the analysis processes and data artifacts\ncan, and should be, captured in a semi-passive manner; and we show that\nquerying and analyzing this information can not only simplify bookkeeping and\ndebugging tasks for data analysts but can also enable a rich new set of\ncapabilities like identifying flaws in the data science process itself. It can\nalso significantly reduce the time spent in fixing post-deployment problems\nthrough automated analysis and monitoring. We have implemented an initial\nprototype of our system, called ProvDB, on top of git (a version control\nsystem) and Neo4j (a graph database), and we describe its key features and\ncapabilities.\n"]},
{"authors": ["Doan Van Thang", "Doan Van Ban"], "title": ["Query Data With Fuzzy Information In Object-Oriented Databases An\n  Approach Interval Values"], "date": ["2016-10-16T12:29:38Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1611.04977v1"], "summary": ["  In this paper, we propose methods of handling attributive values of object\nclasses in object oriented database with fuzzy information and uncertainty\nbased on quantitatively semantics based hedge algebraic. In this approach we\nconsider to attributive values (as well as methods) object class is interval\nvalues and the interval values are converted into sub interval in [0, 1]\nrespectively. That its the fuzziness of the elements in the hedge algebra is\nalso sub interval in [0,1]. So, we present an algorithm allows the comparison\nof two sub interval [0,1] helping the requirements of the query data\n"]},
{"authors": ["Paolo Missier", "Jacek Cala", "Maisha Rathi"], "title": ["Preserving the value of large scale data analytics over time through\n  selective re-computation"], "date": ["2016-10-15T16:08:22Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1610.04752v1"], "summary": ["  A pervasive problem in Data Science is that the knowledge generated by\npossibly expensive analytics processes is subject to decay over time, as the\ndata used to compute it drifts, the algorithms used in the processes are\nimproved, and the external knowledge embodied by reference datasets used in the\ncomputation evolves. Deciding when such knowledge outcomes should be refreshed,\nfollowing a sequence of data change events, requires problem-specific functions\nto quantify their value and its decay over time, as well as models for\nestimating the cost of their re-computation. What makes this problem\nchallenging is the ambition to develop a decision support system for informing\ndata analytics re-computation decisions over time, that is both generic and\ncustomisable. With the help of a case study from genomics, in this vision paper\nwe offer an initial formalisation of this problem, highlight research\nchallenges, and outline a possible approach based on the collection and\nanalysis of metadata from a history of past computations.\n"]},
{"authors": ["Renzo Angles", "Claudio Gutierrez"], "title": ["The multiset semantics of SPARQL patterns"], "date": ["2016-10-14T03:19:54Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1610.04315v1"], "summary": ["  The paper determines the algebraic and logic structure of the multiset\nsemantics of the core patterns of SPARQL. We prove that the fragment formed by\nAND, UNION, OPTIONAL, FILTER, MINUS and SELECT corresponds precisely to both,\nthe intuitive multiset relational algebra (projection, selection, natural join,\narithmetic union and except), and the multiset non-recursive Datalog with safe\nnegation.\n"]},
{"authors": ["Kaustubh Beedkar", "Rainer Gemulla"], "title": ["DESQ: Frequent Sequence Mining with Subsequence Constraints"], "date": ["2016-09-27T13:34:25Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1609.08431v2"], "summary": ["  Frequent sequence mining methods often make use of constraints to control\nwhich subsequences should be mined. A variety of such subsequence constraints\nhas been studied in the literature, including length, gap, span,\nregular-expression, and hierarchy constraints. In this paper, we show that many\nsubsequence constraints---including and beyond those considered in the\nliterature---can be unified in a single framework. A unified treatment allows\nresearchers to study jointly many types of subsequence constraints (instead of\neach one individually) and helps to improve usability of pattern mining systems\nfor practitioners. In more detail, we propose a set of simple and intuitive\n\"pattern expressions\" to describe subsequence constraints and explore\nalgorithms for efficiently mining frequent subsequences under such general\nconstraints. Our algorithms translate pattern expressions to compressed finite\nstate transducers, which we use as computational model, and simulate these\ntransducers in a way suitable for frequent sequence mining. Our experimental\nstudy on real-world datasets indicates that our algorithms---although more\ngeneral---are competitive to existing state-of-the-art algorithms.\n"]},
{"authors": ["Shangbo Mao", "Enmei Tu", "Guanghao Zhang", "Lily Rachmawati", "Eshan Rajabally", "Guang-Bin Huang"], "title": ["An Automatic Identification System (AIS) Database for Maritime\n  Trajectory Prediction and Data Mining"], "date": ["2016-07-12T10:49:59Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1607.03306v4"], "summary": ["  In recent years, maritime safety and efficiency become more and more\nimportant across the world. Automatic Identification System (AIS) tracks vessel\nmovement by onboard transceiver and terrestrial and/or satellite base station.\nThe data collected by AIS contains broadcast kinematic information and static\ninformation. Both of them are useful for anomaly detection and route prediction\nwhich are key techniques in intelligent maritime research area. This paper is\ndevoted to construct a standard AIS database for maritime trajectory learning,\nprediction and data mining. A path prediction algorithm is tested on this AIS\ndatabase and the testing results show this database can be used as a\nstandardized training resource for different trajectory prediction algorithms\nand other AIS data mining algorithms.\n"]},
{"authors": ["Bj\u00f8rn Magnus Mathisen", "Leendert Wienhofen", "Dumitru Roman"], "title": ["Empirical Big Data Research: A Systematic Literature Mapping"], "date": ["2015-09-10T07:45:52Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1509.03045v2"], "summary": ["  Background: Big Data is a relatively new field of research and technology,\nand literature reports a wide variety of concepts labeled with Big Data. The\nmaturity of a research field can be measured in the number of publications\ncontaining empirical results. In this paper we present the current status of\nempirical research in Big Data. Method: We employed a systematic mapping method\nwith which we mapped the collected research according to the labels Variety,\nVolume and Velocity. In addition, we addressed the application areas of Big\nData. Results: We found that 151 of the assessed 1778 contributions contain a\nform of empirical result and can be mapped to one or more of the 3 V's and 59\naddress an application area. Conclusions: The share of publications containing\nempirical results is well below the average compared to computer science\nresearch as a whole. In order to mature the research on Big Data, we recommend\napplying empirical methods to strengthen the confidence in the reported\nresults. Based on our trend analysis we consider Volume and Variety to be the\nmost promising uncharted area in Big Data.\n"]},
{"authors": ["Niek Tax", "Natalia Sidorova", "Wil M. P. van der Aalst", "Reinder Haakma"], "title": ["Heuristic Approaches for Generating Local Process Models through Log\n  Projections"], "date": ["2016-10-10T12:12:46Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1610.02876v1"], "summary": ["  Local Process Model (LPM) discovery is focused on the mining of a set of\nprocess models where each model describes the behavior represented in the event\nlog only partially, i.e. subsets of possible events are taken into account to\ncreate so-called local process models. Often such smaller models provide\nvaluable insights into the behavior of the process, especially when no adequate\nand comprehensible single overall process model exists that is able to describe\nthe traces of the process from start to end. The practical application of LPM\ndiscovery is however hindered by computational issues in the case of logs with\nmany activities (problems may already occur when there are more than 17 unique\nactivities). In this paper, we explore three heuristics to discover subsets of\nactivities that lead to useful log projections with the goal of speeding up LPM\ndiscovery considerably while still finding high-quality LPMs. We found that a\nMarkov clustering approach to create projection sets results in the largest\nimprovement of execution time, with discovered LPMs still being better than\nwith the use of randomly generated activity sets of the same size. Another\nheuristic, based on log entropy, yields a more moderate speedup, but enables\nthe discovery of higher quality LPMs. The third heuristic, based on the\nrelative information gain, shows unstable performance: for some data sets the\nspeedup and LPM quality are higher than with the log entropy based method,\nwhile for other data sets there is no speedup at all.\n"]},
{"authors": ["Faegheh Hasibi", "Svein Erik Bratsberg"], "title": ["Non-hierarchical Structures: How to Model and Index Overlaps?"], "date": ["2014-08-05T16:07:11Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1408.1011v3"], "summary": ["  Overlap is a common phenomenon seen when structural components of a digital\nobject are neither disjoint nor nested inside each other. Overlapping\ncomponents resist reduction to a structural hierarchy, and tree-based indexing\nand query processing techniques cannot be used for them. Our solution to this\ndata modeling problem is TGSA (Tree-like Graph for Structural Annotations), a\nnovel extension of the XML data model for non-hierarchical structures. We\nintroduce an algorithm for constructing TGSA from annotated documents; the\nalgorithm can efficiently process non-hierarchical structures and is associated\nwith formal proofs, ensuring that transformation of the document to the data\nmodel is valid. To enable high performance query analysis in large data\nrepositories, we further introduce an extension of XML pre-post indexing for\nnon-hierarchical structures, which can process both reachability and\noverlapping relationships.\n"]},
{"authors": ["Wen Li", "Ying Zhang", "Yifang Sun", "Wei Wang", "Wenjie Zhang", "Xuemin Lin"], "title": ["Approximate Nearest Neighbor Search on High Dimensional Data ---\n  Experiments, Analyses, and Improvement (v1.0)"], "date": ["2016-10-08T00:40:14Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1610.02455v1"], "summary": ["  Approximate Nearest neighbor search (ANNS) is fundamental and essential\noperation in applications from many domains, such as databases, machine\nlearning, multimedia, and computer vision. Although many algorithms have been\ncontinuously proposed in the literature in the above domains each year, there\nis no comprehensive evaluation and analysis of their performances.\n  In this paper, we conduct a comprehensive experimental evaluation of many\nstate-of-the-art methods for approximate nearest neighbor search. Our study (1)\nis cross-disciplinary (i.e., including 16 algorithms in different domains, and\nfrom practitioners) and (2) has evaluated a diverse range of settings,\nincluding 20 datasets, several evaluation metrics, and different query\nworkloads. The experimental results are carefully reported and analyzed to\nunderstand the performance results. Furthermore, we propose a new method that\nachieves both high query efficiency and high recall empirically on majority of\nthe datasets under a wide range of settings.\n"]},
{"authors": ["Xiaowang Zhang", "Zhiyong Feng", "Xin Wang", "Guozheng Rao", "Wenrui Wu"], "title": ["Context-Free Path Queries on RDF Graphs"], "date": ["2015-06-02T03:39:59Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1506.00743v3"], "summary": ["  Navigational graph queries are an important class of queries that canextract\nimplicit binary relations over the nodes of input graphs. Most of the\nnavigational query languages used in the RDF community, e.g. property paths in\nW3C SPARQL 1.1 and nested regular expressions in nSPARQL, are based on the\nregular expressions. It is known that regular expressions have limited\nexpressivity; for instance, some natural queries, like same generation-queries,\nare not expressible with regular expressions. To overcome this limitation, in\nthis paper, we present cfSPARQL, an extension of SPARQL query language equipped\nwith context-free grammars. The cfSPARQL language is strictly more expressive\nthan property paths and nested expressions. The additional expressivity can be\nused for modelling graph similarities, graph summarization and ontology\nalignment. Despite the increasing expressivity, we show that cfSPARQL still\nenjoys a low computational complexity and can be evaluated efficiently.\n"]},
{"authors": ["Xiaowang Zhang", "Zhenyu Song", "Zhiyong Feng", "Xin Wang"], "title": ["PIWD: A Plugin-based Framework for Well-Designed SPARQL"], "date": ["2016-07-27T05:47:34Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1607.07967v2"], "summary": ["  In the real world datasets (e.g.,DBpedia query log), queries built on\nwell-designed patterns containing only AND and OPT operators (for short,\nWDAO-patterns) account for a large proportion among all SPARQL queries. In this\npaper, we present a plugin-based framework for all SELECT queries built on\nWDAO-patterns, named PIWD. The framework is based on a parse tree called\n\\emph{well-designed AND-OPT tree} (for short, WDAO-tree) whose leaves are basic\ngraph patterns (BGP) and inner nodes are the OPT operators. We prove that for\nany WDAO-pattern, its parse tree can be equivalently transformed into a\nWDAO-tree. Based on the proposed framework, we can employ any query engine to\nevaluate BGP for evaluating queries built on WDAO-patterns in a convenient way.\nTheoretically, we can reduce the query evaluation of WDAO-patterns to subgraph\nhomomorphism as well as BGP since the query evaluation of BGP is equivalent to\nsubgraph homomorphism. Finally, our preliminary experiments on gStore and\nRDF-3X show that PIWD can answer all queries built on WDAO-patterns effectively\nand efficiently.\n"]},
{"authors": ["Luciano Barbosa", "Breno W. Carvalho", "Bianca Zadrozny"], "title": ["Pooling Hybrid Representations for Web Structured Data Annotation"], "date": ["2016-10-03T11:12:12Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1610.00493v1"], "summary": ["  Automatically identifying data types of web structured data is a key step in\nthe process of web data integration. Web structured data is usually associated\nwith entities or objects in a particular domain. In this paper, we aim to map\nattributes of an entity in a given domain to pre-specified classes of\nattributes in the same domain based on their values. To perform this task, we\npropose a hybrid deep learning network that relies on the format of the\nattributes' values. It does so without any pre-processing or using pre-defined\nhand-crafted features. The hybrid network combines sequence-based neural\nnetworks, namely convolutional neural networks (CNN) and recurrent neural\nnetworks (RNN), to learn the sequence structure of attributes' values. The CNN\ncaptures short-distance dependencies in these sequences through a sliding\nwindow approach, and the RNN captures long-distance dependencies by storing\ninformation of previous characters. These networks create different vector\nrepresentations of the input sequence which are combined using a pooling layer.\nThis layer applies a specific operation on these vectors in order to capture\ntheir most useful patterns for the task. Finally, on top of the pooling layer,\na softmax function predicts the label of a given attribute value. We evaluate\nour strategy in four different web domains. The results show that the pooling\nnetwork outperforms previous approaches, which use some kind of input\npre-processing, in all domains.\n"]},
{"authors": ["Jianzhong Qi", "Fei Zuo", "Hanan Samet", "Jia Cheng Yao"], "title": ["K-Regret Queries: From Additive to Multiplicative Utilities"], "date": ["2016-09-26T13:23:43Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1609.07964v3"], "summary": ["  The k-regret query aims to return a size-k subset of the entire database such\nthat, for any query user that selects a data object in this size-k subset\nrather than in the entire database, her regret ratio is minimized. Here, the\nregret ratio is modeled by the level of difference in the optimality between\nthe optimal object in the size-k subset returned and the optimal object in the\nentire database. The optimality of a data object in turn is usually modeled by\na utility function of the query user. Compared with traditional top-k queries,\nk-regret queries have the advantage of not requiring users to specify their\nutility functions. They can discover a size-k subset that minimizes the regret\nratio for a whole family of utility functions without knowing any particular of\nthem. Previous studies have answered k-regret queries with additive utility\nfunctions such as the linear summation function. However, no existing result\nhas been reported to answer k-regret queries with multiplicative utility\nfunctions, which are an important family of utility functions.\n  In this study, we break the barrier of multiplicative utility functions. We\npresent an algorithm that can produce answers with a bounded regret ratio to\nk-regret queries with multiplicative utility functions. As a case study we\napply this algorithm to process a special type of multiplicative utility\nfunctions, the Cobb-Douglas function, and a closely related function, the\nConstant Elasticity of Substitution function. We perform extensive experiments\non the proposed algorithm. The results confirm that the proposed algorithm can\nanswer k-regret queries with multiplicative utility functions efficiently with\na constantly small regret ratio.\n"]},
{"authors": ["Jacob Abernethy", "Cyrus Anderson", "Alex Chojnacki", "Chengyu Dai", "John Dryden", "Eric Schwartz", "Wenbo Shen", "Jonathan Stroud", "Laura Wendlandt", "Sheng Yang", "Daniel Zhang"], "title": ["Data Science in Service of Performing Arts: Applying Machine Learning to\n  Predicting Audience Preferences"], "date": ["2016-09-30T03:49:16Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1611.05788v1"], "summary": ["  Performing arts organizations aim to enrich their communities through the\narts. To do this, they strive to match their performance offerings to the taste\nof those communities. Success relies on understanding audience preference and\npredicting their behavior. Similar to most e-commerce or digital entertainment\nfirms, arts presenters need to recommend the right performance to the right\ncustomer at the right time. As part of the Michigan Data Science Team (MDST),\nwe partnered with the University Musical Society (UMS), a non-profit performing\narts presenter housed in the University of Michigan, Ann Arbor. We are\nproviding UMS with analysis and business intelligence, utilizing historical\nindividual-level sales data. We built a recommendation system based on\ncollaborative filtering, gaining insights into the artistic preferences of\ncustomers, along with the similarities between performances. To better\nunderstand audience behavior, we used statistical methods from customer-base\nanalysis. We characterized customer heterogeneity via segmentation, and we\nmodeled customer cohorts to understand and predict ticket purchasing patterns.\nFinally, we combined statistical modeling with natural language processing\n(NLP) to explore the impact of wording in program descriptions. These ongoing\nefforts provide a platform to launch targeted marketing campaigns, helping UMS\ncarry out its mission by allocating its resources more efficiently. Celebrating\nits 138th season, UMS is a 2014 recipient of the National Medal of Arts, and it\ncontinues to enrich communities by connecting world-renowned artists with\ndiverse audiences, especially students in their formative years. We aim to\ncontribute to that mission through data science and customer analytics.\n"]},
{"authors": ["Davis W. Blalock", "John V. Guttag"], "title": ["EXTRACT: Strong Examples from Weakly-Labeled Sensor Data"], "date": ["2016-09-29T04:02:31Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1609.09196v1"], "summary": ["  Thanks to the rise of wearable and connected devices, sensor-generated time\nseries comprise a large and growing fraction of the world's data.\nUnfortunately, extracting value from this data can be challenging, since\nsensors report low-level signals (e.g., acceleration), not the high-level\nevents that are typically of interest (e.g., gestures). We introduce a\ntechnique to bridge this gap by automatically extracting examples of real-world\nevents in low-level data, given only a rough estimate of when these events have\ntaken place.\n  By identifying sets of features that repeat in the same temporal arrangement,\nwe isolate examples of such diverse events as human actions, power consumption\npatterns, and spoken words with up to 96% precision and recall. Our method is\nfast enough to run in real time and assumes only minimal knowledge of which\nvariables are relevant or the lengths of events. Our evaluation uses numerous\npublicly available datasets and over 1 million samples of manually labeled\nsensor data.\n"]},
{"authors": ["Zhiyong Shan"], "title": ["A Study on Altering PostgreSQL from Multi-Processes Structure to\n  Multi-Threads Structure"], "date": ["2016-09-29T03:01:44Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1609.09062v1"], "summary": ["  How to altering PostgreSQL database from multi-processes structure to\nmulti-threads structure is a difficult problem. In the paper, we bring forward\na comprehensive alteration scheme. Especially, put rational methods to account\nfor three difficult points: semaphores, signal processing and global variables.\nAt last, applied the scheme successfully to modify a famous open source DBMS.\n"]},
{"authors": ["Yonghui Xiao", "Yilin Shen", "Jinfei Liu", "Li Xiong", "Hongxia Jin", "Xiaofeng Xu"], "title": ["DPHMM: Customizable Data Release with Differential Privacy via Hidden\n  Markov Model"], "date": ["2016-09-29T02:00:58Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1609.09172v1"], "summary": ["  Hidden Markov model (HMM) has been well studied and extensively used. In this\npaper, we present DPHMM ({Differentially Private Hidden Markov Model}), an HMM\nembedded with a private data release mechanism, in which the privacy of the\ndata is protected through a graph. Specifically, we treat every state in Markov\nmodel as a node, and use a graph to represent the privacy policy, in which\n\"indistinguishability\" between states is denoted by edges between nodes. Due to\nthe temporal correlations in Markov model, we show that the graph may be\nreduced to a subgraph with disconnected nodes, which become unprotected and\nmight be exposed. To detect such privacy risk, we define sensitivity hull and\ndegree of protection based on the graph to capture the condition of information\nexposure. Then to tackle the detected exposure, we study how to build an\noptimal graph based on the existing graph. We also implement and evaluate the\nDPHMM on real-world datasets, showing that privacy and utility can be better\ntuned with customized policy graph.\n"]},
{"authors": ["Till Sch\u00e4fer", "Petra Mutzel"], "title": ["StruClus: Structural Clustering of Large-Scale Graph Databases"], "date": ["2016-09-28T16:43:12Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1609.09000v1"], "summary": ["  We present a structural clustering algorithm for large-scale datasets of\nsmall labeled graphs, utilizing a frequent subgraph sampling strategy. A set of\nrepresentatives provides an intuitive description of each cluster, supports the\nclustering process, and helps to interpret the clustering results. The\nprojection-based nature of the clustering approach allows us to bypass\ndimensionality and feature extraction problems that arise in the context of\ngraph datasets reduced to pairwise distances or feature vectors. While\nachieving high quality and (human) interpretable clusterings, the runtime of\nthe algorithm only grows linearly with the number of graphs. Furthermore, the\napproach is easy to parallelize and therefore suitable for very large datasets.\nOur extensive experimental evaluation on synthetic and real world datasets\ndemonstrates the superiority of our approach over existing structural and\nsubspace clustering algorithms, both, from a runtime and quality point of view.\n"]},
{"authors": ["Elena Alfaro Martinez", "Maria Hernandez Rubio", "Roberto Maestre Martinez", "Juan Murillo Arias", "Dario Patane", "Amanda Zerbe", "Robert Kirkpatrick", "Miguel Luengo-Oroz", "Amanda Zerbe"], "title": ["Measuring Economic Resilience to Natural Disasters with Big Economic\n  Transaction Data"], "date": ["2016-09-28T01:20:23Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1609.09340v1"], "summary": ["  This research explores the potential to analyze bank card payments and ATM\ncash withdrawals in order to map and quantify how people are impacted by and\nrecover from natural disasters. Our approach defines a disaster-affected\ncommunity's economic recovery time as the time needed to return to baseline\nactivity levels in terms of number of bank card payments and ATM cash\nwithdrawals. For Hurricane Odile, which hit the state of Baja California Sur\n(BCS) in Mexico between 15 and 17 September 2014, we measured and mapped\ncommunities' economic recovery time, which ranged from 2 to 40 days in\ndifferent locations. We found that -- among individuals with a bank account --\nthe lower the income level, the shorter the time needed for economic activity\nto return to normal levels. Gender differences in recovery times were also\ndetected and quantified. In addition, our approach evaluated how communities\nprepared for the disaster by quantifying expenditure growth in food or gasoline\nbefore the hurricane struck. We believe this approach opens a new frontier in\nmeasuring the economic impact of disasters with high temporal and spatial\nresolution, and in understanding how populations bounce back and adapt.\n"]},
{"authors": ["Timothy Weale", "Vijay Gadepally", "Dylan Hutchison", "Jeremy Kepner"], "title": ["Benchmarking the Graphulo Processing Framework"], "date": ["2016-09-27T20:09:03Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1609.08642v1"], "summary": ["  Graph algorithms have wide applicablity to a variety of domains and are often\nused on massive datasets. Recent standardization efforts such as the GraphBLAS\nspecify a set of key computational kernels that hardware and software\ndevelopers can adhere to. Graphulo is a processing framework that enables\nGraphBLAS kernels in the Apache Accumulo database. In our previous work, we\nhave demonstrated a core Graphulo operation called \\textit{TableMult} that\nperforms large-scale multiplication operations of database tables. In this\narticle, we present the results of scaling the Graphulo engine to larger\nproblems and scalablity when a greater number of resources is used.\nSpecifically, we present two experiments that demonstrate Graphulo scaling\nperformance is linear with the number of available resources. The first\nexperiment demonstrates cluster processing rates through Graphulo's TableMult\noperator on two large graphs, scaled between $2^{17}$ and $2^{19}$ vertices.\nThe second experiment uses TableMult to extract a random set of rows from a\nlarge graph ($2^{19}$ nodes) to simulate a cued graph analytic. These\nbenchmarking results are of relevance to Graphulo users who wish to apply\nGraphulo to their graph problems.\n"]},
{"authors": ["Vijil Chenthamarakshan", "Prasad M Desphande", "Raghu Krishnapuram", "Ramakrishna Varadarajan", "Knut Stolze"], "title": ["WYSIWYE: An Algebra for Expressing Spatial and Textual Rules for Visual\n  Information Extraction"], "date": ["2015-06-28T21:17:26Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1506.08454v2"], "summary": ["  The visual layout of a webpage can provide valuable clues for certain types\nof Information Extraction (IE) tasks. In traditional rule based IE frameworks,\nthese layout cues are mapped to rules that operate on the HTML source of the\nwebpages. In contrast, we have developed a framework in which the rules can be\nspecified directly at the layout level. This has many advantages, since the\nhigher level of abstraction leads to simpler extraction rules that are largely\nindependent of the source code of the page, and, therefore, more robust. It can\nalso enable specification of new types of rules that are not otherwise\npossible. To the best of our knowledge, there is no general framework that\nallows declarative specification of information extraction rules based on\nspatial layout. Our framework is complementary to traditional text based rules\nframework and allows a seamless combination of spatial layout based rules with\ntraditional text based rules. We describe the algebra that enables such a\nsystem and its efficient implementation using standard relational and text\nindexing features of a relational database. We demonstrate the simplicity and\nefficiency of this system for a task involving the extraction of software\nsystem requirements from software product pages.\n"]},
{"authors": ["Gregor Joss\u00e9", "Ying Lu", "Tobias Emrich", "Matthias Renz", "Cyrus Shahabi", "Ugur Demiryurek", "Matthias Schubert"], "title": ["Scenic Routes Now: Efficiently Solving the Time-Dependent Arc\n  Orienteering Problem"], "date": ["2016-09-27T14:50:15Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1609.08484v1"], "summary": ["  This paper extends the Arc Orienteering Problem (AOP) to large road networks\nwith time-dependent travel times and time-dependent value gain, termed Twofold\nTime-Dependent AOP or 2TD-AOP for short. In its original definition, the\nNP-hard Orienteering Problem (OP) asks to find a path from a source to a\ndestination maximizing the accumulated value while not exceeding a cost budget.\nVariations of the OP and AOP have many practical applications such as mobile\ncrowdsourcing tasks (e.g., repairing and maintenance or dispatching field\nworkers), diverse logistics problems (e.g., crowd control or controlling\nwildfires) as well as several tourist guidance problems (e.g., generating trip\nrecommendations or navigating through theme parks). In the proposed 2TD-AOP,\ntravel times and value functions are assumed to be time-dependent. The dynamic\nvalues model, for instance, varying rewards in crowdsourcing tasks or varying\nurgency levels in damage control tasks. We discuss this novel problem, prove\nthe benefit of time-dependence empirically and present an efficient\napproximative solution, optimized for fast response systems. Our approach is\nthe first time-dependent variant of the AOP to be evaluated on a large scale,\nfine-grained, real-world road network. We show that optimal solutions are\ninfeasible and solutions to the static problem are often invalid. We propose an\napproximate dynamic programming solution which produces valid paths and is\norders of magnitude faster than any optimal solution.\n"]},
{"authors": ["Jihwan Lee", "Keehwan Park", "Sunil Prabhakar"], "title": ["Mining Statistically Significant Attribute Associations in Attributed\n  Graphs"], "date": ["2016-09-27T05:53:45Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1609.08266v1"], "summary": ["  Recently, graphs have been widely used to represent many different kinds of\nreal world data or observations such as social networks, protein-protein\nnetworks, road networks, and so on. In many cases, each node in a graph is\nassociated with a set of its attributes and it is critical to not only consider\nthe link structure of a graph but also use the attribute information to achieve\nmore meaningful results in various graph mining tasks. Most previous works with\nattributed graphs take into ac- count attribute relationships only between\nindividually connected nodes. However, it should be greatly valuable to find\nout which sets of attributes are associated with each other and whether they\nare statistically significant or not. Mining such significant associations, we\ncan uncover novel relationships among the sets of attributes in the graph. We\npropose an algorithm that can find those attribute associations efficiently and\neffectively, and show experimental results that confirm the high applicability\nof the proposed algorithm.\n"]},
{"authors": ["Sourav Medya", "Petko Bogdanov", "Ambuj Singh"], "title": ["Towards Scalable Network Delay Minimization"], "date": ["2016-09-27T00:29:03Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1609.08228v1"], "summary": ["  Reduction of end-to-end network delays is an optimization task with\napplications in multiple domains. Low delays enable improved information flow\nin social networks, quick spread of ideas in collaboration networks, low travel\ntimes for vehicles on road networks and increased rate of packets in the case\nof communication networks. Delay reduction can be achieved by both improving\nthe propagation capabilities of individual nodes and adding additional edges in\nthe network. One of the main challenges in such design problems is that the\neffects of local changes are not independent, and as a consequence, there is a\ncombinatorial search-space of possible improvements. Thus, minimizing the\ncumulative propagation delay requires novel scalable and data-driven\napproaches.\n  In this paper, we consider the problem of network delay minimization via node\nupgrades. Although the problem is NP-hard, we show that probabilistic\napproximation for a restricted version can be obtained. We design scalable and\nhigh-quality techniques for the general setting based on sampling and targeted\nto different models of delay distribution. Our methods scale almost linearly\nwith the graph size and consistently outperform competitors in quality.\n"]},
{"authors": ["Shahriar Shariat", "Vladimir Pavlovic"], "title": ["Robust Time-Series Retrieval Using Probabilistic Adaptive Segmental\n  Alignment"], "date": ["2016-09-26T21:53:42Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1609.08201v1"], "summary": ["  Traditional pairwise sequence alignment is based on matching individual\nsamples from two sequences, under time monotonicity constraints. However, in\nmany application settings matching subsequences (segments) instead of\nindividual samples may bring in additional robustness to noise or local\nnon-causal perturbations. This paper presents an approach to segmental sequence\nalignment that jointly segments and aligns two sequences, generalizing the\ntraditional per-sample alignment. To accomplish this task, we introduce a\ndistance metric between segments based on average pairwise distances and then\npresent a modified pair-HMM (PHMM) that incorporates the proposed distance\nmetric to solve the joint segmentation and alignment task. We also propose a\nrelaxation to our model that improves the computational efficiency of the\ngeneric segmental PHMM. Our results demonstrate that this new measure of\nsequence similarity can lead to improved classification performance, while\nbeing resilient to noise, on a variety of sequence retrieval problems, from EEG\nto motion sequence classification.\n"]},
{"authors": ["Jayanth Jayanth"], "title": ["Optimizations and Heuristics to improve Compression in Columnar Database\n  Systems"], "date": ["2016-09-26T00:44:51Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1609.07823v1"], "summary": ["  In-memory columnar databases have become mainstream over the last decade and\nhave vastly improved the fast processing of large volumes of data through\nmulti-core parallelism and in-memory compression thereby eliminating the usual\nbottlenecks associated with disk-based databases. For scenarios, where the data\nvolume grows into terabytes and petabytes, keeping all the data in memory is\nexorbitantly expensive. Hence, the data is compressed efficiently using\ndifferent algorithms to exploit the multi-core parallelization technologies for\nquery processing. Several compression methods are studied for compressing the\ncolumn array, post Dictionary Encoding. In this paper, we will present two\nnovel optimizations in compression techniques - Block Size Optimized Cluster\nEncoding and Block Size Optimized Indirect Encoding - which perform better than\ntheir predecessors. In the end, we also propose heuristics to choose the best\nencoding amongst common compression schemes.\n"]},
{"authors": ["Vijay Gadepally", "Peinan Chen", "Jennie Duggan", "Aaron Elmore", "Brandon Haynes", "Jeremy Kepner", "Samuel Madden", "Tim Mattson", "Michael Stonebraker"], "title": ["The BigDAWG Polystore System and Architecture"], "date": ["2016-09-24T01:14:06Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1609.07548v1"], "summary": ["  Organizations are often faced with the challenge of providing data management\nsolutions for large, heterogenous datasets that may have different underlying\ndata and programming models. For example, a medical dataset may have\nunstructured text, relational data, time series waveforms and imagery. Trying\nto fit such datasets in a single data management system can have adverse\nperformance and efficiency effects. As a part of the Intel Science and\nTechnology Center on Big Data, we are developing a polystore system designed\nfor such problems. BigDAWG (short for the Big Data Analytics Working Group) is\na polystore system designed to work on complex problems that naturally span\nacross different processing or storage engines. BigDAWG provides an\narchitecture that supports diverse database systems working with different data\nmodels, support for the competing notions of location transparency and semantic\ncompleteness via islands and a middleware that provides a uniform multi--island\ninterface. Initial results from a prototype of the BigDAWG system applied to a\nmedical dataset validate polystore concepts. In this article, we will describe\npolystore databases, the current BigDAWG architecture and its application on\nthe MIMIC II medical dataset, initial performance results and our future\ndevelopment plans.\n"]},
{"authors": ["Siddharth Samsi", "Laura Brattain", "William Arcand", "David Bestor", "Bill Bergeron", "Chansup Byun", "Vijay Gadepally", "Michael Houle", "Matthew Hubbell", "Michael Jones", "Anna Klein", "Peter Michaleas", "Lauren Milechin", "Julie Mullen", "Andrew Prout", "Antonio Rosa", "Charles Yee", "Jeremy Kepner", "Albert Reuther"], "title": ["Benchmarking SciDB Data Import on HPC Systems"], "date": ["2016-09-24T01:01:30Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1609.07545v1"], "summary": ["  SciDB is a scalable, computational database management system that uses an\narray model for data storage. The array data model of SciDB makes it ideally\nsuited for storing and managing large amounts of imaging data. SciDB is\ndesigned to support advanced analytics in database, thus reducing the need for\nextracting data for analysis. It is designed to be massively parallel and can\nrun on commodity hardware in a high performance computing (HPC) environment. In\nthis paper, we present the performance of SciDB using simulated image data. The\nDynamic Distributed Dimensional Data Model (D4M) software is used to implement\nthe benchmark on a cluster running the MIT SuperCloud software stack. A peak\nperformance of 2.2M database inserts per second was achieved on a single node\nof this system. We also show that SciDB and the D4M toolbox provide more\nefficient ways to access random sub-volumes of massive datasets compared to the\ntraditional approaches of reading volumetric data from individual files. This\nwork describes the D4M and SciDB tools we developed and presents the initial\nperformance results. This performance was achieved by using parallel inserts, a\nin-database merging of arrays as well as supercomputing techniques, such as\ndistributed arrays and single-program-multiple-data programming.\n"]},
{"authors": ["Fernando S\u00e1enz-P\u00e9rez"], "title": ["Intuitionistic Logic Programming for SQL (Extended Abstract)"], "date": ["2016-08-11T17:01:37Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1608.03535v2"], "summary": ["  Intuitionistic logic programming provides the notion of embedded implication\nin rule bodies, which can be used to reason about a current database modified\nby the antecedent. This can be applied to a system that translates SQL to\nDatalog to solve SQL WITH queries, for which relations are locally defined and\ncan therefore be understood as added to the current database. In addition,\nassumptions in SQL queries as either adding or removing data can be modelled in\nthis way as well, which is an interesting feature for decision-support\nscenarios. This work suggests a way to apply intuitionistic logic programming\nto SQL, and provides a pointer to a working system implementing this idea.\n"]},
{"authors": ["Adina Crainiceanu", "Daniel Lemire"], "title": ["Bloofi: Multidimensional Bloom Filters"], "date": ["2015-01-08T20:04:46Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1501.01941v3"], "summary": ["  Bloom filters are probabilistic data structures commonly used for approximate\nmembership problems in many areas of Computer Science (networking, distributed\nsystems, databases, etc.). With the increase in data size and distribution of\ndata, problems arise where a large number of Bloom filters are available, and\nall them need to be searched for potential matches. As an example, in a\nfederated cloud environment, each cloud provider could encode the information\nusing Bloom filters and share the Bloom filters with a central coordinator. The\nproblem of interest is not only whether a given element is in any of the sets\nrepresented by the Bloom filters, but which of the existing sets contain the\ngiven element. This problem cannot be solved by just constructing a Bloom\nfilter on the union of all the sets. Instead, we effectively have a\nmultidimensional Bloom filter problem: given an element, we wish to receive a\nlist of candidate sets where the element might be.\n  To solve this problem, we consider 3 alternatives. Firstly, we can naively\ncheck many Bloom filters. Secondly, we propose to organize the Bloom filters in\na hierarchical index structure akin to a B+ tree, that we call Bloofi. Finally,\nwe propose another data structure that packs the Bloom filters in such a way as\nto exploit bit-level parallelism, which we call Flat-Bloofi.\n  Our theoretical and experimental results show that Bloofi and Flat-Bloofi\nprovide scalable and efficient solutions alternatives to search through a large\nnumber of Bloom filters.\n"]},
{"authors": ["Natacha Crooks", "Youer Pu", "Lorenzo Alvisi", "Allen Clement"], "title": ["Seeing is Believing: A Unified Model for Consistency and Isolation via\n  States"], "date": ["2016-09-21T18:34:54Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1609.06670v1"], "summary": ["  This paper introduces a unified model of consistency and isolation that\nminimizes the gap between how these guarantees are defined and how they are\nperceived. Our approach is premised on a simple observation: applications view\nstorage systems as black-boxes that transition through a series of states, a\nsubset of which are observed by applications. For maximum clarity, isolation\nand consistency guarantees should be expressed as constraints on those states.\nInstead, these properties are currently expressed as constraints on operation\nhistories that are not visible to the application. We show that adopting a\nstate-based approach to expressing these guarantees brings forth several\nbenefits. First, it makes it easier to focus on the anomalies that a given\nisolation or consistency level allows (and that applications must deal with),\nrather than those that it proscribes. Second, it unifies the often disparate\ntheories of isolation and consistency and provides a structure for composing\nthese guarantees. We leverage this modularity to apply to transactions\n(independently of the isolation level under which they execute) the equivalence\nbetween causal consistency and session guarantees that Chockler et al. had\nproved for single operations. Third, it brings clarity to the increasingly\ncrowded field of proposed consistency and isolation properties by winnowing\nspurious distinctions: we find that the recently proposed parallel snapshot\nisolation introduced by Sovran et al. is in fact a specific implementation of\nan older guarantee, lazy consistency (or PL-2+), introduced by Adya et al.\n"]},
{"authors": ["Ladan Golshanara", "Jan Chomicki", "Wang-Chiew Tan"], "title": ["Temporal Data Exchange"], "date": ["2016-09-21T12:32:06Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1609.06526v1"], "summary": ["  Data exchange is the problem of transforming data that is structured under\nthe source schema into data structured under another schema, called the target\nschema, so that both source and target data satisfy the relationship between\nthe schemas. Many applications such as planning, scheduling, medical and fraud\ndetection systems, require data exchange in the context of temporal data. Even\nthough the formal framework of data exchange for relational database systems is\nwell-established, it does not immediately carry over to the setting of temporal\ndata, which necessitates reasoning over unbounded periods of time. In this\nwork, we study data exchange for temporal data. We first motivate the need for\ntwo views of temporal data: the concrete view, which depicts how temporal data\nis compactly represented and on which implementations are based, and the\nabstract view, which defines the semantics of temporal data. We show how the\nframework of data exchange can be systematically extended to temporal data. The\ncore of our framework consists of two new chase algorithms: the abstract chase\nover an abstract temporal instance and the concrete chase over a concrete\ntemporal instance. We show that although the two chase procedures operate over\nfundamentally different views of temporal data, the result of the concrete\nchase is semantically aligned with the result of the abstract chase. To obtain\nthe semantic alignment, the nulls (which are introduced by data exchange and\nmodel incompleteness) in both the concrete view and the abstract view are\nannotated with temporal information. Furthermore, we show that the result of\nthe concrete chase provides a foundation for query answering. We define na\\\"ive\nevaluation on the result of the concrete chase and show it produces certain\nanswers.\n"]},
{"authors": ["Janani Balaji", "Faizan Javed", "Mayank Kejriwal", "Chris Min", "Sam Sander", "Ozgur Ozturk"], "title": ["An Ensemble Blocking Scheme for Entity Resolution of Large and Sparse\n  Datasets"], "date": ["2016-09-20T17:44:28Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1609.06265v2"], "summary": ["  Entity Resolution, also called record linkage or deduplication, refers to the\nprocess of identifying and merging duplicate versions of the same entity into a\nunified representation. The standard practice is to use a Rule based or Machine\nLearning based model that compares entity pairs and assigns a score to\nrepresent the pairs' Match/Non-Match status. However, performing an exhaustive\npair-wise comparison on all pairs of records leads to quadratic matcher\ncomplexity and hence a Blocking step is performed before the Matching to group\nsimilar entities into smaller blocks that the matcher can then examine\nexhaustively. Several blocking schemes have been developed to efficiently and\neffectively block the input dataset into manageable groups. At CareerBuilder\n(CB), we perform deduplication on massive datasets of people profiles collected\nfrom disparate sources with varying informational content. We observed that,\nemploying a single blocking technique did not cover the base for all possible\nscenarios due to the multi-faceted nature of our data sources. In this paper,\nwe describe our ensemble approach to blocking that combines two different\nblocking techniques to leverage their respective strengths.\n"]},
{"authors": ["Maximilian Dylla", "Martin Theobald"], "title": ["Learning Tuple Probabilities"], "date": ["2016-09-16T15:16:25Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1609.05103v2"], "summary": ["  Learning the parameters of complex probabilistic-relational models from\nlabeled training data is a standard technique in machine learning, which has\nbeen intensively studied in the subfield of Statistical Relational Learning\n(SRL), but---so far---this is still an under-investigated topic in the context\nof Probabilistic Databases (PDBs). In this paper, we focus on learning the\nprobability values of base tuples in a PDB from labeled lineage formulas. The\nresulting learning problem can be viewed as the inverse problem to confidence\ncomputations in PDBs: given a set of labeled query answers, learn the\nprobability values of the base tuples, such that the marginal probabilities of\nthe query answers again yield in the assigned probability labels. We analyze\nthe learning problem from a theoretical perspective, cast it into an\noptimization problem, and provide an algorithm based on stochastic gradient\ndescent. Finally, we conclude by an experimental evaluation on three real-world\nand one synthetic dataset, thus comparing our approach to various techniques\nfrom SRL, reasoning in information extraction, and optimization.\n"]},
{"authors": ["Lu\u00eds Cruz-Filipe", "Gra\u00e7a Gaspar", "Isabel Nunes", "Peter Schneider-Kamp"], "title": ["Active Integrity Constraints for Multi-Context Systems"], "date": ["2016-09-20T05:03:58Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1609.06019v1"], "summary": ["  We introduce a formalism to couple integrity constraints over general-purpose\nknowledge bases with actions that can be executed to restore consistency. This\nformalism generalizes active integrity constraints over databases. In the more\ngeneral setting of multi-context systems, adding repair suggestions to\nintegrity constraints allows defining simple iterative algorithms to find all\npossible grounded repairs - repairs for the global system that follow the\nsuggestions given by the actions in the individual rules. We apply our\nmethodology to ontologies, and show that it can express most relevant types of\nintegrity constraints in this domain.\n"]},
{"authors": ["Jose A. Garc\u00eda Guti\u00e9rrez"], "title": ["Applications of Data Mining (DM) in Science and Engineering: State of\n  the art and perspectives"], "date": ["2016-09-17T22:22:17Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1609.05401v1"], "summary": ["  The continuous increase in the availability of data of any kind, coupled with\nthe development of networks of high-speed communications, the popularization of\ncloud computing and the growth of data centers and the emergence of\nhigh-performance computing does essential the task to develop techniques that\nallow more efficient data processing and analyzing of large volumes datasets\nand extraction of valuable information. In the following pages we will discuss\nabout development of this field in recent decades, and its potential and\napplicability present in the various branches of scientific research. Also, we\ntry to review briefly the different families of algorithms that are included in\ndata mining research area, its scalability with increasing dimensionality of\nthe input data and how they can be addressed and what behavior different\nmethods in a scenario in which the information is distributed or decentralized\nprocessed so as to increment performance optimization in heterogeneous\nenvironments.\n"]},
{"authors": ["Praveen Rao", "Anas Katib", "Daniel E. Lopez Barron"], "title": ["A Knowledge Ecosystem for the Food, Energy, and Water System"], "date": ["2016-09-17T16:27:38Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1609.05359v1"], "summary": ["  Food, energy, and water (FEW) are key resources to sustain human life and\neconomic growth. There is an increasing stress on these interconnected\nresources due to population growth, natural disasters, and human activities.\nNew research is necessary to foster more efficient, more secure, and safer use\nof FEW resources in the U.S. and globally. In this position paper, we present\nthe idea of a knowledge ecosystem for enabling the semantic data integration of\nheterogeneous datasets in the FEW system to promote knowledge discovery and\nsuperior decision making through semantic reasoning. Rich, diverse datasets\npublished by U.S. federal agencies will be utilized. Our knowledge ecosystem\nwill build on Semantic Web technologies and advances in statistical relational\nlearning to (a) represent, integrate, and harmonize diverse data sources and\n(b) perform ontology-based reasoning to discover actionable insights from FEW\ndatasets.\n"]},
{"authors": ["Yongchao Tian", "Pietro Michiardi", "Marko Vukolic"], "title": ["Bleach: A Distributed Stream Data Cleaning System"], "date": ["2016-09-16T15:52:44Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1609.05113v1"], "summary": ["  In this paper we address the problem of rule-based stream data cleaning,\nwhich sets stringent requirements on latency, rule dynamics and ability to cope\nwith the unbounded nature of data streams.\n  We design a system, called Bleach, which achieves real-time violation\ndetection and data repair on a dirty data stream. Bleach relies on efficient,\ncompact and distributed data structures to maintain the necessary state to\nrepair data, using an incremental version of the equivalence class algorithm.\nAdditionally, it supports rule dynamics and uses a \"cumulative\" sliding window\noperation to improve cleaning accuracy.\n  We evaluate a prototype of Bleach using a TPC-DS derived dirty data stream\nand observe its high throughput, low latency and high cleaning accuracy, even\nwith rule dynamics. Experimental results indicate superior performance of\nBleach compared to a baseline system built on the micro-batch streaming\nparadigm.\n"]},
{"authors": ["Yongchao Tian", "Ioannis Alagiannis", "Erietta Liarou", "Anastasia Ailamaki", "Pietro Michiardi", "Marko Vukolic"], "title": ["DiNoDB: an Interactive-speed Query Engine for Ad-hoc Queries on\n  Temporary Data"], "date": ["2016-09-16T14:56:31Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1609.05096v1"], "summary": ["  As data sets grow in size, analytics applications struggle to get instant\ninsight into large datasets. Modern applications involve heavy batch processing\njobs over large volumes of data and at the same time require efficient ad-hoc\ninteractive analytics on temporary data. Existing solutions, however, typically\nfocus on one of these two aspects, largely ignoring the need for synergy\nbetween the two. Consequently, interactive queries need to re-iterate costly\npasses through the entire dataset (e.g., data loading) that may provide\nmeaningful return on investment only when data is queried over a long period of\ntime. In this paper, we propose DiNoDB, an interactive-speed query engine for\nad-hoc queries on temporary data. DiNoDB avoids the expensive loading and\ntransformation phase that characterizes both traditional RDBMSs and current\ninteractive analytics solutions. It is tailored to modern workflows found in\nmachine learning and data exploration use cases, which often involve iterations\nof cycles of batch and interactive analytics on data that is typically useful\nfor a narrow processing window. The key innovation of DiNoDB is to piggyback on\nthe batch processing phase the creation of metadata that DiNoDB exploits to\nexpedite the interactive queries. Our experimental analysis demonstrates that\nDiNoDB achieves very good performance for a wide range of ad-hoc queries\ncompared to alternatives %such as Hive, Stado, SparkSQL and Impala.\n"]},
{"authors": ["Bart Kuijpers", "Alejandro Vaisman"], "title": ["A Formal Algebra for OLAP"], "date": ["2016-09-16T12:17:34Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1609.05020v1"], "summary": ["  Online Analytical Processing (OLAP) comprises tools and algorithms that allow\nquerying multidimensional databases. It is based on the multidimensional model,\nwhere data can be seen as a cube, where each cell contains one or more measures\ncan be aggregated along dimensions. Despite the extensive corpus of work in the\nfield, a standard language for OLAP is still needed, since there is no\nwell-defined, accepted semantics, for many of the usual OLAP operations. In\nthis paper, we address this problem, and present a set of operations for\nmanipulating a data cube. We clearly define the semantics of these operations,\nand prove that they can be composed, yielding a language powerful enough to\nexpress complex OLAP queries. We express these operations as a sequence of\natomic transformations over a fixed multidimensional matrix, whose cells\ncontain a sequence of measures. Each atomic transformation produces a new\nmeasure. When a sequence of transformations defines an OLAP operation, a flag\nis produced indicating which cells must be considered as input for the next\noperation. In this way, an elegant algebra is defined. Our main contribution,\nwith respect to other similar efforts in the field is that, for the first time,\na formal proof of the correctness of the operations is given, thus providing a\nclear semantics for them. We believe the present work will serve as a basis to\nbuild more solid practical tools for data analysis.\n"]},
{"authors": ["Sepehr Eghbali", "Ladan Tahvildari"], "title": ["Cosine Similarity Search with Multi Index Hashing"], "date": ["2016-09-14T23:16:37Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1610.00574v1"], "summary": ["  Due to rapid development of the Internet, recent years have witnessed an\nexplosion in the rate of data generation. Dealing with data at current scales\nbrings up unprecedented challenges. From the algorithmic view point, executing\nexisting linear algorithms in information retrieval and machine learning on\nsuch tremendous amounts of data incur intolerable computational and storage\ncosts. To address this issue, there is a growing interest to map data points in\nlarge-scale datasets to binary codes. This can significantly reduce the storage\ncomplexity of large-scale datasets. However, one of the most compelling reasons\nfor using binary codes or any discrete representation is that they can be used\nas direct indices into a hash table. Incorporating hash table offers fast query\nexecution; one can look up the nearby buckets in a hash table populated with\nbinary codes to retrieve similar items. Nonetheless, if binary codes are\ncompared in terms of the cosine similarity rather than the Hamming distance,\nthere is no fast exact sequential procedure to find the $K$ closest items to\nthe query other than the exhaustive search. Given a large dataset of binary\ncodes and a binary query, the problem that we address is to efficiently find\n$K$ closest codes in the dataset that yield the largest cosine similarities to\nthe query. To handle this issue, we first elaborate on the relation between the\nHamming distance and the cosine similarity. This allows finding the sequence of\nbuckets to check in the hash table. Having this sequence, we propose a\nmulti-index hashing approach that can increase the search speed up to orders of\nmagnitude in comparison to the exhaustive search and even approximation methods\nsuch as LSH. We empirically evaluate the performance of the proposed algorithm\non real world datasets.\n"]},
{"authors": ["Hugo Gualdron", "Robson Cordeiro", "Jose Rodrigues-Jr", "Duen Chau", "Minsuk Kahng", "U Kang"], "title": ["M-Flash: Fast Billion-scale Graph Computation Using a Bimodal Block\n  Processing Model"], "date": ["2015-06-03T20:56:30Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1506.01406v5"], "summary": ["  Recent graph computation approaches have demonstrated that a single PC can\nperform efficiently on billion-scale graphs. While these approaches achieve\nscalability by optimizing I/O operations, they do not fully exploit the\ncapabilities of modern hard drives and processors. To overcome their\nperformance, in this work, we introduce the Bimodal Block Processing (BBP), an\ninnovation that is able to boost the graph computation by minimizing the I/O\ncost even further. With this strategy, we achieved the following contributions:\n(1) M-Flash, the fastest graph computation framework to date; (2) a flexible\nand simple programming model to easily implement popular and essential graph\nalgorithms, including the first single-machine billion-scale eigensolver; and\n(3) extensive experiments on real graphs with up to 6.6 billion edges,\ndemonstrating M-Flash's consistent and significant speedup.\n"]},
{"authors": ["J\u00f6rn Hees", "Rouven Bauer", "Joachim Folz", "Damian Borth", "Andreas Dengel"], "title": ["An Evolutionary Algorithm to Learn SPARQL Queries for\n  Source-Target-Pairs: Finding Patterns for Human Associations in DBpedia"], "date": ["2016-07-25T12:47:38Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1607.07249v3"], "summary": ["  Efficient usage of the knowledge provided by the Linked Data community is\noften hindered by the need for domain experts to formulate the right SPARQL\nqueries to answer questions. For new questions they have to decide which\ndatasets are suitable and in which terminology and modelling style to phrase\nthe SPARQL query.\n  In this work we present an evolutionary algorithm to help with this\nchallenging task. Given a training list of source-target node-pair examples our\nalgorithm can learn patterns (SPARQL queries) from a SPARQL endpoint. The\nlearned patterns can be visualised to form the basis for further investigation,\nor they can be used to predict target nodes for new source nodes.\n  Amongst others, we apply our algorithm to a dataset of several hundred human\nassociations (such as \"circle - square\") to find patterns for them in DBpedia.\nWe show the scalability of the algorithm by running it against a SPARQL\nendpoint loaded with > 7.9 billion triples. Further, we use the resulting\nSPARQL queries to mimic human associations with a Mean Average Precision (MAP)\nof 39.9 % and a Recall@10 of 63.9 %.\n"]},
{"authors": ["Babak Salimi", "Dan Suciu"], "title": ["ZaliQL: A SQL-Based Framework for Drawing Causal Inference from Big Data"], "date": ["2016-09-12T19:24:14Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1609.03540v2"], "summary": ["  Causal inference from observational data is a subject of active research and\ndevelopment in statistics and computer science. Many toolkits have been\ndeveloped for this purpose that depends on statistical software. However, these\ntoolkits do not scale to large datasets. In this paper we describe a suite of\ntechniques for expressing causal inference tasks from observational data in\nSQL. This suite supports the state-of-the-art methods for causal inference and\nrun at scale within a database engine. In addition, we introduce several\noptimization techniques that significantly speedup causal inference, both in\nthe online and offline setting. We evaluate the quality and performance of our\ntechniques by experiments of real datasets.\n"]},
{"authors": ["Xuhui Li"], "title": ["A Meaning-oriented Approach to Semantic Data Modeling"], "date": ["2016-09-12T11:16:35Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1609.03346v1"], "summary": ["  Semantic information is often represented as the entities and the\nrelationships among them with conventional semantic models. This approach is\nstraightforward but is not suitable for many posteriori requests in semantic\ndata modeling. In this paper, we propose a meaning-oriented approach to\nmodeling semantic data and establish a graph-based semantic data model. In this\napproach we use the meanings, i.e., the subjective views of the entities and\nrelationships, to describe the semantic information, and use the semantic\ngraphs containing the meaning nodes and the meta-meaning relations to specify\nthe taxonomy and the compound construction of the semantic concepts. We\ndemonstrate how this meaning-oriented approach can address many important\nsemantic representation issues, including dynamic specialization and natural\njoin.\n"]},
{"authors": ["Makoto Hamana", "Kazutaka Matsuda", "Kazuyuki Asada"], "title": ["The Algebra of Recursive Graph Transformation Language UnCAL: Complete\n  Axiomatisation and Iteration Categorical Semantics"], "date": ["2015-11-27T23:26:05Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1511.08851v2"], "summary": ["  The aim of this paper is to provide mathematical foundations of a graph\ntransformation language, called UnCAL, using categorical semantics of type\ntheory and fixed points. About twenty years ago, Buneman et al. developed a\ngraph database query language UnQL on the top of a functional meta-language\nUnCAL for describing and manipulating graphs. Recently, the functional\nprogramming community has shown renewed interest in UnCAL, because it provides\nan efficient graph transformation language which is useful for various\napplications, such as bidirectional computation.\n  In order to make UnCAL more flexible and fruitful for further extensions and\napplications, in this paper, we give a more conceptual understanding of UnCAL\nusing categorical semantics. Our general interest of this paper is to clarify\nwhat is the algebra of UnCAL. Thus, we give an equational axiomatisation and\ncategorical semantics of UnCAL, both of which are new. We show that the\naxiomatisation is complete for the original bisimulation semantics of UnCAL.\nMoreover, we provide a clean characterisation of the computation mechanism of\nUnCAL called \"structural recursion on graphs\" using our categorical semantics.\nWe show a concrete model of UnCAL given by the lambdaG-calculus, which shows an\ninteresting connection to lazy functional programming.\n"]},
{"authors": ["Ali Ben Ammar"], "title": ["Query Optimization Techniques In Graph Databases"], "date": ["2016-09-07T09:08:28Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1609.01893v1"], "summary": ["  Graph databases (GDB) have recently been arisen to overcome the limits of\ntraditional databases for storing and managing data with graph-like structure.\nToday, they represent a requirement for many applications that manage\ngraph-like data, like social networks. Most of the techniques, applied to\noptimize queries in graph databases, have been used in traditional databases,\ndistribution systems... or they are inspired from graph theory. However, their\nreuse in graph databases should take care of the main characteristics of graph\ndatabases, such as dynamic structure, highly interconnected data, and ability\nto efficiently access data relationships. In this paper, we survey the query\noptimization techniques in graph databases. In particular, we focus on the\nfeatures they have introduced to improve querying graph-like data.\n"]},
{"authors": ["Nhien-An Le-Khac", "Sammer Markos", "Tahar Kechadi"], "title": ["A data mining-based solution for detecting suspicious money laundering\n  cases in an investment bank"], "date": ["2016-09-04T21:03:32Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1609.00990v1"], "summary": ["  Today, money laundering poses a serious threat not only to financial\ninstitutions but also to the nation. This criminal activity is becoming more\nand more sophisticated and seems to have moved from the clichy of drug\ntrafficking to financing terrorism and surely not forgetting personal gain.\nMost international financial institutions have been implementing anti-money\nlaundering solutions to fight investment fraud. However, traditional\ninvestigative techniques consume numerous man-hours. Recently, data mining\napproaches have been developed and are considered as well-suited techniques for\ndetecting money laundering activities. Within the scope of a collaboration\nproject for the purpose of developing a new solution for the anti-money\nlaundering Units in an international investment bank, we proposed a simple and\nefficient data mining-based solution for anti-money laundering. In this paper,\nwe present this solution developed as a tool and show some preliminary\nexperiment results with real transaction datasets.\n"]},
{"authors": ["Ruochen Jiang", "Jiannan Wang"], "title": ["Reprowd: Crowdsourced Data Processing Made Reproducible"], "date": ["2016-09-03T04:35:57Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1609.00791v1"], "summary": ["  Crowdsourcing is a multidisciplinary research area including disciplines like\nartificial intelligence, human-computer interaction, database, and social\nscience. To facilitate cooperation across disciplines, reproducibility is a\ncrucial factor, but unfortunately, it has not gotten enough attention in the\nHCOMP community. In this paper, we present Reprowd, a system aiming to make it\neasy to reproduce crowdsourced data processing research. We have open sourced\nReprowd at http://sfu-db.github.io/reprowd/.\n"]},
{"authors": ["Katsumi Kumai", "Yuhki Shiraishi", "Jianwei Zhang", "Hiroyuki Kitagawa", "Atsuyuki Morishima"], "title": ["Group Rotation Type Crowdsourcing"], "date": ["2016-09-01T05:55:02Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1609.00117v1"], "summary": ["  A common workflow to perform a continuous human task stream is to divide\nworkers into groups, have one group perform the newly-arrived task, and rotate\nthe groups. We call this type of workflow the group rotation. This paper\naddresses the problem of how to manage Group Rotation Type Crowdsourcing, the\ngroup rotation in a crowdsourcing setting. In the group-rotation type\ncrowdsourcing, we must change the group structure dynamically because workers\ncome in and leave frequently. This paper proposes an approach to explore a\ndesign space of methods for group restructuring in the group rotation type\ncrowdsourcing.\n"]},
{"authors": ["Olaf Hartig", "Carlos Buil-Aranda"], "title": ["brTPF: Bindings-Restricted Triple Pattern Fragments (Extended Preprint)"], "date": ["2016-08-29T17:09:53Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1608.08148v2"], "summary": ["  The Triple Pattern Fragment (TPF) interface is a recent proposal for reducing\nserver load in Web-based approaches to execute SPARQL queries over public RDF\ndatasets. The price for less overloaded servers is a higher client-side load\nand a substantial increase in network load (in terms of both the number of HTTP\nrequests and data transfer). In this paper, we propose a slightly extended\ninterface that allows clients to attach intermediate results to triple pattern\nrequests. The response to such a request is expected to contain triples from\nthe underlying dataset that do not only match the given triple pattern (as in\nthe case of TPF), but that are guaranteed to contribute in a join with the\ngiven intermediate result. Our hypothesis is that a distributed query execution\nusing this extended interface can reduce the network load (in comparison to a\npure TPF-based query execution) without reducing the overall throughput of the\nclient-server system significantly. Our main contribution in this paper is\ntwofold: we empirically verify the hypothesis and provide an extensive\nexperimental comparison of our proposal and TPF.\n"]},
{"authors": ["Bahareh Sadat Arab", "Dieter Gawlick", "Vasudha Krishnaswamy", "Venkatesh Radhakrishnan", "Boris Glavic"], "title": ["Reenactment for Read-Committed Snapshot Isolation"], "date": ["2016-08-29T21:37:43Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1608.08258v1"], "summary": ["  Provenance for transactional updates is critical for many applications such\nas auditing and debugging of transactions. Recently, we have introduced\nMV-semirings, an extension of the semiring provenance model that supports\nupdates and transactions. Furthermore, we have proposed reenactment, a\ndeclarative form of replay with provenance capture, as an efficient and\nnon-invasive method for computing this type of provenance. However, this\napproach is limited to the snapshot isolation (SI) concurrency control protocol\nwhile many real world applications apply the read committed version of snapshot\nisolation (RC-SI) to improve performance at the cost of consistency. We present\nnon-trivial extensions of the model and reenactment approach to be able to\ncompute provenance of RC-SI transactions efficiently. In addition, we develop\ntechniques for applying reenactment across multiple RC-SI transactions. Our\nexperiments demonstrate that our implementation in the GProM system supports\nefficient re-construction and querying of provenance.\n"]},
{"authors": ["Hoang Nguyen", "Marlon Dumas", "Marcello La Rosa", "Fabrizio Maria Maggi", "Suriadi Suriadi"], "title": ["Business Process Deviance Mining: Review and Evaluation"], "date": ["2016-08-29T21:14:01Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1608.08252v1"], "summary": ["  Business process deviance refers to the phenomenon whereby a subset of the\nexecutions of a business process deviate, in a negative or positive way, with\nrespect to its expected or desirable outcomes. Deviant executions of a business\nprocess include those that violate compliance rules, or executions that\nundershoot or exceed performance targets. Deviance mining is concerned with\nuncovering the reasons for deviant executions by analyzing business process\nevent logs. This article provides a systematic review and comparative\nevaluation of deviance mining approaches based on a family of data mining\ntechniques known as sequence classification. Using real-life logs from multiple\ndomains, we evaluate a range of feature types and classification methods in\nterms of their ability to accurately discriminate between normal and deviant\nexecutions of a process. We also analyze the interestingness of the rule sets\nextracted using different methods. We observe that feature sets extracted using\npattern mining techniques only slightly outperform simpler feature sets based\non counts of individual activity occurrences in a trace.\n"]},
{"authors": ["Magnus Knuth", "Olaf Hartig", "Harald Sack"], "title": ["Scheduling Refresh Queries for Keeping Results from a SPARQL Endpoint\n  Up-to-Date (Extended Version)"], "date": ["2016-08-29T16:16:36Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1608.08130v1"], "summary": ["  Many datasets change over time. As a consequence, long-running applications\nthat cache and repeatedly use query results obtained from a SPARQL endpoint may\nresubmit the queries regularly to ensure up-to-dateness of the results. While\nthis approach may be feasible if the number of such regular refresh queries is\nmanageable, with an increasing number of applications adopting this approach,\nthe SPARQL endpoint may become overloaded with such refresh queries. A more\nscalable approach would be to use a middle-ware component at which the\napplications register their queries and get notified with updated query results\nonce the results have changed. Then, this middle-ware can schedule the repeated\nexecution of the refresh queries without overloading the endpoint. In this\npaper, we study the problem of scheduling refresh queries for a large number of\nregistered queries by assuming an overload-avoiding upper bound on the length\nof a regular time slot available for testing refresh queries. We investigate a\nvariety of scheduling strategies and compare them experimentally in terms of\ntime slots needed before they recognize changes and number of changes that they\nmiss.\n"]},
{"authors": ["Timo Schindler", "Christoph Skornia"], "title": ["Secure Parallel Processing of Big Data Using Order-Preserving Encryption\n  on Google BigQuery"], "date": ["2016-08-29T10:18:01Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1608.07981v1"], "summary": ["  With the increase of centralization of resources in IT-infrastructure and the\ngrowing amount of cloud services, database management systems (DBMS) will be\nmore and more outsourced to Infrastructure-as-a-Service (IaaS) providers. The\noutsourcing of entire databases, or the computation power for processing Big\nData to an external provider also means that the provider has full access to\nthe information contained in the database. In this article we propose a\nfeasible solution with Order-Preserving Encryption (OPE) and further, state of\nthe art, encryption methods to sort and process Big Data on external resources\nwithout exposing the unencrypted data to the IaaS provider. We also introduce a\nproof-of-concept client for Google BigQuery as example IaaS Provider.\n"]},
{"authors": ["Jason Lowe-Power", "Mark D. Hill", "David A. Wood"], "title": ["When to use 3D Die-Stacked Memory for Bandwidth-Constrained Big Data\n  Workloads"], "date": ["2016-08-26T15:04:20Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1608.07485v1"], "summary": ["  Response time requirements for big data processing systems are shrinking. To\nmeet this strict response time requirement, many big data systems store all or\nmost of their data in main memory to reduce the access latency. Main memory\ncapacities have grown, and systems with 2 TB of main memory capacity available\ntoday. However, the rate at which processors can access this data--the memory\nbandwidth--has not grown at the same rate. In fact, some of these big-memory\nsystems can access less than 10% of their main memory capacity in one second\n(billions of processor cycles).\n  3D die-stacking is one promising solution to this bandwidth problem, and\nindustry is investing significantly in 3D die-stacking. We use a simple\nback-of-the-envelope-style model to characterize if and when the 3D die-stacked\narchitecture is more cost-effective than current architectures for in-memory\nbig data workloads. We find that die-stacking has much higher performance than\ncurrent systems (up to 256x lower response times), and it does not require\nexpensive memory over provisioning to meet real-time (10 ms) response time\nservice-level agreements. However, the power requirements of the die-stacked\nsystems are significantly higher (up to 50x) than current systems, and its\nmemory capacity is lower in many cases. Even in this limited case study, we\nfind 3D die-stacking is not a panacea. Today, die-stacking is the most\ncost-effective solution for strict SLAs and by reducing the power of the\ncompute chip and increasing memory densities die-stacking can be cost-effective\nunder other constraints in the future.\n"]},
{"authors": ["Jaroslaw Szlichta", "Parke Godfrey", "Lukasz Golab", "Mehdi Kargar", "Divesh Srivastava"], "title": ["Effective and Complete Discovery of Order Dependencies via Set-based\n  Axiomatization"], "date": ["2016-08-22T14:03:46Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1608.06169v2"], "summary": ["  Integrity constraints (ICs) provide a valuable tool for expressing and\nenforcing application semantics. However, formulating constraints manually\nrequires domain expertise, is prone to human errors, and may be excessively\ntime consuming, especially on large datasets. Hence, proposals for automatic\ndiscovery have been made for some classes of ICs, such as functional\ndependencies (FDs), and recently, order dependencies (ODs). ODs properly\nsubsume FDs, as they can additionally express business rules involving order;\ne.g., an employee never has a higher salary while paying lower taxes compared\nwith another employee.\n  We address the limitations of prior work on OD discovery which has factorial\ncomplexity in the number of attributes, is incomplete (i.e., it does not\ndiscover valid ODs that cannot be inferred from the ones found) and is not\nconcise (i.e., it can result in \"redundant\" discovery and overly large\ndiscovery sets). We improve significantly on complexity, offer completeness,\nand define a compact canonical form. This is based on a novel polynomial\nmapping to a canonical form for ODs, and a sound and complete set of axioms\n(inference rules) for canonical ODs. This allows us to develop an efficient\nset-containment, lattice-driven OD discovery algorithm that uses the inference\nrules to prune the search space. Our algorithm has exponential worst-case time\ncomplexity in the number of attributes and linear complexity in the number of\ntuples. We prove that it produces a complete, minimal set of ODs (i.e., minimal\nwith regards to the canonical representation). Finally, using real and\nsynthetic datasets, we experimentally show orders-of-magnitude performance\nimprovements over the current state-of-the-art algorithm and demonstrate\neffectiveness of our techniques.\n"]},
{"authors": ["Stacey Jeffery", "Fran\u00e7ois Le Gall"], "title": ["Quantum Communication Complexity of Distributed Set Joins"], "date": ["2016-08-23T19:45:38Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1608.06617v1"], "summary": ["  Computing set joins of two inputs is a common task in database theory.\nRecently, Van Gucht, Williams, Woodruff and Zhang [PODS 2015] considered the\ncomplexity of such problems in the natural model of (classical) two-party\ncommunication complexity and obtained tight bounds for the complexity of\nseveral important distributed set joins.\n  In this paper we initiate the study of the *quantum* communication complexity\nof distributed set joins. We design a quantum protocol for distributed Boolean\nmatrix multiplication, which corresponds to computing the composition join of\ntwo databases, showing that the product of two $n\\times n$ Boolean matrices,\neach owned by one of two respective parties, can be computed with\n$\\widetilde{O}(\\sqrt{n}\\ell^{3/4})$ qubits of communication, where $\\ell$\ndenotes the number of non-zero entries of the product. Since Van Gucht et al.\nshowed that the classical communication complexity of this problem is\n$\\widetilde{\\Theta}(n\\sqrt{\\ell})$, our quantum algorithm outperforms classical\nprotocols whenever the output matrix is sparse. We also show a quantum lower\nbound and a matching classical upper bound on the communication complexity of\ndistributed matrix multiplication over $\\mathbb{F}_2$.\n  Besides their applications to database theory, the communication complexity\nof set joins is interesting due to its connections to direct product theorems\nin communication complexity. In this work we also introduce a notion of\n*all-pairs* product theorem, and relate this notion to standard direct product\ntheorems in communication complexity.\n"]},
{"authors": ["Ayb\u00fck\u00eb Ozt\u00fcrk", "Louis Eyango", "Sylvie Yona Waksman", "St\u00e9phane Lallich", "J\u00e9r\u00f4me Darmont"], "title": ["Warehousing Complex Archaeological Objects"], "date": ["2016-08-23T11:30:48Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1608.06469v1"], "summary": ["  Data organization is a difficult and essential component in cultural heritage\napplications. Over the years, a great amount of archaeological ceramic data\nhave been created and processed by various methods and devices. Such ceramic\ndata are stored in databases that concur to increase the amount of available\ninformation rapidly. However , such databases typically focus on one type of\nceramic descriptors, e.g., qualitative textual descriptions, petrographic or\nchemical analysis results, and do not interoperate. Thus, research involving\narchaeological ceramics cannot easily take advantage of combining all these\ntypes of information. In this application paper, we introduce an evolution of\nthe Ceramom database that includes text descriptors of archaeological features,\nchemical analysis results, and various images, including petrographic and\nfabric images. To illustrate what new analyses are permitted by such a\ndatabase, we source it to a data warehouse and present a sample on-line\nanalysis processing (OLAP) scenario to gain deep understanding of ceramic\ncontext.\n"]},
{"authors": ["Qin Liu", "Zhenguo Li", "John C. S. Lui", "Jiefeng Cheng"], "title": ["PowerWalk: Scalable Personalized PageRank via Random Walks with\n  Vertex-Centric Decomposition"], "date": ["2016-08-22T05:31:58Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1608.06054v1"], "summary": ["  Most methods for Personalized PageRank (PPR) precompute and store all\naccurate PPR vectors, and at query time, return the ones of interest directly.\nHowever, the storage and computation of all accurate PPR vectors can be\nprohibitive for large graphs, especially in caching them in memory for\nreal-time online querying. In this paper, we propose a distributed framework\nthat strikes a better balance between offline indexing and online querying. The\noffline indexing attains a fingerprint of the PPR vector of each vertex by\nperforming billions of \"short\" random walks in parallel across a cluster of\nmachines. We prove that our indexing method has an exponential convergence,\nachieving the same precision with previous methods using a much smaller number\nof random walks. At query time, the new PPR vector is composed by a linear\ncombination of related fingerprints, in a highly efficient vertex-centric\ndecomposition manner. Interestingly, the resulting PPR vector is much more\naccurate than its offline counterpart because it actually uses more random\nwalks in its estimation. More importantly, we show that such decomposition for\na batch of queries can be very efficiently processed using a shared\ndecomposition. Our implementation, PowerWalk, takes advantage of advanced\ndistributed graph engines and it outperforms the state-of-the-art algorithms by\norders of magnitude. Particularly, it responses to tens of thousands of queries\non graphs with billions of edges in just a few seconds.\n"]},
{"authors": ["Minsuk Kahng", "Shamkant B. Navathe", "John T. Stasko", "Duen Horng Chau"], "title": ["Interactive Browsing and Navigation in Relational Databases"], "date": ["2016-03-08T03:41:34Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1603.02371v2"], "summary": ["  Although researchers have devoted considerable attention to helping database\nusers formulate queries, many users still find it challenging to specify\nqueries that involve joining tables. To help users construct join queries for\nexploring relational databases, we propose ETable, a novel presentation data\nmodel that provides users with a presentation-level interactive view. This view\ncompactly presents one-to-many and many-to-many relationships within a single\nenriched table by allowing a cell to contain a set of entity references. Users\ncan directly interact with this enriched table to incrementally construct\ncomplex queries and navigate databases on a conceptual entity-relationship\nlevel. In a user study, participants performed a range of database querying\ntasks faster with ETable than with a commercial graphical query builder.\nSubjective feedback about ETable was also positive. All participants found that\nETable was easier to learn and helpful for exploring databases.\n"]},
{"authors": ["Kayhan Dursun", "Carsten Binnig", "Ugur Cetintemel", "Tim Kraska"], "title": ["Revisiting Reuse in Main Memory Database Systems"], "date": ["2016-08-19T17:36:12Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1608.05678v1"], "summary": ["  Reusing intermediates in databases to speed-up analytical query processing\nhas been studied in the past. Existing solutions typically require intermediate\nresults of individual operators to be materialized into temporary tables to be\nconsidered for reuse in subsequent queries. However, these approaches are\nfundamentally ill-suited for use in modern main memory databases. The reason is\nthat modern main memory DBMSs are typically limited by the bandwidth of the\nmemory bus, thus query execution is heavily optimized to keep tuples in the CPU\ncaches and registers. To that end, adding additional materialization operations\ninto a query plan not only add additional traffic to the memory bus but more\nimportantly prevent the important cache- and register-locality opportunities\nresulting in high performance penalties.\n  In this paper we study a novel reuse model for intermediates, which caches\ninternal physical data structures materialized during query processing (due to\npipeline breakers) and externalizes them so that they become reusable for\nupcoming operations. We focus on hash tables, the most commonly used internal\ndata structure in main memory databases to perform join and aggregation\noperations. As queries arrive, our reuse-aware optimizer reasons about the\nreuse opportunities for hash tables, employing cost models that take into\naccount hash table statistics together with the CPU and data movement costs\nwithin the cache hierarchy. Experimental results, based on our HashStash\nprototype demonstrate performance gains of $2\\times$ for typical analytical\nworkloads with no additional overhead for materializing intermediates.\n"]},
{"authors": ["Giacomo Bergami", "Matteo Magnani", "Danilo Montesi"], "title": ["On Joining Graphs"], "date": ["2016-08-19T13:09:04Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1608.05594v1"], "summary": ["  In the graph database literature the term \"join\" does not refer to an\noperator used to merge two graphs. In particular, a counterpart of the\nrelational join is not present in existing graph query languages, and\nconsequently no efficient algorithms have been developed for this operator.\n  This paper provides two main contributions. First, we define a binary graph\njoin operator that acts on the vertices as a standard relational join and\ncombines the edges according to a user-defined semantics. Then we propose the\n\"CoGrouped Graph Conjunctive $\\theta$-Join\" algorithm running over data indexed\nin secondary memory. Our implementation outperforms the execution of the same\noperation in Cypher and SPARQL on major existing graph database management\nsystems by at least one order of magnitude, also including indexing and loading\ntime.\n"]},
{"authors": ["Ninh Pham", "Rasmus Pagh"], "title": ["Scalability and Total Recall with Fast CoveringLSH"], "date": ["2016-02-08T16:03:11Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1602.02620v2"], "summary": ["  Locality-sensitive hashing (LSH) has emerged as the dominant algorithmic\ntechnique for similarity search with strong performance guarantees in\nhigh-dimensional spaces. A drawback of traditional LSH schemes is that they may\nhave \\emph{false negatives}, i.e., the recall is less than 100\\%. This limits\nthe applicability of LSH in settings requiring precise performance guarantees.\nBuilding on the recent theoretical \"CoveringLSH\" construction that eliminates\nfalse negatives, we propose a fast and practical covering LSH scheme for\nHamming space called \\emph{Fast CoveringLSH (fcLSH)}. Inheriting the design\nbenefits of CoveringLSH our method avoids false negatives and always reports\nall near neighbors. Compared to CoveringLSH we achieve an asymptotic\nimprovement to the hash function computation time from $\\mathcal{O}(dL)$ to\n$\\mathcal{O}(d + L\\log{L})$, where $d$ is the dimensionality of data and $L$ is\nthe number of hash tables. Our experiments on synthetic and real-world data\nsets demonstrate that \\emph{fcLSH} is comparable (and often superior) to\ntraditional hashing-based approaches for search radius up to 20 in\nhigh-dimensional Hamming space.\n"]},
{"authors": ["Nandish Jayaram", "Rohit Bhoopalam", "Chengkai Li", "Vassilis Athitsos"], "title": ["Orion: Enabling Suggestions in a Visual Query Builder for\n  Ultra-Heterogeneous Graphs"], "date": ["2016-05-22T21:29:04Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1605.06856v2"], "summary": ["  The database community has long recognized the importance of graphical query\ninterface to the usability of data management systems. Yet, relatively less has\nbeen done. We present Orion, a visual interface for querying\nultra-heterogeneous graphs. It iteratively assists users in query graph\nconstruction by making suggestions via machine learning methods. In its active\nmode, Orion automatically suggests top-k edges to be added to a query graph. In\nits passive mode, the user adds a new edge manually, and Orion suggests a\nranked list of labels for the edge. Orion's edge ranking algorithm, Random\nDecision Paths (RDP), makes use of a query log to rank candidate edges by how\nlikely they will match the user's query intent. Extensive user studies using\nFreebase demonstrated that Orion users have a 70% success rate in constructing\ncomplex query graphs, a significant improvement over the 58% success rate by\nthe users of a baseline system that resembles existing visual query builders.\nFurthermore, using active mode only, the RDP algorithm was compared with\nseveral methods adapting other machine learning algorithms such as random\nforests and naive Bayes classifier, as well as class association rules and\nrecommendation systems based on singular value decomposition. On average, RDP\nrequired 40 suggestions to correctly reach a target query graph (using only its\nactive mode of suggestion) while other methods required 1.5--4 times as many\nsuggestions.\n"]},
{"authors": ["Abdur Rafay"], "title": ["Multi Query Optimization in GLADE"], "date": ["2016-08-16T17:49:43Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1608.04686v1"], "summary": ["  SQL-on-Hadoop systems, query optimization, data distribution over multiple\nnodes and parallelization techniques are few of the areas under extreme\nresearch these days. Big names like Amazon, Google, Microsoft and many more are\nworking on implementing systems for faster access of data from multiple nodes\nreducing data mobility and increasing the parallelization. Queries are\nretrieved and reviewed by the database systems in an efficient way in the least\namount of time by the introduction of various parallelization techniques by\nrunning the same query in parallel over different nodes carrying the data.\nApart from multi-threading parallelization, there is another way of\nparallelization that can be performed in order to further reduce retrieval time\nhence improving the efficiency of the system; parallelization on user queries\non top of a DBMS/RDBMS. In this paper, we will study one such technique of how\nmultiple queries can run simultaneously on a system in order to increase the\nefficiency by reducing the data retrieval from the storage. Maximum sharing of\nworkload has been performed by generating optimal and ubiquitous join plans for\na set of queries and then fed them to GLADE (Generalized Linear Aggregate\nDistribution Engine), a scalable distributed system for large scale data\nanalytics. Our main work is centered on generating GLADE join plans for a\nMulti-Query satisfying maximum number of queries in order to maximize data\nsharing and minimize data retrieval for each individual query.\n"]},
{"authors": ["Mayank Kejriwal", "Daniel P. Miranker"], "title": ["Experience: Type alignment on DBpedia and Freebase"], "date": ["2016-08-15T23:56:08Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1608.04442v1"], "summary": ["  Linked Open Data exhibits growth in both volume and variety of published\ndata. Due to this variety, instances of many different types (e.g. Person) can\nbe found in published datasets. Type alignment is the problem of automatically\nmatching types (in a possibly many-many fashion) between two such datasets.\nType alignment is an important preprocessing step in instance matching.\nInstance matching concerns identifying pairs of instances referring to the same\nunderlying entity. By performing type alignment a priori, only instances\nconforming to aligned types are processed together, leading to significant\nsavings. This article describes a type alignment experience with two\nlarge-scale cross-domain RDF knowledge graphs, DBpedia and Freebase, that\ncontain hundreds, or even thousands, of unique types. Specifically, we present\na MapReduce-based type alignment algorithm and show that there are at least\nthree reasonable ways of evaluating type alignment within the larger context of\ninstance matching. We comment on the consistency of those results, and note\nsome general observations for researchers evaluating similar algorithms on\ncross-domain graphs.\n"]},
{"authors": ["Mayank Kejriwal", "Daniel P. Miranker"], "title": ["Self-contained NoSQL Resources for Cross-Domain RDF"], "date": ["2016-08-15T23:32:31Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1608.04437v1"], "summary": ["  Cross-domain knowledge bases such as DBpedia, Freebase and YAGO have emerged\nas encyclopedic hubs in the Web of Linked Data. Despite enabling several\npractical applications in the Semantic Web, the large-scale, schema-free nature\nof such graphs often precludes research groups from employing them widely as\nevaluation test cases for entity resolution and instance-based ontology\nalignment applications. Although the ground-truth linkages between the three\nknowledge bases above are available, they are not amenable to resource-limited\napplications. One reason is that the ground-truth files are not self-contained,\nmeaning that a researcher must usually perform a series of expensive joins\n(typically in MapReduce) to obtain usable information sets. In this paper, we\nupload several publicly licensed data resources to the public cloud and use\nsimple Hadoop clusters to compile, and make accessible, three cross-domain\nself-contained test cases involving linked instances from DBpedia, Freebase and\nYAGO. Self-containment is enabled by virtue of a simple NoSQL JSON-like\nserialization format. Potential applications for these resources, particularly\nrelated to testing transfer learning research hypotheses, are also briefly\ndescribed.\n"]},
{"authors": ["Evgeny Kharlamov", "Yannis Kotidis", "Theofilos Mailis", "Christian Neuenstadt", "Charalampos Nikolaou", "\u00d6zg\u00fcr \u00d6zcep", "Christoforos Svingos", "Dmitriy Zheleznyakov", "Sebastian Brandt", "Ian Horrocks", "Yannis Ioannidis", "Steffen Lamparter", "Ralf M\u00f6ller"], "title": ["Towards Analytics Aware Ontology Based Access to Static and Streaming\n  Data (Extended Version)"], "date": ["2016-07-18T23:23:21Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1607.05351v2"], "summary": ["  Real-time analytics that requires integration and aggregation of\nheterogeneous and distributed streaming and static data is a typical task in\nmany industrial scenarios such as diagnostics of turbines in Siemens. OBDA\napproach has a great potential to facilitate such tasks; however, it has a\nnumber of limitations in dealing with analytics that restrict its use in\nimportant industrial applications. Based on our experience with Siemens, we\nargue that in order to overcome those limitations OBDA should be extended and\nbecome analytics, source, and cost aware. In this work we propose such an\nextension. In particular, we propose an ontology, mapping, and query language\nfor OBDA, where aggregate and other analytical functions are first class\ncitizens. Moreover, we develop query optimisation techniques that allow to\nefficiently process analytical tasks over static and streaming data. We\nimplement our approach in a system and evaluate our system with Siemens turbine\ndata.\n"]},
{"authors": ["Leopoldo Bertossi", "Flavio Rizzolo"], "title": ["Contexts and Data Quality Assessment"], "date": ["2016-08-14T21:43:20Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1608.04142v1"], "summary": ["  The quality of data is context dependent. Starting from this intuition and\nexperience, we propose and develop a conceptual framework that captures in\nformal terms the notion of \"context-dependent data quality\". We start by\nproposing a generic and abstract notion of context, and also of its uses, in\ngeneral and in data management in particular. On this basis, we investigate\n\"data quality assessment\" and \"quality query answering\" as context-dependent\nactivities. A context for the assessment of a database D at hand is modeled as\nan external database schema, with possibly materialized or virtual data, and\nconnections to external data sources. The database D is put in context via\nmappings to the contextual schema, which produces a collection C of alternative\nclean versions of D. The quality of D is measured in terms of its distance to\nC. The class C} is also used to define and do \"quality query answering\". The\nproposed model allows for natural extensions, like the use of data quality\npredicates, the optimization of the access by the context to external data\nsources, and also the representation of contexts by means of more expressive\nontologies.\n"]},
{"authors": ["Hao Wu", "Maoyuan Sun", "Jilles Vreeken", "Nikolaj Tatti", "Chris North", "Naren Ramakrishnan"], "title": ["Interactive and Iterative Discovery of Entity Network Subgraphs"], "date": ["2016-08-12T19:56:14Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1608.03889v1"], "summary": ["  Graph mining to extract interesting components has been studied in various\nguises, e.g., communities, dense subgraphs, cliques. However, most existing\nworks are based on notions of frequency and connectivity and do not capture\nsubjective interestingness from a user's viewpoint. Furthermore, existing\napproaches to mine graphs are not interactive and cannot incorporate user\nfeedbacks in any natural manner. In this paper, we address these gaps by\nproposing a graph maximum entropy model to discover surprising connected\nsubgraph patterns from entity graphs. This model is embedded in an interactive\nvisualization framework to enable human-in-the-loop, model-guided data\nexploration. Using case studies on real datasets, we demonstrate how\ninteractions between users and the maximum entropy model lead to faster and\nexplainable conclusions.\n"]},
{"authors": ["Thomas Moyer", "Vijay Gadepally"], "title": ["High-throughput Ingest of Provenance Records into Accumulo"], "date": ["2016-08-12T12:56:46Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1608.03780v1"], "summary": ["  Whole-system data provenance provides deep insight into the processing of\ndata on a system, including detecting data integrity attacks. The downside to\nsystems that collect whole-system data provenance is the sheer volume of data\nthat is generated under many heavy workloads. In order to make provenance\nmetadata useful, it must be stored somewhere where it can be queried. This\nproblem becomes even more challenging when considering a network of\nprovenance-aware machines all collecting this metadata. In this paper, we\ninvestigate the use of D4M and Accumulo to support high-throughput data ingest\nof whole-system provenance data. We find that we are able to ingest 3,970 graph\ncomponents per second. Centrally storing the provenance metadata allows us to\nbuild systems that can detect and respond to data integrity attacks that are\ncaptured by the provenance system.\n"]},
{"authors": ["Nikos Bikakis", "Chrisa Tsinaraki", "Nektarios Gioldasis", "Ioannis Stavrakantonakis", "Stavros Christodoulakis"], "title": ["The XML and Semantic Web Worlds: Technologies, Interoperability and\n  Integration. A Survey of the State of the Art"], "date": ["2016-08-11T18:03:04Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1608.03556v1"], "summary": ["  In the context of the emergent Web of Data, a large number of organizations,\ninstitutes and companies (e.g., DBpedia, Geonames, PubMed ACM, IEEE, NASA, BBC)\nadopt the Linked Data practices and publish their data utilizing Semantic Web\n(SW) technologies. On the other hand, the dominant standard for information\nexchange in the Web today is XML. Many international standards (e.g., Dublin\nCore, MPEG-7, METS, TEI, IEEE LOM) have been expressed in XML Schema resulting\nto a large number of XML datasets. The SW and XML worlds and their developed\ninfrastructures are based on different data models, semantics and query\nlanguages. Thus, it is crucial to provide interoperability and integration\nmechanisms to bridge the gap between the SW and XML worlds. In this chapter, we\ngive an overview and a comparison of the technologies and the standards adopted\nby the XML and SW worlds. In addition, we outline the latest efforts from the\nW3C groups, including the latest working drafts and recommendations (e.g., OWL\n2, SPARQL 1.1, XML Schema 1.1). Moreover, we present a survey of the research\napproaches which aim to provide interoperability and integration between the\nXML and SW worlds. Finally, we present the SPARQL2XQuery and XS2OWL Frameworks,\nwhich bridge the gap and create an interoperable environment between the two\nworlds. These Frameworks provide mechanisms for: (a) Query translation (SPARQL\nto XQuery translation); (b) Mapping specification and generation (Ontology to\nXML Schema mapping); and (c) Schema transformation (XML Schema to OWL\ntransformation).\n"]},
{"authors": ["Dylan Hutchison", "Jeremy Kepner", "Vijay Gadepally", "Bill Howe"], "title": ["From NoSQL Accumulo to NewSQL Graphulo: Design and Utility of Graph\n  Algorithms inside a BigTable Database"], "date": ["2016-06-22T20:08:47Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1606.07085v2"], "summary": ["  Google BigTable's scale-out design for distributed key-value storage inspired\na generation of NoSQL databases. Recently the NewSQL paradigm emerged in\nresponse to analytic workloads that demand distributed computation local to\ndata storage. Many such analytics take the form of graph algorithms, a trend\nthat motivated the GraphBLAS initiative to standardize a set of matrix math\nkernels for building graph algorithms. In this article we show how it is\npossible to implement the GraphBLAS kernels in a BigTable database by\npresenting the design of Graphulo, a library for executing graph algorithms\ninside the Apache Accumulo database. We detail the Graphulo implementation of\ntwo graph algorithms and conduct experiments comparing their performance to two\nmain-memory matrix math systems. Our results shed insight into the conditions\nthat determine when executing a graph algorithm is faster inside a database\nversus an external system---in short, that memory requirements and relative I/O\nare critical factors.\n"]},
{"authors": ["Chenwei Zhang", "Sihong Xie", "Yaliang Li", "Jing Gao", "Wei Fan", "Philip S. Yu"], "title": ["Multi-source Hierarchical Prediction Consolidation"], "date": ["2016-08-11T01:55:04Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1608.03344v1"], "summary": ["  In big data applications such as healthcare data mining, due to privacy\nconcerns, it is necessary to collect predictions from multiple information\nsources for the same instance, with raw features being discarded or withheld\nwhen aggregating multiple predictions. Besides, crowd-sourced labels need to be\naggregated to estimate the ground truth of the data. Because of the imperfect\npredictive models or human crowdsourcing workers, noisy and conflicting\ninformation is ubiquitous and inevitable. Although state-of-the-art aggregation\nmethods have been proposed to handle label spaces with flat structures, as the\nlabel space is becoming more and more complicated, aggregation under a label\nhierarchical structure becomes necessary but has been largely ignored. These\nlabel hierarchies can be quite informative as they are usually created by\ndomain experts to make sense of highly complex label correlations for many\nreal-world cases like protein functionality interactions or disease\nrelationships.\n  We propose a novel multi-source hierarchical prediction consolidation method\nto effectively exploits the complicated hierarchical label structures to\nresolve the noisy and conflicting information that inherently originates from\nmultiple imperfect sources. We formulate the problem as an optimization problem\nwith a closed-form solution. The proposed method captures the smoothness\noverall information sources as well as penalizing any consolidation result that\nviolates the constraints derived from the label hierarchy. The hierarchical\ninstance similarity, as well as the consolidation result, are inferred in a\ntotally unsupervised, iterative fashion. Experimental results on both synthetic\nand real-world datasets show the effectiveness of the proposed method over\nexisting alternatives.\n"]},
{"authors": ["Jian Li", "Amol Deshpande"], "title": ["Maximizing Expected Utility for Stochastic Combinatorial Optimization\n  Problems"], "date": ["2010-12-14T22:34:32Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1012.3189v7"], "summary": ["  We study the stochastic versions of a broad class of combinatorial problems\nwhere the weights of the elements in the input dataset are uncertain. The class\nof problems that we study includes shortest paths, minimum weight spanning\ntrees, and minimum weight matchings, and other combinatorial problems like\nknapsack. We observe that the expected value is inadequate in capturing\ndifferent types of {\\em risk-averse} or {\\em risk-prone} behaviors, and instead\nwe consider a more general objective which is to maximize the {\\em expected\nutility} of the solution for some given utility function, rather than the\nexpected weight (expected weight becomes a special case). Under the assumption\nthat there is a pseudopolynomial time algorithm for the {\\em exact} version of\nthe problem (This is true for the problems mentioned above), we can obtain the\nfollowing approximation results for several important classes of utility\nfunctions: (1) If the utility function $\\uti$ is continuous, upper-bounded by a\nconstant and $\\lim_{x\\rightarrow+\\infty}\\uti(x)=0$, we show that we can obtain\na polynomial time approximation algorithm with an {\\em additive error}\n$\\epsilon$ for any constant $\\epsilon>0$. (2) If the utility function $\\uti$ is\na concave increasing function, we can obtain a polynomial time approximation\nscheme (PTAS). (3) If the utility function $\\uti$ is increasing and has a\nbounded derivative, we can obtain a polynomial time approximation scheme. Our\nresults recover or generalize several prior results on stochastic shortest\npath, stochastic spanning tree, and stochastic knapsack. Our algorithm for\nutility maximization makes use of the separability of exponential utility and a\ntechnique to decompose a general utility function into exponential utility\nfunctions, which may be useful in other stochastic optimization problems.\n"]},
{"authors": ["Harsh Thakkar", "Mohnish Dubey", "Gezim Sejdiu", "Axel-Cyrille Ngonga Ngomo", "Jeremy Debattista", "Christoph Lange", "Jens Lehmann", "S\u00f6ren Auer", "Maria-Esther Vidal"], "title": ["LITMUS: An Open Extensible Framework for Benchmarking RDF Data\n  Management Solutions"], "date": ["2016-08-09T13:40:59Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1608.02800v1"], "summary": ["  Developments in the context of Open, Big, and Linked Data have led to an\nenormous growth of structured data on the Web. To keep up with the pace of\nefficient consumption and management of the data at this rate, many data\nManagement solutions have been developed for specific tasks and applications.\nWe present LITMUS, a framework for benchmarking data management solutions.\nLITMUS goes beyond classical storage benchmarking frameworks by allowing for\nanalysing the performance of frameworks across query languages. In this\nposition paper we present the conceptual architecture of LITMUS as well as the\nconsiderations that led to this architecture.\n"]},
{"authors": ["Zhan Li", "Olga Papaemmanouil", "Mitch Cherniack"], "title": ["OptMark: A Toolkit for Benchmarking Query Optimizers"], "date": ["2016-08-08T20:19:19Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1608.02611v1"], "summary": ["  Query optimizers have long been considered as among the most complex\ncomponents of a database engine, while the assessment of an optimizer's quality\nremains a challenging task. Indeed, existing performance benchmarks for\ndatabase engines (like TPC benchmarks) produce a performance assessment of the\nquery runtime system rather than its query optimizer. To address this\nchallenge, this paper introduces OptMark, a toolkit for evaluating the quality\nof a query optimizer. OptMark is designed to offer a number of desirable\nproperties. First, it decouples the quality of an optimizer from the quality of\nits underlying execution engine. Second it evaluates independently both the\neffectiveness of an optimizer (i.e., quality of the chosen plans) and its\nefficiency (i.e., optimization time). OptMark includes also a generic\nbenchmarking toolkit that is minimum invasive to the DBMS that wishes to use\nit. Any DBMS can provide a system-specific implementation of a simple API that\nallows OptMark to run and generate benchmark scores for the specific system.\nThis paper discusses the metrics we propose for evaluating an optimizer's\nquality, the benchmark's design and the toolkit's API and functionality. We\nhave implemented OptMark on the open-source MySQL engine as well as two\ncommercial database systems. Using these implementations we are able to assess\nthe quality of the optimizers on these three systems based on the TPC-DS\nbenchmark queries.\n"]},
{"authors": ["Shumo Chu", "Konstantin Weitz", "Alvin Cheung", "Dan Suciu"], "title": ["HoTTSQL: Proving Query Rewrites with Univalent SQL Semantics"], "date": ["2016-07-17T03:15:20Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1607.04822v2"], "summary": ["  Every database system contains a query optimizer that performs query\nrewrites. Unfortunately, developing query optimizers remains a highly\nchallenging task. Part of the challenges comes from the intricacies and rich\nfeatures of query languages, which makes reasoning about rewrite rules\ndifficult. In this paper, we propose a machine-checkable denotational semantics\nfor SQL, the de facto language for relational database, for rigorously\nvalidating rewrite rules. Unlike previously proposed semantics that are either\nnon-mechanized or only cover a small amount of SQL language features, our\nsemantics covers all major features of SQL, including bags, correlated\nsubqueries, aggregation, and indexes. Our mechanized semantics, called HoTTSQL,\nis based on K-Relations and homotopy type theory, where we denote relations as\nmathematical functions from tuples to univalent types. We have implemented\nHoTTSQL in Coq, which takes only fewer than 300 lines of code and have proved a\nwide range of SQL rewrite rules, including those from database research\nliterature (e.g., magic set rewrites) and real-world query optimizers (e.g.,\nsubquery elimination). Several of these rewrite rules have never been\npreviously proven correct. In addition, while query equivalence is generally\nundecidable, we have implemented an automated decision procedure using HoTTSQL\nfor conjunctive queries: a well-studied decidable fragment of SQL that\nencompasses many real-world queries.\n"]},
{"authors": ["Hyeok Kong", "Cholyong Jong", "Unhyok Ryang"], "title": ["Implementation of Association Rule Mining for Network Intrusion\n  Detection"], "date": ["2016-01-20T17:15:44Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1601.05335v2"], "summary": ["  Many modern intrusion detection systems are based on data mining and\ndatabase-centric architecture, where a number of data mining techniques have\nbeen found. Among the most popular techniques, association rule mining is one\nof the important topics in data mining research. This approach determines\ninteresting relationships between large sets of data items. This technique was\ninitially applied to the so-called market basket analysis, which aims at\nfinding regularities in shopping behaviour of customers of supermarkets. In\ncontrast to dataset for market basket analysis, which takes usually hundreds of\nattributes, network audit databases face tens of attributes. So the typical\nApriori algorithm of association rule mining, which needs so many database\nscans, can be improved, dealing with such characteristics of transaction\ndatabase. In this paper we propose an impoved Apriori algorithm, very useful in\npractice, using scan of network audit database only once by transaction cutting\nand hashing.\n"]},
{"authors": ["Gokhan Kul", "Duc Luong", "Ting Xie", "Patrick Coonan", "Varun Chandola", "Oliver Kennedy", "Shambhu Upadhyaya"], "title": ["Summarizing Large Query Logs in Ettu"], "date": ["2016-08-02T21:43:09Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1608.01013v1"], "summary": ["  Database access logs are large, unwieldy, and hard for humans to inspect and\nsummarize. In spite of this, they remain the canonical go-to resource for tasks\nranging from performance tuning to security auditing. In this paper, we address\nthe challenge of compactly encoding large sequences of SQL queries for\npresentation to a human user. Our approach is based on the Weisfeiler-Lehman\n(WL) approximate graph isomorphism algorithm, which identifies salient features\nof a graph or in our case of an abstract syntax tree. Our generalization of WL\nallows us to define a distance metric for SQL queries, which in turn permits\nautomated clustering of queries. We also present two techniques for visualizing\nquery clusters, and an algorithm that allows these visualizations to be\nconstructed at interactive speeds. Finally, we evaluate our algorithms in the\ncontext of a motivating example: insider threat detection at a large US bank.\nWe show experimentally on real world query logs that (a) our distance metric\ncaptures a meaningful notion of similarity, and (b) the log summarization\nprocess is scalable and performant.\n"]},
{"authors": ["Massimo Cafaro", "Marco Pulimeno", "Italo Epicoco", "Giovanni Aloisio"], "title": ["Mining frequent items in the time fading model"], "date": ["2016-01-15T12:21:47Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1601.03892v3"], "summary": ["  We present FDCMSS, a new sketch-based algorithm for mining frequent items in\ndata streams. The algorithm cleverly combines key ideas borrowed from forward\ndecay, the Count-Min and the Space Saving algorithms. It works in the time\nfading model, mining data streams according to the cash register model. We\nformally prove its correctness and show, through extensive experimental\nresults, that our algorithm outperforms $\\lambda$-HCount, a recently developed\nalgorithm, with regard to speed, space used, precision attained and error\ncommitted on both synthetic and real datasets.\n"]},
{"authors": ["Daniel Lemire", "Owen Kaser", "Kamel Aouiche"], "title": ["Sorting improves word-aligned bitmap indexes"], "date": ["2009-01-23T19:01:06Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/0901.3751v7"], "summary": ["  Bitmap indexes must be compressed to reduce input/output costs and minimize\nCPU usage. To accelerate logical operations (AND, OR, XOR) over bitmaps, we use\ntechniques based on run-length encoding (RLE), such as Word-Aligned Hybrid\n(WAH) compression. These techniques are sensitive to the order of the rows: a\nsimple lexicographical sort can divide the index size by 9 and make indexes\nseveral times faster. We investigate row-reordering heuristics. Simply\npermuting the columns of the table can increase the sorting efficiency by 40%.\nSecondary contributions include efficient algorithms to construct and aggregate\nbitmaps. The effect of word length is also reviewed by constructing 16-bit,\n32-bit and 64-bit indexes. Using 64-bit CPUs, we find that 64-bit indexes are\nslightly faster than 32-bit indexes despite being nearly twice as large.\n"]},
{"authors": ["Davide Lanti", "Guohui Xiao", "Diego Calvanese"], "title": ["Data Scaling in OBDA Benchmarks: The VIG Approach"], "date": ["2016-07-21T14:37:31Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1607.06343v2"], "summary": ["  In this paper we describe VIG, a data scaler for benchmarks in the context of\nontology-based data access (OBDA). Data scaling is a relatively recent\napproach, proposed in the database community, that allows for quickly scaling\nup an input data instance to s times its size, while preserving certain\napplication-specific characteristics. The advantage of the approach is that the\nuser is not required to manually input the characteristics of the data to be\nproduced, making it particularly suitable for OBDA benchmarks, where the\ncomplexity of database schemas might pose a challenge for manual input (e.g.,\nthe NPD benchmark contains 70 tables with some containing more than 60\ncolumns). As opposed to a traditional data scaler, VIG includes domain\ninformation provided by the OBDA mappings and the ontology in order to produce\ndata. VIG is currently used in the NPD benchmark, but it is not NPD-specific\nand can be seeded with any data instance. The distinguishing features of VIG\nare (1) its simple and clear generation strategy; (2) its efficiency, as each\nvalue is generated in constant time, without accesses to the disk or to RAM to\nretrieve previously generated values; (3) and its generality, as the data is\nexported in CSV files that can be easily imported by any RDBMS system. VIG is a\njava implementation licensed under Apache 2.0, and its source code is available\non GitHub (https://github.com/ontop/vig) in the form of a Maven project. The\ncode is being maintained since two years by the -ontop- team at the Free\nUniversity of Bozen-Bolzano.\n"]},
{"authors": ["Dingming Wu", "Christian S. Jensen"], "title": ["A Density-Based Approach to the Retrieval of Top-K Spatial Textual\n  Clusters"], "date": ["2016-07-29T03:15:58Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1607.08681v1"], "summary": ["  Keyword-based web queries with local intent retrieve web content that is\nrelevant to supplied keywords and that represent points of interest that are\nnear the query location. Two broad categories of such queries exist. The first\nencompasses queries that retrieve single spatial web objects that each satisfy\nthe query arguments. Most proposals belong to this category. The second\ncategory, to which this paper's proposal belongs, encompasses queries that\nsupport exploratory user behavior and retrieve sets of objects that represent\nregions of space that may be of interest to the user. Specifically, the paper\nproposes a new type of query, namely the top-k spatial textual clusters (k-STC)\nquery that returns the top-k clusters that (i) are located the closest to a\ngiven query location, (ii) contain the most relevant objects with regard to\ngiven query keywords, and (iii) have an object density that exceeds a given\nthreshold. To compute this query, we propose a basic algorithm that relies on\non-line density-based clustering and exploits an early stop condition. To\nimprove the response time, we design an advanced approach that includes three\ntechniques: (i) an object skipping rule, (ii) spatially gridded posting lists,\nand (iii) a fast range query algorithm. An empirical study on real data\ndemonstrates that the paper's proposals offer scalability and are capable of\nexcellent performance.\n"]},
{"authors": ["Foto Afrati", "Shlomi Dolev", "Shantanu Sharma", "Jeffrey D. Ullman"], "title": ["Meta-MapReduce: A Technique for Reducing Communication in MapReduce\n  Computations"], "date": ["2015-08-05T18:54:32Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1508.01171v2"], "summary": ["  MapReduce has proven to be one of the most useful paradigms in the revolution\nof distributed computing, where cloud services and cluster computing become the\nstandard venue for computing. The federation of cloud and big data activities\nis the next challenge where MapReduce should be modified to avoid (big) data\nmigration across remote (cloud) sites. This is exactly our scope of research,\nwhere only the very essential data for obtaining the result is transmitted,\nreducing communication, processing and preserving data privacy as much as\npossible. In this work, we propose an algorithmic technique for MapReduce\nalgorithms, called Meta-MapReduce, that decreases the communication cost by\nallowing us to process and move metadata to clouds and from the map phase to\nreduce phase. In Meta-MapReduce, the reduce phase fetches only the required\ndata at required iterations, which in turn, assists in preserving the data\nprivacy.\n"]},
{"authors": ["Nicolas Kourtellis", "Gianmarco De Francisci Morales", "Albert Bifet", "Arinto Murdopo"], "title": ["VHT: Vertical Hoeffding Tree"], "date": ["2016-07-28T06:15:24Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1607.08325v1"], "summary": ["  IoT Big Data requires new machine learning methods able to scale to large\nsize of data arriving at high speed. Decision trees are popular machine\nlearning models since they are very effective, yet easy to interpret and\nvisualize. In the literature, we can find distributed algorithms for learning\ndecision trees, and also streaming algorithms, but not algorithms that combine\nboth features. In this paper we present the Vertical Hoeffding Tree (VHT), the\nfirst distributed streaming algorithm for learning decision trees. It features\na novel way of distributing decision trees via vertical parallelism. The\nalgorithm is implemented on top of Apache SAMOA, a platform for mining\ndistributed data streams, and thus able to run on real-world clusters. We run\nseveral experiments to study the accuracy and throughput performance of our new\nVHT algorithm, as well as its ability to scale while keeping its superior\nperformance with respect to non-distributed decision trees.\n"]},
{"authors": ["Mostafa Milani", "Andrea Cali", "Leopoldo Bertossi"], "title": ["A Hybrid Approach to Query Answering under Expressive Datalog+/-"], "date": ["2016-04-22T18:46:10Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1604.06770v2"], "summary": ["  Datalog+/- is a family of ontology languages that combine good computational\nproperties with high expressive power. Datalog+/- languages are provably able\nto capture the most relevant Semantic Web languages. In this paper we consider\nthe class of weakly-sticky (WS) Datalog+/- programs, which allow for certain\nuseful forms of joins in rule bodies as well as extending the well-known class\nof weakly-acyclic TGDs. So far, only non-deterministic algorithms were known\nfor answering queries on WS Datalog+/- programs. We present novel deterministic\nquery answering algorithms under WS Datalog+/-. In particular, we propose: (1)\na bottom-up grounding algorithm based on a query-driven chase, and (2) a hybrid\napproach based on transforming a WS program into a so-called sticky one, for\nwhich query rewriting techniques are known. We discuss how our algorithms can\nbe optimized and effectively applied for query answering in real-world\nscenarios.\n"]},
{"authors": ["Yehezkel S. Resheff"], "title": ["Online Trajectory Segmentation and Summary With Applications to\n  Visualization and Retrieval"], "date": ["2016-07-24T14:50:45Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1607.08188v1"], "summary": ["  Trajectory segmentation is the process of subdividing a trajectory into parts\neither by grouping points similar with respect to some measure of interest, or\nby minimizing a global objective function. Here we present a novel online\nalgorithm for segmentation and summary, based on point density along the\ntrajectory, and based on the nature of the naturally occurring structure of\nintermittent bouts of locomotive and local activity. We show an application to\nvisualization of trajectory datasets, and discuss the use of the summary as an\nindex allowing efficient queries which are otherwise impossible or\ncomputationally expensive, over very large datasets.\n"]},
{"authors": ["Erkang Zhu", "Fatemeh Nargesian", "Ken Q. Pu", "Ren\u00e9e J. Miller"], "title": ["LSH Ensemble: Internet-Scale Domain Search"], "date": ["2016-03-24T01:43:28Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1603.07410v4"], "summary": ["  We study the problem of domain search where a domain is a set of distinct\nvalues from an unspecified universe. We use Jaccard set containment, defined as\n$|Q \\cap X|/|Q|$, as the relevance measure of a domain $X$ to a query domain\n$Q$. Our choice of Jaccard set containment over Jaccard similarity makes our\nwork particularly suitable for searching Open Data and data on the web, as\nJaccard similarity is known to have poor performance over sets with large\ndifferences in their domain sizes. We demonstrate that the domains found in\nseveral real-life Open Data and web data repositories show a power-law\ndistribution over their domain sizes.\n  We present a new index structure, Locality Sensitive Hashing (LSH) Ensemble,\nthat solves the domain search problem using set containment at Internet scale.\nOur index structure and search algorithm cope with the data volume and skew by\nmeans of data sketches (MinHash) and domain partitioning. Our index structure\ndoes not assume a prescribed set of values. We construct a cost model that\ndescribes the accuracy of LSH Ensemble with any given partitioning. This allows\nus to formulate the partitioning for LSH Ensemble as an optimization problem.\nWe prove that there exists an optimal partitioning for any distribution.\nFurthermore, for datasets following a power-law distribution, as observed in\nOpen Data and Web data corpora, we show that the optimal partitioning can be\napproximated using equi-depth, making it efficient to use in practice.\n  We evaluate our algorithm using real data (Canadian Open Data and WDC Web\nTables) containing up over 262 M domains. The experiments demonstrate that our\nindex consistently outperforms other leading alternatives in accuracy and\nperformance. The improvements are most dramatic for data with large skew in the\ndomain sizes. Even at 262 M domains, our index sustains query performance with\nunder 3 seconds response time.\n"]},
{"authors": ["Dingxiong Deng", "Cyrus Shahabi", "Ugur Demiryurek", "Linhong Zhu", "Rose Yu", "Yan Liu"], "title": ["Latent Space Model for Road Networks to Predict Time-Varying Traffic"], "date": ["2016-02-13T08:18:07Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1602.04301v3"], "summary": ["  Real-time traffic prediction from high-fidelity spatiotemporal traffic sensor\ndatasets is an important problem for intelligent transportation systems and\nsustainability. However, it is challenging due to the complex topological\ndependencies and high dynamics associated with changing road conditions. In\nthis paper, we propose a Latent Space Model for Road Networks (LSM-RN) to\naddress these challenges. In particular, given a series of road network\nsnapshots, we learn the attributes of vertices in latent spaces which capture\nboth topological and temporal properties. As these latent attributes are\ntime-dependent, they can estimate how traffic patterns form and evolve. In\naddition, we present an incremental online algorithm which sequentially and\nadaptively learn the latent attributes from the temporal graph changes. Our\nframework enables real-time traffic prediction by 1) exploiting real-time\nsensor readings to adjust/update the existing latent spaces, and 2) training as\ndata arrives and making predictions on-the-fly with given data. By conducting\nextensive experiments with a large volume of real-world traffic sensor data, we\ndemonstrate the utility superiority of our framework for real-time traffic\nprediction on large road networks over competitors as well as a baseline\ngraph-based LSM.\n"]},
{"authors": ["Mohammad Reza Abbasifard", "Omid Isfahani Alamdari"], "title": ["Fragment Allocation Configuration in Distributed Database Systems"], "date": ["2016-07-20T19:10:48Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1607.06063v1"], "summary": ["  In distributed database (DDB) management systems, fragment allocation is one\nof the most important components that can directly affect the performance of\nDDB. In this research work, we will show that declarative programming\nlanguages, e.g. logic programming languages, can be used to represent different\ndata fragment allocation techniques. Results indicate that, using declarative\nprogramming language significantly simplifies the representation of fragment\nallocation algorithm, thus opens door for any further developments and\noptimizations. The under consideration case study also show that our approach\ncan be extended to be used in different areas of distributed systems.\n"]},
{"authors": ["William W. Cohen"], "title": ["TensorLog: A Differentiable Deductive Database"], "date": ["2016-05-20T20:10:46Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1605.06523v2"], "summary": ["  Large knowledge bases (KBs) are useful in many tasks, but it is unclear how\nto integrate this sort of knowledge into \"deep\" gradient-based learning\nsystems. To address this problem, we describe a probabilistic deductive\ndatabase, called TensorLog, in which reasoning uses a differentiable process.\nIn TensorLog, each clause in a logical theory is first converted into certain\ntype of factor graph. Then, for each type of query to the factor graph, the\nmessage-passing steps required to perform belief propagation (BP) are\n\"unrolled\" into a function, which is differentiable. We show that these\nfunctions can be composed recursively to perform inference in non-trivial\nlogical theories containing multiple interrelated clauses and predicates. Both\ncompilation and inference in TensorLog are efficient: compilation is linear in\ntheory size and proof depth, and inference is linear in database size and the\nnumber of message-passing steps used in BP. We also present experimental\nresults with TensorLog and discuss its relationship to other first-order\nprobabilistic logics.\n"]},
{"authors": ["Fereidoon Sadri", "Gayatri Tallur"], "title": ["Integration of Probabilistic Uncertain Information"], "date": ["2016-07-19T19:18:08Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1607.05702v1"], "summary": ["  We study the problem of data integration from sources that contain\nprobabilistic uncertain information. Data is modeled by possible-worlds with\nprobability distribution, compactly represented in the probabilistic relation\nmodel. Integration is achieved efficiently using the extended probabilistic\nrelation model. We study the problem of determining the probability\ndistribution of the integration result. It has been shown that, in general,\nonly probability ranges can be determined for the result of integration. In\nthis paper we concentrate on a subclass of extended probabilistic relations,\nthose that are obtainable through integration. We show that under intuitive and\nreasonable assumptions we can determine the exact probability distribution of\nthe result of integration.\n"]},
{"authors": ["Antoine Amarilli", "Silviu Maniu", "Mika\u00ebl Monet"], "title": ["Challenges for Efficient Query Evaluation on Structured Probabilistic\n  Data"], "date": ["2016-07-19T12:15:37Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1607.05538v1"], "summary": ["  Query answering over probabilistic data is an important task but is generally\nintractable. However, a new approach for this problem has recently been\nproposed, based on structural decompositions of input databases, following,\ne.g., tree decompositions. This paper presents a vision for a database\nmanagement system for probabilistic data built following this structural\napproach. We review our existing and ongoing work on this topic and highlight\nmany theoretical and practical challenges that remain to be addressed.\n"]},
{"authors": ["Lu\u00eds Cruz-Filipe"], "title": ["Grounded Fixpoints and Active Integrity Constraints"], "date": ["2016-07-19T05:57:05Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1607.05412v1"], "summary": ["  The formalism of active integrity constraints was introduced as a way to\nspecify particular classes of integrity constraints over relational databases\ntogether with preferences on how to repair existing inconsistencies. The\nrule-based syntax of such integrity constraints also provides algorithms for\nfinding such repairs that achieve the best asymptotic complexity. However, the\ndifferent semantics that have been proposed for these integrity constraints all\nexhibit some counter-intuitive examples. In this work, we look at active\nintegrity constraints using ideas from algebraic fixpoint theory. We show how\ndatabase repairs can be modeled as fixpoints of particular operators on\ndatabases, and study how the notion of grounded fixpoint induces a\ncorresponding notion of grounded database repair that captures several natural\nintuitions, and in particular avoids the problems of previous alternative\nsemantics. In order to study grounded repairs in their full generality, we need\nto generalize the notion of grounded fixpoint to non-deterministic operators.\nWe propose such a definition and illustrate its plausibility in the database\ncontext.\n"]},
{"authors": ["Abolfazl Asudeh", "Nan Zhang", "Gautam Das"], "title": ["Query Reranking As A Service"], "date": ["2016-02-07T04:03:26Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1602.05100v2"], "summary": ["  The ranked retrieval model has rapidly become the de facto way for search\nquery processing in client-server databases, especially those on the web.\nDespite of the extensive efforts in the database community on designing better\nranking functions/mechanisms, many such databases in practice still fail to\naddress the diverse and sometimes contradicting preferences of users on tuple\nranking, perhaps (at least partially) due to the lack of expertise and/or\nmotivation for the database owner to design truly effective ranking functions.\nThis paper takes a different route on addressing the issue by defining a novel\n{\\em query reranking problem}, i.e., we aim to design a third-party service\nthat uses nothing but the public search interface of a client-server database\nto enable the on-the-fly processing of queries with any user-specified ranking\nfunctions (with or without selection conditions), no matter if the ranking\nfunction is supported by the database or not. We analyze the worst-case\ncomplexity of the problem and introduce a number of ideas, e.g., on-the-fly\nindexing, domination detection and virtual tuple pruning, to reduce the\naverage-case cost of the query reranking algorithm. We also present extensive\nexperimental results on real-world datasets, in both offline and live online\nsystems, that demonstrate the effectiveness of our proposed techniques.\n"]},
{"authors": ["Yanhong A. Liu", "Jon Brandvein", "Scott D. Stoller", "Bo Lin"], "title": ["Demand-Driven Incremental Object Queries"], "date": ["2015-11-14T17:27:33Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1511.04583v3"], "summary": ["  Object queries are essential in information seeking and decision making in\nvast areas of applications. However, a query may involve complex conditions on\nobjects and sets, which can be arbitrarily nested and aliased. The objects and\nsets involved as well as the demand---the given parameter values of\ninterest---can change arbitrarily. How to implement object queries efficiently\nunder all possible updates, and furthermore to provide complexity guarantees?\n  This paper describes an automatic method. The method allows powerful queries\nto be written completely declaratively. It transforms demand as well as all\nobjects and sets into relations. Most importantly, it defines invariants for\nnot only the query results, but also all auxiliary values about the objects and\nsets involved, including those for propagating demand, and incrementally\nmaintains all of them. Implementation and experiments with problems from a\nvariety of application areas, including distributed algorithms and\nprobabilistic queries, confirm the analyzed complexities, trade-offs, and\nsignificant improvements over prior work.\n"]},
{"authors": ["Andre Araujo", "Jason Chaves", "Haricharan Lakshman", "Roland Angst", "Bernd Girod"], "title": ["Large-Scale Query-by-Image Video Retrieval Using Bloom Filters"], "date": ["2016-04-27T05:46:52Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1604.07939v2"], "summary": ["  We consider the problem of using image queries to retrieve videos from a\ndatabase. Our focus is on large-scale applications, where it is infeasible to\nindex each database video frame independently. Our main contribution is a\nframework based on Bloom filters, which can be used to index long video\nsegments, enabling efficient image-to-video comparisons. Using this framework,\nwe investigate several retrieval architectures, by considering different types\nof aggregation and different functions to encode visual information -- these\nplay a crucial role in achieving high performance. Extensive experiments show\nthat the proposed technique improves mean average precision by 24% on a public\ndataset, while being 4X faster, compared to the previous state-of-the-art.\n"]},
{"authors": ["Daniel Hern\u00e1ndez", "Claudio Gutierrez", "Renzo Angles"], "title": ["Correlation and Substitution in SPARQL"], "date": ["2016-06-05T00:43:44Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1606.01441v2"], "summary": ["  In the current SPARQL specification the notion of correlation and\nsubstitution are not well defined. This problem triggers several ambiguities in\nthe semantics. In fact, implementations as Fuseki and Virtuoso assume different\nsemantics.\n  In this technical report, we provide a semantics of correlation and\nsubstitution following the classic philosophy of substitution and correlation\nin logic, programming languages and SQL. We think this proposal not only fix\nthe current ambiguities and problems, but helps to set a safe formal base to\nfurther extensions of the language.\n  This work is part of an ongoing work of Daniel Hernandez. These anomalies in\nthe W3C Specification of SPARQL 1.1 were detected early and reported no later\nthan 2014, when two erratas were registered (cf.\nhttps://www.w3.org/2013/sparql-errata#errata-query-8 and\nhttps://www.w3.org/2013/sparql-errata#errata-query-10).\n"]},
{"authors": ["Mostafa Milani", "Leopoldo Bertossi"], "title": ["Extending Weakly-Sticky Datalog+/-: Query-Answering Tractability and\n  Optimizations"], "date": ["2016-07-10T02:56:33Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1607.02682v1"], "summary": ["  Weakly-sticky (WS) Datalog+/- is an expressive member of the family of\nDatalog+/- programs that is based on the syntactic notions of stickiness and\nweak-acyclicity. Query answering over the WS programs has been investigated,\nbut there is still much work to do on the design and implementation of\npractical query answering (QA) algorithms and their optimizations. Here, we\nstudy sticky and WS programs from the point of view of the behavior of the\nchase procedure, extending the stickiness property of the chase to that of\ngeneralized stickiness of the chase (gsch-property). With this property we\nspecify the semantic class of GSCh programs, which includes sticky and WS\nprograms, and other syntactic subclasses that we identify. In particular, we\nintroduce joint-weakly-sticky (JWS) programs, that include WS programs. We also\npropose a bottom-up QA algorithm for a range of subclasses of GSCh. The\nalgorithm runs in polynomial time (in data) for JWS programs. Unlike the WS\nclass, JWS is closed under a general magic-sets rewriting procedure for the\noptimization of programs with existential rules. We apply the magic-sets\nrewriting in combination with the proposed QA algorithm for the optimization of\nQA over JWS programs.\n"]},
{"authors": ["Carlos R. Rivero", "Hasan M. Jamil"], "title": ["A Novel Model for Distributed Big Data Service Composition using\n  Stratified Functional Graph Matching"], "date": ["2016-07-09T22:04:39Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1607.02669v1"], "summary": ["  A significant number of current industrial applications rely on web services.\nA cornerstone task in these applications is discovering a suitable service that\nmeets the threshold of some user needs. Then, those services can be composed to\nperform specific functionalities. We argue that the prevailing approach to\ncompose services based on the \"all or nothing\" paradigm is limiting and leads\nto exceedingly high rejection of potentially suitable services. Furthermore,\ncontemporary models do not allow \"mix and match\" composition from atomic\nservices of different composite services when binary matching is not possible\nor desired. In this paper, we propose a new model for service composition based\non \"stratified graph summarization\" and \"service stitching\". We discuss the\nlimitations of existing approaches with a motivating example, present our\napproach to overcome these limitations, and outline a possible architecture for\nservice composition from atomic services. Our thesis is that, with the advent\nof Big Data, our approach will reduce latency in service discovery, and will\nimprove efficiency and accuracy of matchmaking and composition of services.\n"]},
{"authors": ["Frank Rosner", "Alexander Hinneburg"], "title": ["Translating Bayesian Networks into Entity Relationship Models, Extended\n  Version"], "date": ["2016-07-08T15:06:46Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1607.02399v1"], "summary": ["  Big data analytics applications drive the convergence of data management and\nmachine learning. But there is no conceptual language available that is spoken\nin both worlds. The main contribution of the paper is a method to translate\nBayesian networks, a main conceptual language for probabilistic graphical\nmodels, into usable entity relationship models. The transformed representation\nof a Bayesian network leaves out mathematical details about probabilistic\nrelationships but unfolds all information relevant for data management tasks.\nAs a real world example, we present the TopicExplorer system that uses Bayesian\ntopic models as a core component in an interactive, database-supported web\napplication. Last, we sketch a conceptual framework that eases machine learning\nspecific development tasks while building big data analytics applications.\n"]},
{"authors": ["Pablo Barcelo", "Miguel Romero"], "title": ["The complexity of reverse engineering problems for conjunctive queries"], "date": ["2016-06-03T18:14:35Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1606.01206v2"], "summary": ["  Reverse engineering problems for conjunctive queries (CQs), such as query by\nexample (QBE) or definability, take a set of user examples and convert them\ninto an explanatory CQ. Despite their importance, the complexity of these\nproblems is prohibitively high (coNEXPTIME-complete). We isolate their two main\nsources of complexity and propose relaxations of them that reduce the\ncomplexity while having meaningful theoretical interpretations. The first\nrelaxation is based on the idea of using existential pebble games for\napproximating homomorphism tests. We show that this characterizes\nQBE/definability for CQs up to treewidth $k$, while reducing the complexity to\nEXPTIME. As a side result, we obtain that the complexity of the\nQBE/definability problems for CQs of treewidth $k$ is EXPTIME-complete for each\n$k \\geq 1$. The second relaxation is based on the idea of \"desynchronizing\"\ndirect products, which characterizes QBE/definability for unions of CQs and\nreduces the complexity to coNP. The combination of these two relaxations yields\ntractability for QBE and characterizes it in terms of unions of CQs of\ntreewidth at most $k$. We also study the complexity of these problems for\nconjunctive regular path queries over graph databases, showing them to be no\nmore difficult than for CQs.\n"]},
{"authors": ["Daniel P. Lupp", "Evgenij Thorstensen"], "title": ["Mapping Data to Ontologies with Exceptions Using Answer Set Programming"], "date": ["2016-07-07T14:00:06Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1607.02018v1"], "summary": ["  In ontology-based data access, databases are connected to an ontology via\nmappings from queries over the database to queries over the ontology. In this\npaper, we consider mappings from relational databases to first-order\nontologies, and define an ASP-based framework for GLAV mappings with queries\nover the ontology in the mapping rule bodies. We show that this type of\nmappings can be used to express constraints and exceptions, as well as being a\npowerful mechanism for succinctly representing OBDA mappings. We give an\nalgorithm for brave reasoning in this setting, and show that this problem has\neither the same data complexity as ASP (NP- complete), or it is at least as\nhard as the complexity of checking entailment for the ontology queries.\nFurthermore, we show that for ontologies with UCQ-rewritable queries there\nexists a natural reduction from mapping programs to \\exists-ASP, an extension\nof ASP with existential variables that itself admits a natural reduction to\nASP.\n"]},
{"authors": ["Chunbin Lin", "Jianguo Wang", "Yannis Papakonstantinou"], "title": ["Data Compression for Analytics over Large-scale In-memory Column\n  Databases"], "date": ["2016-06-30T00:44:05Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1606.09315v2"], "summary": ["  Data compression schemes have exhibited their importance in column databases\nby contributing to the high-performance OLAP (Online Analytical Processing)\nquery processing. Existing works mainly concentrate on evaluating compression\nschemes for disk-resident databases as data is mostly stored on disks. With the\ncontinuously decreasing of the price/capacity ratio of main memory, it is the\ntendencies of the times to reside data in main memory. But the discussion of\ndata compression on in-memory databases is very vague in the literature. In\nthis work, we present an updated discussion about whether it is valuable to use\ndata compression techniques in memory databases. If yes, how should memory\ndatabases apply data compression schemes to maximize performance?\n"]},
{"authors": ["Gianluigi Greco", "Francesco Scarcello"], "title": ["Greedy Strategies and Larger Islands of Tractability for Conjunctive\n  Queries and Constraint Satisfaction Problems"], "date": ["2016-03-31T14:57:33Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1603.09617v2"], "summary": ["  Structural decomposition methods have been developed for identifying\ntractable classes of instances of fundamental problems in databases, such as\nconjunctive queries and query containment, of the constraint satisfaction\nproblem in artificial intelligence, or more generally of the homomorphism\nproblem over relational structures. Most structural decomposition methods can\nbe characterized through hypergraph games that are variations of the Robber and\nCops graph game that characterizes the notion of treewidth. In particular,\ndecomposition trees somehow correspond to monotone winning strategies, where\nthe escape space of the robber on the hypergraph is shrunk monotonically by the\ncops. In fact, unlike the treewidth case, there are hypergraphs where monotonic\nstrategies do not exist, while the robber can be captured by means of more\ncomplex non-monotonic strategies. However, these powerful strategies do not\ncorrespond in general to valid decompositions. The paper provides a general way\nto exploit the power of non-monotonic strategies, by allowing a \"disciplined\"\nform of non-monotonicity, characteristic of cops playing in a greedy way. It is\nshown that deciding the existence of a (non-monotone) greedy winning strategy\n(and compute one, if any) is tractable. Moreover, despite their\nnon-monotonicity, such strategies always induce valid decomposition trees,\nwhich can be computed efficiently based on them. As a consequence, greedy\nstrategies allow us to define new islands of tractability for the considered\nproblems properly including all previously known classes of tractable\ninstances.\n"]},
{"authors": ["Olaf Hartig", "M. Tamer \u00d6zsu"], "title": ["Walking without a Map: Optimizing Response Times of Traversal-Based\n  Linked Data Queries (Extended Version)"], "date": ["2016-07-04T21:05:01Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1607.01046v1"], "summary": ["  The emergence of Linked Data on the WWW has spawned research interest in an\nonline execution of declarative queries over this data. A particularly\ninteresting approach is traversal-based query execution which fetches data by\ntraversing data links and, thus, is able to make use of up-to-date data from\ninitially unknown data sources. The downside of this approach is the delay\nbefore the query engine completes a query execution. In this paper, we address\nthis problem by proposing an approach to return as many elements of the result\nset as soon as possible. The basis of this approach is a traversal strategy\nthat aims to fetch result-relevant data as early as possible. The challenge for\nsuch a strategy is that the query engine does not know a priori which of the\ndata sources that will be discovered during the query execution contain\nresult-relevant data. We introduce 16 different traversal approaches and\nexperimentally study their impact on response times. Our experiments show that\nsome of the approaches can achieve significant improvements over the baseline\nof looking up URIs on a first-come, first-served basis. Additionally, we verify\nthe importance of these approaches by showing that typical query optimization\ntechniques that focus solely on the process of constructing the query result\ncannot have any significant impact on the response times of traversal-based\nquery executions.\n"]},
{"authors": ["Antoine Amarilli", "Michael Benedikt", "Pierre Bourhis", "Michael Vanden Boom"], "title": ["Query Answering with Transitive and Linear-Ordered Data"], "date": ["2016-07-04T10:39:58Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1607.00813v1"], "summary": ["  We consider entailment problems involving powerful constraint languages such\nas guarded existential rules, in which additional semantic restrictions are put\non a set of distinguished relations. We consider restricting a relation to be\ntransitive, restricting a relation to be the transitive closure of another\nrelation, and restricting a relation to be a linear order. We give some natural\ngeneralizations of guardedness that allow inference to be decidable in each\ncase, and isolate the complexity of the corresponding decision problems.\nFinally we show that slight changes in our conditions lead to undecidability.\n"]},
{"authors": ["Jiawei Zhang", "Senzhang Wang", "Qianyi Zhan", "Philip S. Yu"], "title": ["Intertwined Viral Marketing through Online Social Networks"], "date": ["2016-07-02T18:00:33Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1607.00542v1"], "summary": ["  Traditional viral marketing problems aim at selecting a subset of seed users\nfor one single product to maximize its awareness in social networks. However,\nin real scenarios, multiple products can be promoted in social networks at the\nsame time. At the product level, the relationships among these products can be\nquite intertwined, e.g., competing, complementary and independent. In this\npaper, we will study the \"interTwined Influence Maximization\" (i.e., TIM)\nproblem for one product that we target on in online social networks, where\nmultiple other competing/complementary/independent products are being promoted\nsimultaneously. The TIM problem is very challenging to solve due to (1) few\nexisting models can handle the intertwined diffusion procedure of multiple\nproducts concurrently, and (2) optimal seed user selection for the target\nproduct may depend on other products' marketing strategies a lot. To address\nthe TIM problem, a unified greedy framework TIER (interTwined Influence\nEstimatoR) is proposed in this paper. Extensive experiments conducted on four\ndifferent types of real-world social networks demonstrate that TIER can\noutperform all the comparison methods with significant advantages in solving\nthe TIM problem.\n"]},
{"authors": ["Shaleen Deep", "Paraschos Koutris"], "title": ["The Design of Arbitrage-Free Data Pricing Schemes"], "date": ["2016-06-30T07:44:18Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1606.09376v1"], "summary": ["  Motivated by a growing market that involves buying and selling data over the\nweb, we study pricing schemes that assign value to queries issued over a\ndatabase. Previous work studied pricing mechanisms that compute the price of a\nquery by extending a data seller's explicit prices on certain queries, or\ninvestigated the properties that a pricing function should exhibit without\ndetailing a generic construction. In this work, we present a formal framework\nfor pricing queries over data that allows the construction of general families\nof pricing functions, with the main goal of avoiding arbitrage. We consider two\ntypes of pricing schemes: instance-independent schemes, where the price depends\nonly on the structure of the query, and answer-dependent schemes, where the\nprice also depends on the query output. Our main result is a complete\ncharacterization of the structure of pricing functions in both settings, by\nrelating it to properties of a function over a lattice. We use our\ncharacterization, together with information-theoretic methods, to construct a\nvariety of arbitrage-free pricing functions. Finally, we discuss various\ntradeoffs in the design space and present techniques for efficient computation\nof the proposed pricing functions.\n"]},
{"authors": ["Ashwin Narayan", "Vuk Markovic", "Natalia Postawa", "Anna King", "Alejandro Morales", "K. Ashwin Kumar", "Petros Efstathopoulos"], "title": ["Efficient Routing for Cost Effective Scale-out Data Architectures"], "date": ["2016-06-28T20:53:33Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1606.08884v1"], "summary": ["  Efficient retrieval of information is of key importance when using Big Data\nsystems. In large scale-out data architectures, data are distributed and\nreplicated across several machines. Queries/tasks to such data architectures,\nare sent to a router which determines the machines containing the requested\ndata. Ideally, to reduce the overall cost of analytics, the smallest set of\nmachines required to satisfy the query should be returned by the router.\nMathematically, this can be modeled as the set cover problem, which is NP-hard,\nthus making the routing process a balance between optimality and performance.\nEven though an efficient greedy approximation algorithm for routing a single\nquery exists, there is currently no better method for processing multiple\nqueries than running the greedy set cover algorithm repeatedly for each query.\nThis method is impractical for Big Data systems and the state-of-the-art\ntechniques route a query to all machines and choose as a cover the machines\nthat respond fastest. In this paper, we propose an efficient technique to\nspeedup the routing of a large number of real-time queries while minimizing the\nnumber of machines that each query touches (query span). We demonstrate that by\nanalyzing the correlation between known queries and performing query\nclustering, we can reduce the set cover computation time, thereby significantly\nspeeding up routing of unknown queries. Experiments show that our incremental\nset cover-based routing is 2.5 times faster and can return on average 50% fewer\nmachines per query when compared to repeated greedy set cover and baseline\nrouting techniques.\n"]},
{"authors": ["Lorenzo De Stefani", "Alessandro Epasto", "Matteo Riondato", "Eli Upfal"], "title": ["TRI\u00c8ST: Counting Local and Global Triangles in Fully-dynamic Streams\n  with Fixed Memory Size"], "date": ["2016-02-24T07:39:27Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1602.07424v2"], "summary": ["  We present TRI\\`EST, a suite of one-pass streaming algorithms to compute\nunbiased, low-variance, high-quality approximations of the global and local\n(i.e., incident to each vertex) number of triangles in a fully-dynamic graph\nrepresented as an adversarial stream of edge insertions and deletions. Our\nalgorithms use reservoir sampling and its variants to exploit the\nuser-specified memory space at all times. This is in contrast with previous\napproaches which use hard-to-choose parameters (e.g., a fixed sampling\nprobability) and offer no guarantees on the amount of memory they will use. We\nshow a full analysis of the variance of the estimations and novel concentration\nbounds for these quantities. Our experimental results on very large graphs show\nthat TRI\\`EST outperforms state-of-the-art approaches in accuracy and exhibits\na small update time.\n"]},
{"authors": ["Peter Buneman", "S\u0142awek Staworko"], "title": ["RDF Graph Alignment with Bisimulation"], "date": ["2016-06-28T11:33:55Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1606.08657v1"], "summary": ["  We investigate the problem of aligning two RDF databases, an essential\nproblem in understanding the evolution of ontologies. Our approaches address\nthree fundamental challenges: 1) the use of \"blank\" (null) names, 2) ontology\nchanges in which different names are used to identify the same entity, and 3)\nsmall changes in the data values as well as small changes in the graph\nstructure of the RDF database. We propose approaches inspired by the classical\nnotion of graph bisimulation and extend them to capture the natural metrics of\nedit distance on the data values and the graph structure. We evaluate our\nmethods on three evolving curated data sets. Overall, our results show that the\nproposed methods perform well and are scalable.\n"]},
{"authors": ["Jure Leskovec", "Rok Sosic"], "title": ["SNAP: A General Purpose Network Analysis and Graph Mining Library"], "date": ["2016-06-24T03:17:12Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1606.07550v1"], "summary": ["  Large networks are becoming a widely used abstraction for studying complex\nsystems in a broad set of disciplines, ranging from social network analysis to\nmolecular biology and neuroscience. Despite an increasing need to analyze and\nmanipulate large networks, only a limited number of tools are available for\nthis task.\n  Here, we describe Stanford Network Analysis Platform (SNAP), a\ngeneral-purpose, high-performance system that provides easy to use, high-level\noperations for analysis and manipulation of large networks. We present SNAP\nfunctionality, describe its implementational details, and give performance\nbenchmarks. SNAP has been developed for single big-memory machines and it\nbalances the trade-off between maximum performance, compact in-memory graph\nrepresentation, and the ability to handle dynamic graphs where nodes and edges\nare being added or removed over time. SNAP can process massive networks with\nhundreds of millions of nodes and billions of edges. SNAP offers over 140\ndifferent graph algorithms that can efficiently manipulate large graphs,\ncalculate structural properties, generate regular and random graphs, and handle\nattributes and meta-data on nodes and edges. Besides being able to handle large\ngraphs, an additional strength of SNAP is that networks and their attributes\nare fully dynamic, they can be modified during the computation at low cost.\nSNAP is provided as an open source library in C++ as well as a module in\nPython.\n  We also describe the Stanford Large Network Dataset, a set of social and\ninformation real-world networks and datasets, which we make publicly available.\nThe collection is a complementary resource to our SNAP software and is widely\nused for development and benchmarking of graph analytics algorithms.\n"]},
{"authors": ["Dalila Attaf", "Djamila Hamdadou", "Sidahmed Benabderrahmane", "Aicha Lafrid"], "title": ["Satellite Images Analysis with Symbolic Time Series: A Case Study of the\n  Algerian Zone"], "date": ["2016-06-23T18:47:48Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1606.07784v1"], "summary": ["  Satellite Image Time Series (SITS) are an important source of information for\nstudying land occupation and its evolution. Indeed, the very large volumes of\ndigital data stored, usually are not ready to a direct analysis. In order to\nboth reduce the dimensionality and information extraction, time series data\nmining generally gives rise to change of time series representation. In an\nobjective of information intelligibility extracted from the representation\nchange, we may use symbolic representations of time series. Many high level\nrepresentations of time series have been proposed for data mining, including\nFourier transforms, wavelets, piecewise polynomial models, etc. Many\nresearchers have also considered symbolic representations of time series,\nnoting that such representations would potentiality allow researchers to avail\nof the wealth of data structures and algorithms from the text processing and\nbioinformatics communities. We present in this work, one of the main symbolic\nrepresentation methods \"SAX\"(Symbolic Aggregate Approximation) and we\nexperience this method to symbolize and reduce the dimensionality of a\nSatellite Image Times Series acquired over a period of 5 years by\ncharacterizing the evolution of a vegetation index (NDVI).\n"]},
{"authors": ["Hugo Firth", "Paolo Missier"], "title": ["TAPER: query-aware, partition-enhancement for large, heterogenous,\n  graphs"], "date": ["2016-03-15T10:41:59Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1603.04626v2"], "summary": ["  Graph partitioning has long been seen as a viable approach to address Graph\nDBMS scalability. A partitioning, however, may introduce extra query processing\nlatency unless it is sensitive to a specific query workload, and optimised to\nminimise inter-partition traversals for that workload. Additionally, it should\nalso be possible to incrementally adjust the partitioning in reaction to\nchanges in the graph topology, the query workload, or both. Because of their\ncomplexity, current partitioning algorithms fall short of one or both of these\nrequirements, as they are designed for offline use and as one-off operations.\nThe TAPER system aims to address both requirements, whilst leveraging existing\npartitioning algorithms. TAPER takes any given initial partitioning as a\nstarting point, and iteratively adjusts it by swapping chosen vertices across\npartitions, heuristically reducing the probability of inter-partition\ntraversals for a given pattern matching queries workload. Iterations are\ninexpensive thanks to time and space optimisations in the underlying support\ndata structures. We evaluate TAPER on two different large test graphs and over\nrealistic query workloads. Our results indicate that, given a hash-based\npartitioning, TAPER reduces the number of inter-partition traversals by around\n80%; given an unweighted METIS partitioning, by around 30%. These reductions\nare achieved within 8 iterations and with the additional advantage of being\nworkload-aware and usable online.\n"]},
{"authors": ["Niek Tax", "Natalia Sidorova", "Reinder Haakma", "Wil M. P. van der Aalst"], "title": ["Log-based Evaluation of Label Splits for Process Models"], "date": ["2016-06-23T10:29:48Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1606.07259v1"], "summary": ["  Process mining techniques aim to extract insights in processes from event\nlogs. One of the challenges in process mining is identifying interesting and\nmeaningful event labels that contribute to a better understanding of the\nprocess. Our application area is mining data from smart homes for elderly,\nwhere the ultimate goal is to signal deviations from usual behavior and provide\ntimely recommendations in order to extend the period of independent living.\nExtracting individual process models showing user behavior is an important\ninstrument in achieving this goal. However, the interpretation of sensor data\nat an appropriate abstraction level is not straightforward. For example, a\nmotion sensor in a bedroom can be triggered by tossing and turning in bed or by\ngetting up. We try to derive the actual activity depending on the context\n(time, previous events, etc.). In this paper we introduce the notion of label\nrefinements, which links more abstract event descriptions with their more\nrefined counterparts. We present a statistical evaluation method to determine\nthe usefulness of a label refinement for a given event log from a process\nperspective. Based on data from smart homes, we show how our statistical\nevaluation method for label refinements can be used in practice. Our method was\nable to select two label refinements out of a set of candidate label\nrefinements that both had a positive effect on model precision.\n"]},
{"authors": ["Robert John Rovetto", "T. S. Kelso"], "title": ["Preliminaries of a Space Situational Awareness Ontology"], "date": ["2016-06-02T19:37:14Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1606.01924v2"], "summary": ["  Space situational awareness (SSA) is vital for international safety and\nsecurity, and the future of space travel. By improving SSA data-sharing we\nimprove global SSA. Computational ontology may provide one means toward that\ngoal. This paper develops the ontology of the SSA domain and takes steps in the\ncreation of the space situational awareness ontology. Ontology objectives,\nrequirements and desiderata are outlined; and both the SSA domain and the\ndiscipline of ontology are described. The purposes of the ontology include:\nexploring the potential for ontology development and engineering to (i)\nrepresent SSA data, general domain knowledge, objects and relationships (ii)\nannotate and express the meaning of that data, and (iii) foster SSA\ndata-exchange and integration among SSA actors, orbital debris databases, space\nobject catalogs and other SSA data repositories. By improving SSA via data- and\nknowledge-sharing, we can (iv) expand our scientific knowledge of the space\nenvironment, (v) advance our capacity for planetary defense from near-Earth\nobjects, and (vi) ensure the future of safe space flight for generations to\ncome.\n"]},
{"authors": ["Carl Friedrich Bolz", "Darya Kurilova", "Laurence Tratt"], "title": ["Making an Embedded DBMS JIT-friendly"], "date": ["2015-12-10T10:57:56Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1512.03207v4"], "summary": ["  While database management systems (DBMSs) are highly optimized, interactions\nacross the boundary between the programming language (PL) and the DBMS are\ncostly, even for in-process embedded DBMSs. In this paper, we show that\nprograms that interact with the popular embedded DBMS SQLite can be\nsignificantly optimized - by a factor of 3.4 in our benchmarks - by inlining\nacross the PL / DBMS boundary. We achieved this speed-up by replacing parts of\nSQLite's C interpreter with RPython code and composing the resulting\nmeta-tracing virtual machine (VM) - called SQPyte - with the PyPy VM. SQPyte\ndoes not compromise stand-alone SQL performance and is 2.2% faster than SQLite\non the widely used TPC-H benchmark suite.\n"]},
{"authors": ["Ayush Dubey", "Greg D. Hill", "Robert Escriva", "Emin G\u00fcn Sirer"], "title": ["Weaver: A High-Performance, Transactional Graph Database Based on\n  Refinable Timestamps"], "date": ["2015-09-28T19:30:30Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1509.08443v2"], "summary": ["  Graph databases have become an increasingly common infrastructure component.\nYet existing systems either operate on offline snapshots, provide weak\nconsistency guarantees, or use expensive concurrency control techniques that\nlimit performance. In this paper, we introduce a new distributed graph\ndatabase, called Weaver, which enables efficient, transactional graph analyses\nas well as strictly serializable ACID transactions on dynamic graphs. The key\ninsight that allows Weaver to combine strict serializability with horizontal\nscalability and high performance is a novel request ordering mechanism called\nrefinable timestamps. This technique couples coarse-grained vector timestamps\nwith a fine-grained timeline oracle to pay the overhead of strong consistency\nonly when needed. Experiments show that Weaver enables a Bitcoin blockchain\nexplorer that is 8x faster than Blockchain.info, and achieves 12x higher\nthroughput than the Titan graph database on social network workloads and 4x\nlower latency than GraphLab on offline graph traversal workloads.\n"]},
{"authors": ["Yihan Gao", "Aditya Parameswaran"], "title": ["Squish: Near-Optimal Compression for Archival of Relational Datasets"], "date": ["2016-02-12T22:46:57Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1602.04256v2"], "summary": ["  Relational datasets are being generated at an alarmingly rapid rate across\norganizations and industries. Compressing these datasets could significantly\nreduce storage and archival costs. Traditional compression algorithms, e.g.,\ngzip, are suboptimal for compressing relational datasets since they ignore the\ntable structure and relationships between attributes.\n  We study compression algorithms that leverage the relational structure to\ncompress datasets to a much greater extent. We develop Squish, a system that\nuses a combination of Bayesian Networks and Arithmetic Coding to capture\nmultiple kinds of dependencies among attributes and achieve near-entropy\ncompression rate. Squish also supports user-defined attributes: users can\ninstantiate new data types by simply implementing five functions for a new\nclass interface. We prove the asymptotic optimality of our compression\nalgorithm and conduct experiments to show the effectiveness of our system:\nSquish achieves a reduction of over 50\\% in storage size relative to systems\ndeveloped in prior work on a variety of real datasets.\n"]},
{"authors": ["Jeremy Kepner", "Vijay Gadepally", "Dylan Hutchison", "Hayden Jananthan", "Timothy Mattson", "Siddharth Samsi", "Albert Reuther"], "title": ["Associative Array Model of SQL, NoSQL, and NewSQL Databases"], "date": ["2016-06-18T19:29:17Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1606.05797v1"], "summary": ["  The success of SQL, NoSQL, and NewSQL databases is a reflection of their\nability to provide significant functionality and performance benefits for\nspecific domains, such as financial transactions, internet search, and data\nanalysis. The BigDAWG polystore seeks to provide a mechanism to allow\napplications to transparently achieve the benefits of diverse databases while\ninsulating applications from the details of these databases. Associative arrays\nprovide a common approach to the mathematics found in different databases: sets\n(SQL), graphs (NoSQL), and matrices (NewSQL). This work presents the SQL\nrelational model in terms of associative arrays and identifies the key\nmathematical properties that are preserved within SQL. These properties include\nassociativity, commutativity, distributivity, identities, annihilators, and\ninverses. Performance measurements on distributivity and associativity show the\nimpact these properties can have on associative array operations. These results\ndemonstrate that associative arrays could provide a mathematical model for\npolystores to optimize the exchange of data and execution queries.\n"]},
{"authors": ["Xiufeng Liu", "Per Sieverts Nielsen"], "title": ["A Hybrid ICT-Solution for Smart Meter Data Analytics"], "date": ["2016-06-18T18:07:37Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1606.05787v1"], "summary": ["  Smart meters are increasingly used worldwide. Smart meters are the advanced\nmeters capable of measuring energy consumption at a fine-grained time interval,\ne.g., every 15 minutes. Smart meter data are typically bundled with social\neconomic data in analytics, such as meter geographic locations, weather\nconditions and user information, which makes the data sets very sizable and the\nanalytics complex. Data mining and emerging cloud computing technologies make\ncollecting, processing, and analyzing the so-called big data possible. This\npaper proposes an innovative ICT-solution to streamline smart meter data\nanalytics. The proposed solution offers an information integration pipeline for\ningesting data from smart meters, a scalable platform for processing and mining\nbig data sets, and a web portal for visualizing analytics results. The\nimplemented system has a hybrid architecture of using Spark or Hive for big\ndata processing, and using the machine learning toolkit, MADlib, for doing\nin-database data analytics in PostgreSQL database. This paper evaluates the key\ntechnologies of the proposed ICT-solution, and the results show the\neffectiveness and efficiency of using the system for both batch and online\nanalytics.\n"]},
{"authors": ["Xiufeng Liu", "Per Sieverts Nielsen"], "title": ["Regression-based Online Anomaly Detection for Smart Grid Data"], "date": ["2016-06-18T17:03:18Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1606.05781v1"], "summary": ["  With the widely used smart meters in the energy sector, anomaly detection\nbecomes a crucial mean to study the unusual consumption behaviors of customers,\nand to discover unexpected events of using energy promptly. Detecting\nconsumption anomalies is, essentially, a real-time big data analytics problem,\nwhich does data mining on a large amount of parallel data streams from smart\nmeters. In this paper, we propose a supervised learning and statistical-based\nanomaly detection method, and implement a Lambda system using the in-memory\ndistributed computing framework, Spark and its extension Spark Streaming. The\nsystem supports not only iterative detection model refreshment from scalable\ndata sets, but also real-time detection on scalable live data streams. This\npaper empirically evaluates the system and the detection algorithm, and the\nresults show the effectiveness and the scalability of the proposed lambda\ndetection system.\n"]},
{"authors": ["Kristi Morton", "Hannaneh Hajishirzi", "Magdalena Balazinska", "Dan Grossman"], "title": ["View-Driven Deduplication with Active Learning"], "date": ["2016-06-17T23:38:51Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1606.05708v1"], "summary": ["  Visual analytics systems such as Tableau are increasingly popular for\ninteractive data exploration. These tools, however, do not currently assist\nusers with detecting or resolving potential data quality problems including the\nwell-known deduplication problem. Recent approaches for deduplication focus on\ncleaning entire datasets and commonly require hundreds to thousands of user\nlabels. In this paper, we address the problem of deduplication in the context\nof visual data analytics. We present a new approach for record deduplication\nthat strives to produce the cleanest view possible with a limited budget for\ndata labeling. The key idea behind our approach is to consider the impact that\nindividual tuples have on a visualization and to monitor how the view changes\nduring cleaning. With experiments on nine different visualizations for two\nreal-world datasets, we show that our approach produces significantly cleaner\nviews for small labeling budgets than state-of-the-art alternatives and that it\nalso stops the cleaning process after requesting fewer labels.\n"]},
{"authors": ["Burak Y\u0131ld\u0131z", "Tolga B\u00fcy\u00fcktan\u0131r", "Fatih Emekci"], "title": ["Equi-depth Histogram Construction for Big Data with Quality Guarantees"], "date": ["2016-06-17T19:45:03Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1606.05633v1"], "summary": ["  The amount of data generated and stored in cloud systems has been increasing\nexponentially. The examples of data include user generated data, machine\ngenerated data as well as data crawled from the Internet. There have been\nseveral frameworks with proven efficiency to store and process the petabyte\nscale data such as Apache Hadoop, HDFS and several NoSQL frameworks. These\nsystems have been widely used in industry and thus are subject to several\nresearch. The proposed data processing techniques should be compatible with the\nabove frameworks in order to be practical. One of the key data operations is\nderiving equi-depth histograms as they are crucial in understanding the\nstatistical properties of the underlying data with many applications including\nquery optimization. In this paper, we focus on approximate equi-depth histogram\nconstruction for big data and propose a novel merge based histogram\nconstruction method with a histogram processing framework which constructs an\nequi-depth histogram for a given time interval. The proposed method constructs\napproximate equi-depth histograms by merging exact equi-depth histograms of\npartitioned data by guaranteeing a maximum error bound on the number of items\nin a bucket (bucket size) as well as any range on the histogram.\n"]},
{"authors": ["Th\u00f4ng T. Nguy\u00ean", "Xiaokui Xiao", "Yin Yang", "Siu Cheung Hui", "Hyejin Shin", "Junbum Shin"], "title": ["Collecting and Analyzing Data from Smart Device Users with Local\n  Differential Privacy"], "date": ["2016-06-16T05:25:58Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1606.05053v1"], "summary": ["  Organizations with a large user base, such as Samsung and Google, can\npotentially benefit from collecting and mining users' data. However, doing so\nraises privacy concerns, and risks accidental privacy breaches with serious\nconsequences. Local differential privacy (LDP) techniques address this problem\nby only collecting randomized answers from each user, with guarantees of\nplausible deniability; meanwhile, the aggregator can still build accurate\nmodels and predictors by analyzing large amounts of such randomized data. So\nfar, existing LDP solutions either have severely restricted functionality, or\nfocus mainly on theoretical aspects such as asymptotical bounds rather than\npractical usability and performance. Motivated by this, we propose Harmony, a\npractical, accurate and efficient system for collecting and analyzing data from\nsmart device users, while satisfying LDP. Harmony applies to multi-dimensional\ndata containing both numerical and categorical attributes, and supports both\nbasic statistics (e.g., mean and frequency estimates), and complex machine\nlearning tasks (e.g., linear regression, logistic regression and SVM\nclassification). Experiments using real data confirm Harmony's effectiveness.\n"]},
{"authors": ["Wolfgang Gatterbauer", "Dan Suciu"], "title": ["Dissociation and Propagation for Approximate Lifted Inference with\n  Standard Relational Database Management Systems"], "date": ["2013-10-23T15:14:41Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1310.6257v4"], "summary": ["  Probabilistic inference over large data sets is a challenging data management\nproblem since exact inference is generally #P-hard and is most often solved\napproximately with sampling-based methods today. This paper proposes an\nalternative approach for approximate evaluation of conjunctive queries with\nstandard relational databases: In our approach, every query is evaluated\nentirely in the database engine by evaluating a fixed number of query plans,\neach providing an upper bound on the true probability, then taking their\nminimum. We provide an algorithm that takes into account important schema\ninformation to enumerate only the minimal necessary plans among all possible\nplans. Importantly, this algorithm is a strict generalization of all known\nPTIME self-join-free conjunctive queries: A query is in PTIME if and only if\nour algorithm returns one single plan. Furthermore, our approach is a\ngeneralization of a family of efficient ranking methods from graphs to\nhypergraphs. We also adapt three relational query optimization techniques to\nevaluate all necessary plans very fast. We give a detailed experimental\nevaluation of our approach and, in the process, provide a new way of thinking\nabout the value of probabilistic methods over non-probabilistic methods for\nranking query answers. We also note that the techniques developed in this paper\napply immediately to lifted inference from statistical relational models since\nlifted inference corresponds to PTIME plans in probabilistic databases.\n"]},
{"authors": ["Ruifeng Liu", "Ada WaiChee Fu", "Zitong Chen", "Silu Huang", "Yubao Liu"], "title": ["Finding Multiple New Optimal Locations in a Road Network"], "date": ["2016-06-04T07:42:16Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1606.01340v2"], "summary": ["  We study the problem of optimal location querying for location based services\nin road networks, which aims to find locations for new servers or facilities.\nThe existing optimal solutions on this problem consider only the cases with one\nnew server. When two or more new servers are to be set up, the problem with\nminmax cost criteria, MinMax, becomes NP-hard. In this work we identify some\nuseful properties about the potential locations for the new servers, from which\nwe derive a novel algorithm for MinMax, and show that it is efficient when the\nnumber of new servers is small. When the number of new servers is large, we\npropose an efficient 3-approximate algorithm. We verify with experiments on\nreal road networks that our solutions are effective and attains significantly\nbetter result quality compared to the existing greedy algorithms.\n"]},
{"authors": ["L\u00e1szl\u00f3 Dobos", "Erika Varga-Vereb\u00e9lyi", "Eva Verdugo", "David Teyssier", "Katrina Exter", "Ivan Valtchanov", "Tam\u00e1s Budav\u00e1ri", "Csaba Kiss"], "title": ["The Footprint Database and Web Services of the Herschel Space\n  Observatory"], "date": ["2016-06-13T14:05:52Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1606.03957v1"], "summary": ["  Data from the Herschel Space Observatory is freely available to the public\nbut no uniformly processed catalogue of the observations has been published so\nfar. To date, the Herschel Science Archive does not contain the exact sky\ncoverage (footprint) of individual observations and supports search for\nmeasurements based on bounding circles only. Drawing on previous experience in\nimplementing footprint databases, we built the Herschel Footprint Database and\nWeb Services for the Herschel Space Observatory to provide efficient search\ncapabilities for typical astronomical queries. The database was designed with\nthe following main goals in mind: (a) provide a unified data model for\nmeta-data of all instruments and observational modes, (b) quickly find\nobservations covering a selected object and its neighbourhood, (c) quickly find\nevery observation in a larger area of the sky, (d) allow for finding solar\nsystem objects crossing observation fields. As a first step, we developed a\nunified data model of observations of all three Herschel instruments for all\npointing and instrument modes. Then, using telescope pointing information and\nobservational meta-data, we compiled a database of footprints. As opposed to\nmethods using pixellation of the sphere, we represent sky coverage in an exact\ngeometric form allowing for precise area calculations. For easier handling of\nHerschel observation footprints with rather complex shapes, two algorithms were\nimplemented to reduce the outline. Furthermore, a new visualisation tool to\nplot footprints with various spherical projections was developed. Indexing of\nthe footprints using Hierarchical Triangular Mesh makes it possible to quickly\nfind observations based on sky coverage, time and meta-data. The database is\naccessible via a web site (http://herschel.vo.elte.hu) and also as a set of\nREST web service functions.\n"]},
{"authors": ["Janis Barzdins", "Mikus Grasmanis", "Edgars Rencis", "Agris Sostaks", "Juris Barzdins"], "title": ["Self-service Ad-hoc Querying Using Controlled Natural Language"], "date": ["2016-06-08T14:24:07Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1606.02573v1"], "summary": ["  The ad-hoc querying process is slow and error prone due to inability of\nbusiness experts of accessing data directly without involving IT experts. The\nproblem lies in complexity of means used to query data. We propose a new\nnatural language- and semistar ontology-based ad-hoc querying approach which\nlowers the steep learning curve required to be able to query data. The proposed\napproach would significantly shorten the time needed to master the ad-hoc\nquerying and to gain the direct access to data by business experts, thus\nfacilitating the decision making process in enterprises, government\ninstitutions and other organizations.\n"]},
{"authors": ["Sutanay Choudhury", "Khushbu Agarwal", "Sumit Purohit", "Baichuan Zhang", "Meg Pirrung", "Will Smith", "Mathew Thomas"], "title": ["NOUS: Construction and Querying of Dynamic Knowledge Graphs"], "date": ["2016-06-07T20:02:04Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1606.02314v1"], "summary": ["  The ability to construct domain specific knowledge graphs (KG) and perform\nquestion-answering or hypothesis generation is a transformative capability.\nDespite their value, automated construction of knowledge graphs remains an\nexpensive technical challenge that is beyond the reach for most enterprises and\nacademic institutions. We propose an end-to-end framework for developing custom\nknowledge graph driven analytics for arbitrary application domains. The\nuniqueness of our system lies A) in its combination of curated KGs along with\nknowledge extracted from unstructured text, B) support for advanced trending\nand explanatory questions on a dynamic KG, and C) the ability to answer queries\nwhere the answer is embedded across multiple data sources.\n"]},
{"authors": ["Poonam Kumari", "Said Achmiz", "Oliver Kennedy"], "title": ["Communicating Data Quality in On-Demand Curation"], "date": ["2016-06-07T18:31:54Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1606.02250v1"], "summary": ["  On-demand curation (ODC) tools like Paygo, KATARA, and Mimir allow users to\ndefer expensive curation effort until it is necessary. In contrast to classical\ndatabases that do not respond to queries over potentially erroneous data, ODC\nsystems instead answer with guesses or approximations. The quality and scope of\nthese guesses may vary and it is critical that an ODC system be able to\ncommunicate this information to an end-user. The central contribution of this\npaper is a preliminary user study evaluating the cognitive burden and\nexpressiveness of four representations of \"attribute-level\" uncertainty. The\nstudy shows (1) insignificant differences in time taken for users to interpret\nthe four types of uncertainty tested, and (2) that different presentations of\nuncertainty change the way people interpret and react to data. Ultimately, we\nshow that a set of UI design guidelines and best practices for conveying\nuncertainty will be necessary for ODC tools to be effective. This paper\nrepresents the first step towards establishing such guidelines.\n"]},
{"authors": ["Alexandr Savinov"], "title": ["Concept-Oriented Model: the Functional View"], "date": ["2016-06-07T18:05:50Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1606.02237v1"], "summary": ["  The plethora of existing data models and specific data modeling techniques is\nnot only confusing but leads to complex, eclectic and inefficient designs of\nsystems for data management and analytics. The main goal of this paper is to\ndescribe a unified approach to data modeling, called the concept-oriented model\n(COM), by using functions as a basis for its formalization. COM tries to answer\nthe question what is data and to rethink basic assumptions underlying this and\nrelated notions. Its main goal is to unify major existing views on data\n(generality), using only a few main notions (simplicity) which are very close\nto how data is used in real life (naturalness).\n"]},
{"authors": ["Kalyani Natu"], "title": ["Initialization Errors in Quantum Data Base Recall"], "date": ["2016-06-07T16:46:53Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1606.02208v1"], "summary": ["  This paper analyzes the relationship between initialization error and recall\nof a specific memory in the Grover algorithm for quantum database search. It is\nshown that the correct memory is obtained with high probability even when the\ninitial state is far removed from the correct one. The analysis is done by\nrelating the variance of error in the initial state to the recovery of the\ncorrect memory and the surprising result is obtained that the relationship\nbetween the two is essentially linear.\n"]},
{"authors": ["Lei Guo", "Dejun Teng", "Rubao Lee", "Feng Chen", "Siyuan Ma", "Xiaodong Zhang"], "title": ["Re-enabling high-speed caching for LSM-trees"], "date": ["2016-06-07T03:53:45Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1606.02015v1"], "summary": ["  LSM-tree has been widely used in cloud computing systems by Google, Facebook,\nand Amazon, to achieve high performance for write-intensive workloads. However,\nin LSM-tree, random key-value queries can experience long latency and low\nthroughput due to the interference from the compaction, a basic operation in\nthe algorithm, to caching. LSM-tree relies on frequent compaction operations to\nmerge data into a sorted structure. After a compaction, the original data are\nreorganized and written to other locations on the disk. As a result, the cached\ndata are invalidated since their referencing addresses are changed, causing\nserious performance degradations.\n  We propose dLSM in order to re-enable high-speed caching during intensive\nwrites. dLSM is an LSM-tree with a compaction buffer on the disk, working as a\ncushion to minimize the cache invalidation caused by compactions. The\ncompaction buffer maintains a series of snapshots of the frequently compacted\ndata, which represent a consistent view of the corresponding data in the\nunderlying LSM-tree. Being updated in a much lower rate than that of\ncompactions, data in the compaction buffer are almost stationary. In dLSM, an\nobject is referenced by the disk address of the corresponding block either in\nthe compaction buffer for frequently compacted data, or in the underlying\nLSM-tree for infrequently compacted data. Thus, hot objects can be effectively\nkept in the cache without harmful invalidations. With the help of a small\non-disk compaction buffer, dLSM achieves a high query performance by enabling\neffective caching, while retaining all merits of LSM-tree for write-intensive\ndata processing. We have implemented dLSM based on LevelDB. Our evaluations\nshow that with a standard DRAM cache, dLSM can achieve 5--8x performance\nimprovement over LSM with the same cache on HDD storage.\n"]},
{"authors": ["Hasan M. Jamil", "Fereidoon Sadri"], "title": ["Reliable Querying of Very Large, Fast Moving and Noisy Predicted\n  Interaction Data using Hierarchical Crowd Curation"], "date": ["2016-06-06T22:06:26Z"], "category": "cs.DB", "url": ["http://arxiv.org/abs/1606.01957v1"], "summary": ["  The abundance of predicted and mined but uncertain biological data show huge\nneeds for massive, efficient and scalable curation efforts. The human expertise\nwarranted by any successful curation enterprize is often economically\nprohibitive especially for speculative end user queries that may not ultimately\nbear fruit. So the challenge remains in devising a low cost engine capable of\ndelivering fast but tentative annotation and curation of a set of data items\nthat can be authoritatively validated by experts later demanding significantly\nsmall investment. The aim thus is to make a large volume of predicted data\navailable for use as early as possible with an acceptable degree of confidence\nin their accuracy while the curation continues. In this paper, we present a\nnovel approach to annotation and curation of biological database contents using\ncrowd computing. The technical contribution is in the identification and\nmanagement of trust of mechanical turks, and support for ad hoc declarative\nqueries, both of which are leveraged to support reliable analytics using noisy\npredicted interactions.\n"]}
]
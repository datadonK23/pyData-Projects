{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "ML Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc count: 60\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "corpus = load_files(\"../data/\")\n",
    "\n",
    "doc_count = len(corpus.data)\n",
    "print(\"Doc count:\", doc_count)\n",
    "assert doc_count is 60, \"Wrong number of documents loaded, should be 60 (4 novels + 56 stories)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to ../nltk/...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to ../nltk/...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents='ascii', sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=<tokenizer.TextWrangler object at 0x7f78d2c09048>,\n",
       "        use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tokenizer import TextWrangler\n",
    "\n",
    "tfidf_stem = TfidfVectorizer(strip_accents=\"ascii\", tokenizer=TextWrangler(kind=\"stem\"))\n",
    "tfidf_stem.fit(corpus.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD, NMF\n",
    "\n",
    "n_topics = 20 #FIXME GridSearch\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=n_topics)\n",
    "lsa = TruncatedSVD(n_components=n_topics)\n",
    "nmf = NMF(n_components=n_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "model = Pipeline([\n",
    "    (\"tfidf\", tfidf_stem),\n",
    "    (\"model\", lda)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/datadonk23/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ...ol=0.1, random_state=None,\n",
       "             topic_word_prior=None, total_samples=1000000.0, verbose=0))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(corpus.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Likelelihood: -146336.68785699786\n"
     ]
    }
   ],
   "source": [
    "print(\"Log Likelelihood:\", model.score(corpus.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "blym haf shelterless malefact divin streamed abstract vic slumb throng\n",
      "Topic #1:\n",
      "cruel discern unimport watercours gauz hoolig crav jupit queen ft\n",
      "Topic #2:\n",
      "holm said briarbra night com man atom wom scribbled know\n",
      "Topic #3:\n",
      "condon cruellest lanky unbrush dainty torto hut hum employ keepsak\n",
      "Topic #4:\n",
      "holm said man mr com bust beetl whereabout await shal\n",
      "Topic #5:\n",
      "beetl subordin attir electro vic battlefield breech pres lik afflu\n",
      "Topic #6:\n",
      "holm said murdoch man switzerland did peer cork hardiest mr\n",
      "Topic #7:\n",
      "helpm unimpass meredi transylvan prevy jefferson finest waldba ardu buzz\n",
      "Topic #8:\n",
      "nant vish phrases cloudy sigismond intrud naught unquest whe impetu\n",
      "Topic #9:\n",
      "mood stal provoc flesh entangl foreign slap eclips eastern interject\n",
      "Topic #10:\n",
      "unexplain promiscu hampshir carpet rein glossiest wait metic wallow porty\n",
      "Topic #11:\n",
      "holm said man mr com hand room watson know look\n",
      "Topic #12:\n",
      "raj remonst cap wholesom saturnin rul vanguard nov exclam fortescu\n",
      "Topic #13:\n",
      "crack holm bombay roof wip plaint outlay straight tab moriarty\n",
      "Topic #14:\n",
      "holm melt fleshy treason chast plu yawl toed forthcom xi\n",
      "Topic #15:\n",
      "mod incorrect tight uninterest stil heh impaty blist eckerman fril\n",
      "Topic #16:\n",
      "soam fac unaccustom patriot fantast unsystem firearm barrack nark scholars\n",
      "Topic #17:\n",
      "thorneycroft nitrit sauv cab boodl you holm kerchief bubbl secret\n",
      "Topic #18:\n",
      "reb beatitud nak reap atkinson rang composit parv sor cardiac\n",
      "Topic #19:\n",
      "discuss haughty mort chain stretch landown struggled paganin pierrot embroid\n"
     ]
    }
   ],
   "source": [
    "# Inspect topics\n",
    "names = tfidf_stem.get_feature_names()\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "\n",
    "print_top_words(lda, names, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
